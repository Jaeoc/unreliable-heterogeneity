@incollection{10.1002/9781119099369.ch10,
  title = {Exploring {{Heterogeneity}}},
  booktitle = {Systematic {{Reviews}} in {{Health Research}}},
  author = {Higgins, Julian P.T. and Li, Tianjing},
  year = {2022},
  pages = {185--203},
  publisher = {{John Wiley \& Sons, Ltd}},
  doi = {10.1002/9781119099369.ch10},
  urldate = {2023-04-24},
  abstract = {Studies brought together in a meta-analysis are likely to vary in terms of where, when, why, and how they were undertaken. This diversity across the studies often gives rise to heterogeneity: the statistical variability of results beyond what would be expected by chance alone. As a general principle, heterogeneity in meta-analysis should be investigated. In this chapter, we discuss the main two approaches to investigating heterogeneity, which are (i) to group the studies (or participants) into subsets and compare the overall effects across these subsets, an analysis known as subgroup analysis; and (ii) to explore the gradient of effects across studies according to one or more numeric study features, an analysis known as meta-regression. However, subgroup analyses and meta-regression can be problematic for several reasons. Associations between study characteristics and study results are observational and are subject to confounding. Aggregation biases can obscure true associations. Small numbers of studies and numerous potential causes of heterogeneity create a high risk of finding spurious associations. Some analyses that seem appealing, including investigations of small-study effects and baseline risk, can give rise to spurious findings due to statistical artefacts.},
  chapter = {10},
  isbn = {978-1-119-09936-9},
  langid = {english},
  keywords = {aggregation bias,baseline risk,heterogeneity,meta-regression,small-study effects,subgroup analysis},
  file = {C\:\\Users\\u237972\\Zotero\\storage\\FVZVHSL2\\9781119099369.html}
}

@article{10.1002/jrsm.1164,
  title = {Methods to Estimate the Between-Study Variance and Its Uncertainty in Meta-Analysis},
  author = {Veroniki, Areti Angeliki and Jackson, Dan and Viechtbauer, Wolfgang and Bender, Ralf and Bowden, Jack and Knapp, Guido and Kuss, Oliver and Higgins, Julian PT and Langan, Dean and Salanti, Georgia},
  year = {2016},
  journal = {Research Synthesis Methods},
  volume = {7},
  number = {1},
  pages = {55--79},
  issn = {1759-2887},
  doi = {10.1002/jrsm.1164},
  urldate = {2023-05-21},
  abstract = {Meta-analyses are typically used to estimate the overall/mean of an outcome of interest. However, inference about between-study variability, which is typically modelled using a between-study variance parameter, is usually an additional aim. The DerSimonian and Laird method, currently widely used by default to estimate the between-study variance, has been long challenged. Our aim is to identify known methods for estimation of the between-study variance and its corresponding uncertainty, and to summarise the simulation and empirical evidence that compares them. We identified 16 estimators for the between-study variance, seven methods to calculate confidence intervals, and several comparative studies. Simulation studies suggest that for both dichotomous and continuous data the estimator proposed by Paule and Mandel and for continuous data the restricted maximum likelihood estimator are better alternatives to estimate the between-study variance. Based on the scenarios and results presented in the published studies, we recommend the Q-profile method and the alternative approach based on a `generalised Cochran between-study variance statistic' to compute corresponding confidence intervals around the resulting estimates. Our recommendations are based on a qualitative evaluation of the existing literature and expert consensus. Evidence-based recommendations require an extensive simulation study where all methods would be compared under the same scenarios. \textcopyright{} 2015 The Authors. Research Synthesis Methods published by John Wiley \& Sons Ltd.},
  langid = {english},
  keywords = {bias,confidence interval,coverage probability,heterogeneity,mean squared error},
  file = {C\:\\Users\\u237972\\Zotero\\storage\\NTYQNRVP\\jrsm.html}
}

@article{10.1002/jrsm.1198,
  title = {Comparative Performance of Heterogeneity Variance Estimators in Meta-Analysis: A Review of Simulation Studies},
  shorttitle = {Comparative Performance of Heterogeneity Variance Estimators in Meta-Analysis},
  author = {Langan, Dean and Higgins, Julian P. T. and Simmonds, Mark},
  year = {2017},
  journal = {Research Synthesis Methods},
  volume = {8},
  number = {2},
  pages = {181--198},
  issn = {1759-2887},
  doi = {10.1002/jrsm.1198},
  urldate = {2023-04-24},
  abstract = {Random-effects meta-analysis methods include an estimate of between-study heterogeneity variance. We present a systematic review of simulation studies comparing the performance of different estimation methods for this parameter. We summarise the performance of methods in relation to estimation of heterogeneity and of the overall effect estimate, and of confidence intervals for the latter. Among the twelve included simulation studies, the DerSimonian and Laird method was most commonly evaluated. This estimate is negatively biased when heterogeneity is moderate to high and therefore most studies recommended alternatives. The Paule\textendash Mandel method was recommended by three studies: it is simple to implement, is less biased than DerSimonian and Laird and performs well in meta-analyses with dichotomous and continuous outcomes. In many of the included simulation studies, results were based on data that do not represent meta-analyses observed in practice, and only small selections of methods were compared. Furthermore, potential conflicts of interest were present when authors of novel methods interpreted their results. On the basis of current evidence, we provisionally recommend the Paule\textendash Mandel method for estimating the heterogeneity variance, and using this estimate to calculate the mean effect and its 95\% confidence interval. However, further simulation studies are required to draw firm conclusions. Copyright \textcopyright{} 2016 John Wiley \& Sons, Ltd.},
  langid = {english},
  keywords = {DerSimonian\textendash Laird,heterogeneity,meta-analysis,random effects,simulation},
  file = {C\:\\Users\\u237972\\Zotero\\storage\\4U6GECHU\\Langan et al_2017_Comparative performance of heterogeneity variance estimators in meta-analysis.pdf;C\:\\Users\\u237972\\Zotero\\storage\\932AULMB\\jrsm.html}
}

@article{10.1002/jrsm.12,
  title = {A Basic Introduction to Fixed-Effect and Random-Effects Models for Meta-Analysis},
  author = {Borenstein, Michael and Hedges, Larry V. and Higgins, Julian P.T. and Rothstein, Hannah R.},
  year = {2010},
  month = apr,
  journal = {Research Synthesis Methods},
  volume = {1},
  number = {2},
  pages = {97--111},
  issn = {17592879},
  doi = {10.1002/jrsm.12},
  urldate = {2020-03-06},
  langid = {english},
  file = {C\:\\Users\\u237972\\Zotero\\storage\\9M96Z68Y\\Borenstein et al_2010_A basic introduction to fixed-effect and random-effects models for meta-analysis.pdf}
}

@article{10.1002/jrsm.1230,
  title = {Basics of Meta-Analysis: {{I2}} Is Not an Absolute Measure of Heterogeneity},
  shorttitle = {Basics of Meta-Analysis},
  author = {Borenstein, Michael and Higgins, Julian P. T. and Hedges, Larry V. and Rothstein, Hannah R.},
  year = {2017},
  journal = {Research Synthesis Methods},
  volume = {8},
  number = {1},
  pages = {5--18},
  issn = {1759-2887},
  doi = {10.1002/jrsm.1230},
  urldate = {2023-05-21},
  abstract = {When we speak about heterogeneity in a meta-analysis, our intent is usually to understand the substantive implications of the heterogeneity. If an intervention yields a mean effect size of 50 points, we want to know if the effect size in different populations varies from 40 to 60, or from 10 to 90, because this speaks to the potential utility of the intervention. While there is a common belief that the I2 statistic provides this information, it actually does not. In this example, if we are told that I2 is 50\%, we have no way of knowing if the effects range from 40 to 60, or from 10 to 90, or across some other range. Rather, if we want to communicate the predicted range of effects, then we should simply report this range. This gives readers the information they think is being captured by I2 and does so in a way that is concise and unambiguous. Copyright \textcopyright{} 2017 John Wiley \& Sons, Ltd.},
  langid = {english},
  keywords = {heterogeneity,I-squared,I2,inconsistency,meta-analysis,prediction intervals},
  file = {C\:\\Users\\u237972\\Zotero\\storage\\YVT9S3BT\\jrsm.html}
}

@article{10.1002/jrsm.1316,
  title = {A Comparison of Heterogeneity Variance Estimators in Simulated Random-Effects Meta-Analyses},
  author = {Langan, Dean and Higgins, Julian P.T. and Jackson, Dan and Bowden, Jack and Veroniki, Areti Angeliki and Kontopantelis, Evangelos and Viechtbauer, Wolfgang and Simmonds, Mark},
  year = {2019},
  journal = {Research Synthesis Methods},
  volume = {10},
  number = {1},
  pages = {83--98},
  issn = {1759-2887},
  doi = {10.1002/jrsm.1316},
  urldate = {2023-04-24},
  abstract = {Studies combined in a meta-analysis often have differences in their design and conduct that can lead to heterogeneous results. A random-effects model accounts for these differences in the underlying study effects, which includes a heterogeneity variance parameter. The DerSimonian-Laird method is often used to estimate the heterogeneity variance, but simulation studies have found the method can be biased and other methods are available. This paper compares the properties of nine different heterogeneity variance estimators using simulated meta-analysis data. Simulated scenarios include studies of equal size and of moderate and large differences in size. Results confirm that the DerSimonian-Laird estimator is negatively biased in scenarios with small studies and in scenarios with a rare binary outcome. Results also show the Paule-Mandel method has considerable positive bias in meta-analyses with large differences in study size. We recommend the method of restricted maximum likelihood (REML) to estimate the heterogeneity variance over other methods. However, considering that meta-analyses of health studies typically contain few studies, the heterogeneity variance estimate should not be used as a reliable gauge for the extent of heterogeneity in a meta-analysis. The estimated summary effect of the meta-analysis and its confidence interval derived from the Hartung-Knapp-Sidik-Jonkman method are more robust to changes in the heterogeneity variance estimate and show minimal deviation from the nominal coverage of 95\% under most of our simulated scenarios.},
  langid = {english},
  keywords = {DerSimonian-Laird,heterogeneity,random-effects,REML,simulation},
  file = {C\:\\Users\\u237972\\Zotero\\storage\\LWJPEB34\\Langan et al_2019_A comparison of heterogeneity variance estimators in simulated random-effects.pdf}
}

@article{10.1002/jrsm.1584,
  title = {Meta-Analyzing Individual Participant Data from Studies with Complex Survey Designs: {{A}} Tutorial on Using the Two-Stage Approach for Data from Educational Large-Scale Assessments},
  shorttitle = {Meta-Analyzing Individual Participant Data from Studies with Complex Survey Designs},
  author = {Brunner, Martin and Keller, Lena and Stallasch, Sophie E. and Kretschmann, Julia and Hasl, Andrea and Preckel, Franzis and L{\"u}dtke, Oliver and Hedges, Larry V.},
  year = {2023},
  journal = {Research Synthesis Methods},
  volume = {14},
  number = {1},
  pages = {5--35},
  issn = {1759-2887},
  doi = {10.1002/jrsm.1584},
  urldate = {2023-05-25},
  abstract = {Descriptive analyses of socially important or theoretically interesting phenomena and trends are a vital component of research in the behavioral, social, economic, and health sciences. Such analyses yield reliable results when using representative individual participant data (IPD) from studies with complex survey designs, including educational large-scale assessments (ELSAs) or social, health, and economic survey and panel studies. The meta-analytic integration of these results offers unique and novel research opportunities to provide strong empirical evidence of the consistency and generalizability of important phenomena and trends. Using ELSAs as an example, this tutorial offers methodological guidance on how to use the two-stage approach to IPD meta-analysis to account for the statistical challenges of complex survey designs (e.g., sampling weights, clustered and missing IPD), first, to conduct descriptive analyses (Stage 1), and second, to integrate results with three-level meta-analytic and meta-regression models to take into account dependencies among effect sizes (Stage 2). The two-stage approach is illustrated with IPD on reading achievement from the Programme for International Student Assessment (PISA). We demonstrate how to analyze and integrate standardized mean differences (e.g., gender differences), correlations (e.g., with students' socioeconomic status [SES]), and interactions between individual characteristics at the participant level (e.g., the interaction between gender and SES) across several PISA cycles. All the datafiles and R scripts we used are available online. Because complex social, health, or economic survey and panel studies share many methodological features with ELSAs, the guidance offered in this tutorial is also helpful for synthesizing research evidence from these studies.},
  langid = {english},
  keywords = {complex survey designs,educational large-scale assessments,individual participant data,meta-analysis,Programme for International Student Assessment},
  file = {C\:\\Users\\u237972\\Zotero\\storage\\QZ93T4J4\\jrsm.html}
}

@article{10.1002/jrsm.47,
  title = {Conclusions from Meta-Analytic Structural Equation Models Generally Do Not Change Due to Corrections for Study Artifacts},
  author = {Michel, Jesse S. and Viswesvaran, Chockalingam and Thomas, Jeffrey},
  year = {2011},
  journal = {Research Synthesis Methods},
  volume = {2},
  number = {3},
  pages = {174--187},
  issn = {1759-2887},
  doi = {10.1002/jrsm.47},
  urldate = {2023-04-28},
  abstract = {Meta-analytic structural equations modeling is increasingly used in theory testing. There has been much debate when meta-analyzed correlation matrices are used in structural equations modeling on whether to use mean observed correlations (i.e., corrected only for sampling error) or correlations corrected for study artifacts such as unreliability in measures. This paper investigates whether the fit indices are affected by the corrections and if the stability of the paths (i.e., changes in significance, magnitude, and relative strengths or rank order) is affected by the corrections. Results suggest that substantive model conclusions are generally unaffected by study artifacts and related statistical corrections as long as the variables included in the path analyses had typical levels of reliability as found in the psychological literature. More specifically, all models examined exhibited similar model fit and pathway stability. Copyright \textcopyright{} 2011 John Wiley \& Sons, Ltd.},
  langid = {english},
  keywords = {meta-analysis,path analysis,psychometrics,structural equation modeling,validity generalization},
  file = {C\:\\Users\\u237972\\Zotero\\storage\\57TVUMVE\\Michel et al_2011_Conclusions from meta-analytic structural equation models generally do not.pdf;C\:\\Users\\u237972\\Zotero\\storage\\EG5QG6YX\\jrsm.html}
}

@article{10.1002/sim.1186,
  title = {Quantifying Heterogeneity in a Meta-Analysis},
  author = {Higgins, Julian P. T. and Thompson, Simon G.},
  year = {2002},
  month = jun,
  journal = {Statistics in Medicine},
  volume = {21},
  number = {11},
  pages = {1539--1558},
  issn = {0277-6715, 1097-0258},
  doi = {10.1002/sim.1186},
  urldate = {2018-06-28},
  langid = {english},
  file = {C\:\\Users\\u237972\\Zotero\\storage\\NFWLC2UD\\Higgins_Thompson_2002_Quantifying heterogeneity in a meta-analysis.pdf;C\:\\Users\\u237972\\Zotero\\storage\\89IDLSLE\\search.crossref.org.html}
}

@article{10.1007/s11336-008-9098-4,
  title = {Commentary on {{Coefficient Alpha}}: {{A Cautionary Tale}}},
  shorttitle = {Commentary on {{Coefficient Alpha}}},
  author = {Green, Samuel B. and Yang, Yanyun},
  year = {2009},
  month = mar,
  journal = {Psychometrika},
  volume = {74},
  number = {1},
  pages = {121--135},
  issn = {1860-0980},
  doi = {10.1007/s11336-008-9098-4},
  urldate = {2023-06-01},
  abstract = {The general use of coefficient alpha to assess reliability should be discouraged on a number of grounds. The assumptions underlying coefficient alpha are unlikely to hold in practice, and violation of these assumptions can result in nontrivial negative or positive bias. Structural equation modeling was discussed as an informative process both to assess the assumptions underlying coefficient alpha and to estimate reliability},
  langid = {english},
  keywords = {coefficient alpha,reliability,structural equation modeling,violation of assumptions},
  file = {C\:\\Users\\u237972\\Zotero\\storage\\MD4BW4KQ\\Green_Yang_2009_Commentary on Coefficient Alpha.pdf}
}

@article{10.1007/s11336-008-9101-0,
  title = {On the {{Use}}, the {{Misuse}}, and the {{Very Limited Usefulness}} of~{{Cronbach}}'s {{Alpha}}},
  author = {Sijtsma, Klaas},
  year = {2008},
  month = dec,
  journal = {Psychometrika},
  volume = {74},
  number = {1},
  pages = {107},
  issn = {1860-0980},
  doi = {10.1007/s11336-008-9101-0},
  urldate = {2018-11-09},
  abstract = {This discussion paper argues that both the use of Cronbach's alpha as a reliability estimate and as a measure of internal consistency suffer from major problems. First, alpha always has a value, which cannot be equal to the test score's reliability given the interitem covariance matrix and the usual assumptions about measurement error. Second, in practice, alpha is used more often as a measure of the test's internal consistency than as an estimate of reliability. However, it can be shown easily that alpha is unrelated to the internal structure of the test. It is further discussed that statistics based on a single test administration do not convey much information about the accuracy of individuals' test performance. The paper ends with a list of conclusions about the usefulness of alpha.},
  langid = {english},
  keywords = {Cronbach's alpha,internal consistency,reliability,unidimensionality},
  file = {C\:\\Users\\u237972\\Zotero\\storage\\WU3PMP6R\\Sijtsma_2008_On the Use, the Misuse, and the Very Limited Usefulness of Cronbach’s Alpha.pdf}
}

@article{10.1007/s40299-013-0075-z,
  title = {Coefficient {{Alpha}} and {{Beyond}}: {{Issues}} and {{Alternatives}} for {{Educational Research}}},
  shorttitle = {Coefficient {{Alpha}} and {{Beyond}}},
  author = {Teo, Timothy and Fan, Xitao},
  year = {2013},
  month = may,
  journal = {The Asia-Pacific Education Researcher},
  volume = {22},
  number = {2},
  pages = {209--213},
  issn = {2243-7908},
  doi = {10.1007/s40299-013-0075-z},
  urldate = {2023-05-31},
  abstract = {Cronbach's coefficient alpha has been widely known and used in educational research. Many education research practitioners, however, may not be aware of the potential issues when the main assumptions for coefficient alpha are violated in research practice. This paper provides a brief discussion about two assumptions that may make the use and interpretation of coefficient alpha less appropriate in education research: violations of the tau-equivalence model assumption and the error independence assumption. Violation of either or both of these assumptions will have negative effects on the precision of coefficient alpha as reliability estimate. The paper further presents two alternative reliability estimates without the assumptions of tau-equivalence or error independence. Research practitioners may consider these and other alternatives, when measurement data may not satisfy the assumptions for coefficient alpha.},
  langid = {english},
  keywords = {Cronbach coefficient alpha,Generalizability theory,Independent errors,Internal consistency,Latent variable modeling,Reliability,Tau-equivalence},
  file = {C\:\\Users\\u237972\\Zotero\\storage\\6SBQW4AV\\Teo_Fan_2013_Coefficient Alpha and Beyond.pdf}
}

@article{10.1016/0001-87918890043-7,
  title = {Why Plaintiffs' Counsel Challenge Tests, and How They Can Successfully Challenge the Theory of ``Validity Generalization''},
  author = {Seymour, Richard T},
  year = {1988},
  month = dec,
  journal = {Journal of Vocational Behavior},
  volume = {33},
  number = {3},
  pages = {331--364},
  issn = {0001-8791},
  doi = {10.1016/0001-8791(88)90043-7},
  urldate = {2023-05-11},
  abstract = {The use of tests can be an engine for the exclusion of racial minorities more permanent and thorough than individual ill will. Both in 1964 and 1972, Congress expressly chose to distinguish between good and bad tests, immunizing only the adverse impact arising from good tests. The various means of challenging tests are explored, with an explanation of the differences and similarities between disparate-impact and disparate-treatment analysis. The factors which lead plaintiffs' counsel to decide to challenge particular tests are explained, including differences among tests in the degree of adverse impact, the role of ``face invalidity,'' and other matters. Successful legal challenges can be made to the more extreme claims of validity generalization, based on: (1) the inability of the theory to detect racially unfair tests and the implications of that failure for the entire approach; (2) the use of hydraulic ``corrections'' to observed correlations, without satisfying the statistical conditions for making such corrections; (3) the assumption of linear relationships between test scores and performance without demonstrating that the populations at the top and bottom of the scale are the same, and without demonstrating that a change of a given number of points in the test score corresponds to the same fixed change in performance score regardless of where along the continuum of test scores that change takes place; (4) the problem of low coefficients of correlation and the basic misunderstanding by some theorists of the meaning of those coefficients; (5) unexamined problems in the claims made for the utility of testing; and (6) problems with the use of meta-analysis in a field in which experiments cannot be reproduced, and in which particular problems with probabilistic calculations have not been recognized.},
  langid = {english},
  file = {C\:\\Users\\u237972\\Zotero\\storage\\M5CA3FJ2\\Seymour_1988_Why plaintiffs' counsel challenge tests, and how they can successfully.pdf;C\:\\Users\\u237972\\Zotero\\storage\\ZMH65GCV\\0001879188900437.html}
}

@article{10.1016/S0160-28960200082-X,
  title = {True Scores, Latent Variables, and Constructs: {{A}} Comment on {{Schmidt}} and {{Hunter}}},
  shorttitle = {True Scores, Latent Variables, and Constructs},
  author = {Borsboom, Denny and Mellenbergh, Gideon J.},
  year = {2002},
  month = nov,
  journal = {Intelligence},
  volume = {30},
  number = {6},
  pages = {505--514},
  issn = {0160-2896},
  doi = {10.1016/S0160-2896(02)00082-X},
  urldate = {2023-05-24},
  abstract = {This paper comments on an article by Schmidt and Hunter [Intelligence 27 (1999) 183.], who argue that the correction for attenuation should be routinely used in theory testing. It is maintained that Schmidt and Hunter's arguments are based on mistaken assumptions. We discuss our critique of Schmidt and Hunter in terms of two arguments against a routine use of the correction for attenuation within the classical test theory framework: (1) corrected correlations do not, as Schmidt and Hunter claim, provide correlations between constructs, and (2) corrections for measurement error should be made using modern test theory models instead of the classical model. The arguments that Schmidt and Hunter advance in favor of the correction for attenuation can be traced to an implicit identification of true scores with construct scores. First, we show that this identification confounds issues of validity and issues of reliability. Second, it is pointed out that equating true scores with construct scores is logically inconsistent with the classical test theory model itself. Third, it is argued that the classical model is not suited for detecting the dimensionality of test scores, which severely limits the interpretation of the corrected correlation coefficients. It is concluded that most measurement problems in psychology concern issues of validity, and that the correction for attenuation within classical test theory does not help in solving them.},
  langid = {english},
  keywords = {Classical test theory,Correction for attenuation,Modern test theory},
  file = {C\:\\Users\\u237972\\Zotero\\storage\\662BAET2\\Borsboom_Mellenbergh_2002_True scores, latent variables, and constructs.pdf;C\:\\Users\\u237972\\Zotero\\storage\\4HN5QA3K\\S016028960200082X.html}
}

@article{10.1016/S0895-43569900161-4,
  title = {Publication Bias in Meta-Analysis: Its Causes and Consequences},
  shorttitle = {Publication Bias in Meta-Analysis},
  author = {Thornton, Alison and Lee, Peter},
  year = {2000},
  month = feb,
  journal = {Journal of Clinical Epidemiology},
  volume = {53},
  number = {2},
  pages = {207--216},
  issn = {0895-4356},
  doi = {10.1016/S0895-4356(99)00161-4},
  urldate = {2023-06-01},
  abstract = {Publication bias is a widespread problem that may seriously distort attempts to estimate the effect under investigation. The literature is reviewed to determine features of the design and execution of both single studies and meta-analyses leading to publication bias, and the role the author, journal editor, and reviewer play in selecting studies for publication. Methods of detecting, correcting for, and preventing publication bias are reviewed. The design of the meta-analysis itself, and the studies included in it, are shown to be important among a number of sources of publication bias. Various factors influence an author's decision to submit results for publication. Journal editors and reviewers are crucial in deciding which studies to publish. Various methods proposed for detecting and correcting for publication bias, though useful, all have limitations. However, prevention of publication bias by registering every trial undertaken or publishing all studies is an ideal that is hard to achieve.},
  langid = {english},
  keywords = {Publication bias},
  file = {C\:\\Users\\u237972\\Zotero\\storage\\2TGQRF3X\\S0895435699001614.html}
}

@article{10.1017/iop.2015.17,
  title = {Imperfect {{Corrections}} or {{Correct Imperfections}}? {{Psychometric Corrections}} in {{Meta-Analysis}}},
  shorttitle = {Imperfect {{Corrections}} or {{Correct Imperfections}}?},
  author = {Oswald, Frederick L. and Ercan, Seydahmet and McAbee, Samuel T. and Ock, Jisoo and Shaw, Amy},
  year = {2015},
  month = jun,
  journal = {Industrial and Organizational Psychology},
  volume = {8},
  number = {2},
  pages = {e1-e4},
  publisher = {{Cambridge University Press}},
  issn = {1754-9426, 1754-9434},
  doi = {10.1017/iop.2015.17},
  urldate = {2023-04-28},
  abstract = {There is understandable concern by LeBreton, Scherer, and James (2014) that psychometric corrections in organizational research are nothing more than a form of statistical hydraulics. Statistical corrections for measurement error variance and range restriction might inappropriately ratchet observed effects upward into regions of practical significance and publication glory\textemdash at the expense of highly questionable results.},
  langid = {english},
  file = {C\:\\Users\\u237972\\Zotero\\storage\\PSDUB5Y6\\Oswald et al_2015_Imperfect Corrections or Correct Imperfections.pdf}
}

@article{10.1027/0044-3409.215.2.90,
  title = {Current {{Methods}} for {{Meta-Analysis}}},
  author = {Schulze, Ralf},
  year = {2007},
  month = jan,
  journal = {Zeitschrift f\"ur Psychologie / Journal of Psychology},
  volume = {215},
  number = {2},
  pages = {90--103},
  publisher = {{Hogrefe Publishing}},
  issn = {0044-3409},
  doi = {10.1027/0044-3409.215.2.90},
  urldate = {2023-05-21},
  abstract = {. The bulk of conceptual and statistical developments as well as applications of meta-analysis have been published in the last 30 years. The methods for meta-analysis continue to be refined and new methods are applied to new types of research questions and data. Such current approaches, issues, and developments prevalent in the behavioral sciences are presented, reviewed, and discussed in this paper. The areas that are covered include: the fixed effects and random effects model of meta-analysis, new findings concerning effect sizes and their statistical properties, the comparison of different meta-analytic approaches, and multivariate procedures for meta-analysis. The latter also covers the stepwise combination of meta-analysis and structural equation modeling (MASEM).},
  keywords = {effect sizes,fixed and random effects model,meta-analysis,meta-analytic structural equation modeling,validity generalization},
  file = {C\:\\Users\\u237972\\Zotero\\storage\\4FCE6594\\Schulze_2007_Current Methods for Meta-Analysis.pdf}
}

@article{10.1027/1864-9335/a000178,
  title = {Investigating {{Variation}} in {{Replicability}}: {{A}} ``{{Many Labs}}'' {{Replication Project}}},
  shorttitle = {Investigating {{Variation}} in {{Replicability}}},
  author = {Klein, Richard A. and Ratliff, Kate A. and Vianello, Michelangelo and Adams, Reginald B. and Bahn{\'i}k, {\v S}t{\v e}p{\'a}n and Bernstein, Michael J. and Bocian, Konrad and Brandt, Mark J. and Brooks, Beach and Brumbaugh, Claudia Chloe and Cemalcilar, Zeynep and Chandler, Jesse and Cheong, Winnee and Davis, William E. and Devos, Thierry and Eisner, Matthew and Frankowska, Natalia and Furrow, David and Galliani, Elisa Maria and Hasselman, Fred and Hicks, Joshua A. and Hovermale, James F. and Hunt, S. Jane and Huntsinger, Jeffrey R. and IJzerman, Hans and John, Melissa-Sue and {Joy-Gaba}, Jennifer A. and Barry Kappes, Heather and Krueger, Lacy E. and Kurtz, Jaime and Levitan, Carmel A. and Mallett, Robyn K. and Morris, Wendy L. and Nelson, Anthony J. and Nier, Jason A. and Packard, Grant and Pilati, Ronaldo and Rutchick, Abraham M. and Schmidt, Kathleen and Skorinko, Jeanine L. and Smith, Robert and Steiner, Troy G. and Storbeck, Justin and Van Swol, Lyn M. and Thompson, Donna and {van `t Veer}, A. E. and Ann Vaughn, Leigh and Vranka, Marek and Wichman, Aaron L. and Woodzicka, Julie A. and Nosek, Brian A.},
  year = {2014},
  month = may,
  journal = {Social Psychology},
  volume = {45},
  number = {3},
  pages = {142--152},
  issn = {1864-9335, 2151-2590},
  doi = {10.1027/1864-9335/a000178},
  urldate = {2018-06-28},
  langid = {english},
  file = {C\:\\Users\\u237972\\Zotero\\storage\\ABS89NYL\\2014-20922-002.html}
}

@article{10.1027/2151-2604/a000511,
  title = {Meta-{{Analytic Structural Equation Modeling With Fallible Measurements}}},
  author = {Gnambs, Timo and Sengewald, Marie-Ann},
  year = {2023},
  month = feb,
  journal = {Zeitschrift f\"ur Psychologie},
  volume = {231},
  number = {1},
  pages = {39--52},
  issn = {2190-8370, 2151-2604},
  doi = {10.1027/2151-2604/a000511},
  urldate = {2023-05-25},
  abstract = {Abstract. Meta-analytic structural equation modeling (MASEM) combines the strengths of meta-analysis with the flexibility of path models to address multivariate research questions using summary statistics. Because many research questions refer to latent constructs, measurement error can distort effect estimates in MASEMs if the unreliability of study variables is not properly acknowledged. Therefore, a comprehensive Monte Carlo simulation evaluated the impact of measurement error on MASEM results for different mediation models. These analyses showed that point estimates in MASEM were distorted by up to a third of the true effect, while confidence intervals exhibited undercoverage that were less than 10\% in some situations. However, the use of adjustments for attenuation facilitated recovering largely undistorted point and interval estimates in MASEMs. These findings emphasize that MASEMs with fallible measurements can often yield highly distorted results. We encourage applied researchers to regularly adopt adjustment methods that account for attenuation in MASEMs.},
  langid = {english},
  file = {C\:\\Users\\u237972\\Zotero\\storage\\RWDVF6V3\\Meta-analytic structural equation modeling with fallible measurements.pdf;C\:\\Users\\u237972\\Zotero\\storage\\W59GL8MW\\2023-51192-005.html}
}

@article{10.1037/0003-066X.42.5.443,
  title = {How Hard Is Hard Science, How Soft Is Soft Science? {{The}} Empirical Cumulativeness of Research.},
  shorttitle = {How Hard Is Hard Science, How Soft Is Soft Science?},
  author = {Hedges, Larry V.},
  year = {1987},
  month = may,
  journal = {American Psychologist},
  volume = {42},
  number = {5},
  pages = {443--455},
  issn = {1935-990X, 0003-066X},
  doi = {10.1037/0003-066X.42.5.443},
  urldate = {2023-06-01},
  langid = {english},
  file = {C\:\\Users\\u237972\\Zotero\\storage\\SHI5BMAH\\Hedges_1987_How hard is hard science, how soft is soft science.pdf}
}

@article{10.1037/0021-9010.76.3.432,
  title = {A New Meta-Analytic Approach.},
  author = {Raju, Nambury S. and Burke, Michael J. and Normand, Jacques and Langlois, George M.},
  year = {1991},
  month = jun,
  journal = {Journal of Applied Psychology},
  volume = {76},
  number = {3},
  pages = {432--446},
  issn = {1939-1854, 0021-9010},
  doi = {10.1037/0021-9010.76.3.432},
  urldate = {2023-06-07},
  langid = {english},
  file = {C\:\\Users\\u237972\\Zotero\\storage\\XHBEL87S\\Raju et al_1991_A new meta-analytic approach.pdf}
}

@article{10.1037/0021-9010.87.2.377,
  title = {Comparison of Two Random-Effects Methods of Meta-Analysis.},
  author = {Hall, Steven M. and Brannick, Michael T.},
  year = {2002},
  journal = {Journal of Applied Psychology},
  volume = {87},
  number = {2},
  pages = {377--389},
  issn = {1939-1854, 0021-9010},
  doi = {10.1037/0021-9010.87.2.377},
  urldate = {2023-04-24},
  langid = {english},
  file = {C\:\\Users\\u237972\\Zotero\\storage\\E2E8ISGM\\Hall and Brannick - 2002 - Comparison of two random-effects methods of meta-a.pdf}
}

@article{10.1037/1082-989X.10.1.40,
  title = {Meta-Analytic Structural Equation Modeling: {{A}} Two-Stage Approach.},
  shorttitle = {Meta-Analytic Structural Equation Modeling},
  author = {Cheung, Mike W. L. and Chan, Wai},
  year = {2005},
  month = mar,
  journal = {Psychological Methods},
  volume = {10},
  number = {1},
  pages = {40--64},
  issn = {1939-1463, 1082-989X},
  doi = {10.1037/1082-989X.10.1.40},
  urldate = {2023-06-17},
  langid = {english}
}

@article{10.1037/1082-989X.10.2.206,
  title = {The {{Correction}} for {{Attenuation Due}} to {{Measurement Error}}: {{Clarifying Concepts}} and {{Creating Confidence Sets}}},
  shorttitle = {The {{Correction}} for {{Attenuation Due}} to {{Measurement Error}}},
  author = {Charles, Eric P.},
  year = {2005},
  journal = {Psychological Methods},
  volume = {10},
  pages = {206--226},
  publisher = {{American Psychological Association}},
  address = {{US}},
  issn = {1939-1463},
  doi = {10.1037/1082-989X.10.2.206},
  abstract = {The correction for attenuation due to measurement error (CAME) has received many historical criticisms, most of which can be traced to the limited ability to use CAME inferentially. Past attempts to determine confidence intervals for CAME are summarized and their limitations discussed. The author suggests that inference requires confidence sets that demarcate those population parameters likely to have produced an obtained value-rather than indicating the samples likely to be produced by a given population-and that most researchers tend to confuse these 2 types of confidence sets. Three different Monte-Carlo methods are presented, each offering a different way of examining confidence sets under the new conceptualization. Exploring the implications of these approaches for CAME suggests potential consequences for other statistics. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  keywords = {Confidence Limits (Statistics),Error of Measurement,Inference,Statistical Correlation},
  file = {C\:\\Users\\u237972\\Zotero\\storage\\MK4NFJYX\\2005-07009-005.html}
}

@article{10.1037/1082-989X.10.4.444,
  title = {Is the {{Meta-Analysis}} of {{Correlation Coefficients Accurate When Population Correlations Vary}}?},
  author = {Field, Andy P.},
  year = {2005},
  month = dec,
  journal = {Psychological Methods},
  volume = {10},
  number = {4},
  pages = {444--467},
  issn = {1939-1463, 1082-989X},
  doi = {10.1037/1082-989X.10.4.444},
  urldate = {2023-03-30},
  langid = {english},
  file = {C\:\\Users\\u237972\\Zotero\\storage\\VXUD4RLM\\Field - 2005 - Is the Meta-Analysis of Correlation Coefficients A.pdf}
}

@article{10.1037/1082-989X.3.4.486,
  title = {Fixed- and Random-Effects Models in Meta-Analysis},
  author = {Hedges, Larry V. and Vevea, Jack L.},
  year = {1998},
  journal = {Psychological Methods},
  volume = {3},
  pages = {486--504},
  publisher = {{American Psychological Association}},
  address = {{US}},
  issn = {1939-1463},
  doi = {10.1037/1082-989X.3.4.486},
  abstract = {There are 2 families of statistical procedures in meta-analysis: fixed- and random-effects procedures. They were developed for somewhat different inference goals: making inferences about the effect parameters in the studies that have been observed versus making inferences about the distribution of effect parameters in a population of studies from a random sample of studies. The authors evaluate the performance of confidence intervals and hypothesis tests when each type of statistical procedure is used for each type of inference and confirm that each procedure is best for making the kind of inference for which it was designed. Conditionally random-effects procedures (a hybrid type) are shown to have properties in between those of fixed- and random-effects procedures. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  keywords = {Confidence Limits (Statistics),Hypothesis Testing,Meta Analysis,Models,Statistical Analysis},
  file = {C\:\\Users\\u237972\\Zotero\\storage\\WSC28I6P\\1998-11538-006.html}
}

@article{10.1037/1082-989X.7.2.147,
  title = {Missing Data: {{Our}} View of the State of the Art.},
  shorttitle = {Missing Data},
  author = {Schafer, Joseph L. and Graham, John W.},
  year = {2002},
  journal = {Psychological Methods},
  volume = {7},
  number = {2},
  pages = {147--177},
  issn = {1939-1463, 1082-989X},
  doi = {10.1037/1082-989X.7.2.147},
  urldate = {2023-06-17},
  langid = {english}
}

@article{10.1037/a0014697,
  title = {Meta-Analysis of Correlations Revisited: {{Attempted}} Replication and Extension of {{Field}}'s (2001) Simulation Studies},
  shorttitle = {Meta-Analysis of Correlations Revisited},
  author = {Hafdahl, Adam R. and Williams, Michelle A.},
  year = {2009},
  journal = {Psychological Methods},
  volume = {14},
  pages = {24--42},
  publisher = {{American Psychological Association}},
  address = {{US}},
  issn = {1939-1463},
  doi = {10.1037/a0014697},
  abstract = {In 2 Monte Carlo studies of fixed- and random-effects meta-analysis for correlations, A. P. Field (2001) ostensibly evaluated Hedges\textendash Olkin\textendash Vevea Fisher-{$\Elzrtlz$} and Schmidt\textendash Hunter Pearson-r estimators and tests in 120 conditions. Some authors have cited those results as evidence not to meta-analyze Fisher-{$\Elzrtlz$} correlations, especially with heterogeneous correlation parameters. The present attempt to replicate Field's simulations included comparisons with analytic values as well as results for efficiency and confidence-interval coverage. Field's results under homogeneity were mostly replicable, but those under heterogeneity were not: The latter exhibited up to over .17 more bias than ours and, for tests of the mean correlation and homogeneity, respectively, nonnull rejection rates up to .60 lower and .65 higher. Changes to Field's observations and conclusions are recommended, and practical guidance is offered regarding simulation evidence and choices among methods. Most cautions about poor performance of Fisher-{$\Elzrtlz$}methods are largely unfounded, especially with a more appropriate {$\Elzrtlz$}-to-r transformation. The Appendix gives a computer program for obtaining Pearson-r moments from a normal Fisher-{$\Elzrtlz$} distribution, which is used to demonstrate distortion due to direct {$\Elzrtlz$}-to-r transformation of a mean Fisher-{$\Elzrtlz$} correlation. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  keywords = {Meta Analysis,Simulation,Statistical Correlation,Statistical Validity},
  file = {C\:\\Users\\u237972\\Zotero\\storage\\NCNKXJ4S\\Hafdahl_Williams_2009_Meta-analysis of correlations revisited.pdf;C\:\\Users\\u237972\\Zotero\\storage\\GBF5NEX8\\2009-02702-002.html}
}

@article{10.1037/bul0000294,
  title = {Heterogeneity in Direct Replications in Psychology and Its Association with Effect Size},
  author = {{Olsson-Collentine}, Anton and Wicherts, Jelte M. and {van Assen}, Marcel A. L. M.},
  year = {2020},
  month = jul,
  journal = {Psychological Bulletin},
  volume = {146},
  number = {10},
  pages = {922--940},
  issn = {1939-1455},
  doi = {10.1037/bul0000294},
  abstract = {We examined the evidence for heterogeneity (of effect sizes) when only minor changes to sample population and settings were made between studies and explored the association between heterogeneity and average effect size in a sample of 68 meta-analyses from 13 preregistered multilab direct replication projects in social and cognitive psychology. Among the many examined effects, examples include the Stroop effect, the "verbal overshadowing" effect, and various priming effects such as "anchoring" effects. We found limited heterogeneity; 48/68 (71\%) meta-analyses had nonsignificant heterogeneity, and most (49/68; 72\%) were most likely to have zero to small heterogeneity. Power to detect small heterogeneity (as defined by Higgins, Thompson, Deeks, \& Altman, 2003) was low for all projects (mean 43\%), but good to excellent for medium and large heterogeneity. Our findings thus show little evidence of widespread heterogeneity in direct replication studies in social and cognitive psychology, suggesting that minor changes in sample population and settings are unlikely to affect research outcomes in these fields of psychology. We also found strong correlations between observed average effect sizes (standardized mean differences and log odds ratios) and heterogeneity in our sample. Our results suggest that heterogeneity and moderation of effects is unlikely for a 0 average true effect size, but increasingly likely for larger average true effect size. (PsycInfo Database Record (c) 2020 APA, all rights reserved).},
  langid = {english},
  pmid = {32700942}
}

@article{10.1037/cap0000220,
  title = {Measurement Practices in Large-Scale Replications: {{Insights}} from {{Many Labs}} 2.},
  shorttitle = {Measurement Practices in Large-Scale Replications},
  author = {Shaw, Mairead and Cloos, Leonie J. R. and Luong, Raymond and Elbaz, Sasha and Flake, Jessica Kay},
  year = {2020},
  month = nov,
  journal = {Canadian Psychology/Psychologie canadienne},
  volume = {61},
  number = {4},
  pages = {289--298},
  issn = {1878-7304, 0708-5591},
  doi = {10.1037/cap0000220},
  urldate = {2021-01-25},
  langid = {english},
  file = {C\:\\Users\\u237972\\Zotero\\storage\\K5AZSZ4V\\Shaw et al_2020_Measurement practices in large-scale replications.pdf}
}

@article{10.1037/met0000023,
  title = {A Comparison of Procedures to Test for Moderators in Mixed-Effects Meta-Regression Models.},
  author = {Viechtbauer, Wolfgang and {L{\'o}pez-L{\'o}pez}, Jos{\'e} Antonio and {S{\'a}nchez-Meca}, Julio and {Mar{\'i}n-Mart{\'i}nez}, Fulgencio},
  year = {2015},
  journal = {Psychological Methods},
  volume = {20},
  number = {3},
  pages = {360--374},
  issn = {1939-1463, 1082-989X},
  doi = {10.1037/met0000023},
  urldate = {2023-06-17},
  langid = {english}
}

@article{10.1037/met0000197,
  title = {The Effect of Publication Bias on the {{Q}} Test and Assessment of Heterogeneity},
  author = {Augusteijn, Hilde E. M. and {van Aert}, Robbie C. M. and {van Assen}, Marcel A. L. M.},
  year = {2019},
  month = feb,
  journal = {Psychological Methods},
  volume = {24},
  number = {1},
  pages = {116--134},
  issn = {1939-1463},
  doi = {10.1037/met0000197},
  abstract = {One of the main goals of meta-analysis is to test for and estimate the heterogeneity of effect sizes. We examined the effect of publication bias on the Q test and assessments of heterogeneity as a function of true heterogeneity, publication bias, true effect size, number of studies, and variation of sample sizes. The present study has two main contributions and is relevant to all researchers conducting meta-analysis. First, we show when and how publication bias affects the assessment of heterogeneity. The expected values of heterogeneity measures H{$^2$} and I{$^2$} were analytically derived, and the power and Type I error rate of the Q test were examined in a Monte Carlo simulation study. Our results show that the effect of publication bias on the Q test and assessment of heterogeneity is large, complex, and nonlinear. Publication bias can both dramatically decrease and increase heterogeneity in true effect size, particularly if the number of studies is large and population effect size is small. We therefore conclude that the Q test of homogeneity and heterogeneity measures H{$^2$} and I{$^2$} are generally not valid when publication bias is present. Our second contribution is that we introduce a web application, Q-sense, which can be used to determine the impact of publication bias on the assessment of heterogeneity within a certain meta-analysis and to assess the robustness of the meta-analytic estimate to publication bias. Furthermore, we apply Q-sense to 2 published meta-analyses, showing how publication bias can result in invalid estimates of effect size and heterogeneity. (PsycINFO Database Record (c) 2019 APA, all rights reserved).},
  langid = {english},
  pmid = {30489099},
  keywords = {{Data Interpretation, Statistical},Humans,Meta-Analysis as Topic,Normal Distribution,Publication Bias},
  file = {C\:\\Users\\u237972\\Zotero\\storage\\P92CALTI\\Augusteijn et al_2019_The effect of publication bias on the Q test and assessment of heterogeneity.pdf}
}

@article{10.1037/met0000498,
  title = {Meta-Analysis of Correlation Coefficients: {{A}} Cautionary Tale on Treating Measurement Error},
  shorttitle = {Meta-Analysis of Correlation Coefficients},
  author = {Zhang, Qian},
  year = {2022},
  month = may,
  journal = {Psychological Methods},
  issn = {1939-1463},
  doi = {10.1037/met0000498},
  abstract = {A scale to measure a psychological construct is subject to measurement error. When meta-analyzing correlations obtained from scale scores, many researchers recommend correcting for measurement error. I considered three caveats when correcting for measurement error in meta-analysis of correlations: (a) the distribution of true scores can be non-normal, resulting in violation of the normality assumption for raw correlations and Fisher's z transformed correlations; (b) coefficient alpha is often used as the reliability, but correlations corrected for measurement error using alpha can be inaccurate when some assumptions of alpha (e.g., tau-equivalence) are violated; and (c) item scores are often ordinal, making the disattenuation formula potentially problematic. Via three simulation studies, I examined the performance of two meta-analysis approaches-with raw correlations and z scores. In terms of estimation accuracy and coverage probability of the mean correlation, results showed that (a) considering the true-score distribution alone, estimation of the mean correlation was slightly worse when true scores of the constructs were skewed rather than normal; (b) when the tau-equivalence assumption was violated and coefficient alpha was used for correcting measurement error, the mean correlation estimates can be biased and coverage probabilities can be low; and (c) discretization of continuous items can result in biased estimates and undercoverage of the mean correlations even when tau-equivalence was satisfied. With more categories and/or items on a scale, results can improve whether tau-equivalence was met or not. Based on these findings, I gave recommendations for conducting meta-analyses of correlations. (PsycInfo Database Record (c) 2022 APA, all rights reserved).},
  langid = {english},
  pmid = {35604700},
  file = {C\:\\Users\\u237972\\Zotero\\storage\\WTT2E834\\Zhang_2022_Meta-analysis of correlation coefficients.pdf}
}

@article{10.1037/pas0000754,
  title = {Reliability from {$\alpha$} to {$\omega$}: {{A}} Tutorial.},
  shorttitle = {Reliability from {$\alpha$} to {$\omega$}},
  author = {Revelle, William and Condon, David M.},
  year = {2019},
  month = dec,
  journal = {Psychological Assessment},
  volume = {31},
  number = {12},
  pages = {1395--1411},
  issn = {1939-134X, 1040-3590},
  doi = {10.1037/pas0000754},
  urldate = {2023-05-31},
  langid = {english},
  file = {C\:\\Users\\u237972\\Zotero\\storage\\GDWMSH9G\\Revelle_Condon_2019_Reliability from α to ω.pdf}
}

@article{10.1038/s41562-019-0787-z,
  title = {Comparing Meta-Analyses and Preregistered Multiple-Laboratory Replication Projects},
  author = {Kvarven, Amanda and Str{\o}mland, Eirik and Johannesson, Magnus},
  year = {2019},
  month = dec,
  journal = {Nature Human Behaviour},
  volume = {4},
  pages = {423--434},
  issn = {2397-3374},
  doi = {10.1038/s41562-019-0787-z},
  urldate = {2020-03-09},
  langid = {english}
}

@article{10.1111/bmsp.12242,
  title = {Fisher Transformation Based Confidence Intervals of Correlations in Fixed- and Random-Effects Meta-Analysis},
  author = {Welz, Thilo and Doebler, Philipp and Pauly, Markus},
  year = {2022},
  journal = {British Journal of Mathematical and Statistical Psychology},
  volume = {75},
  number = {1},
  pages = {1--22},
  issn = {2044-8317},
  doi = {10.1111/bmsp.12242},
  urldate = {2023-05-21},
  abstract = {Meta-analyses of correlation coefficients are an important technique to integrate results from many cross-sectional and longitudinal research designs. Uncertainty in pooled estimates is typically assessed with the help of confidence intervals, which can double as hypothesis tests for two-sided hypotheses about the underlying correlation. A standard approach to construct confidence intervals for the main effect is the Hedges-Olkin-Vevea Fisher-z (HOVz) approach, which is based on the Fisher-z transformation. Results from previous studies (Field, 2005, Psychol. Meth., 10, 444; Hafdahl and Williams, 2009, Psychol. Meth., 14, 24), however, indicate that in random-effects models the performance of the HOVz confidence interval can be unsatisfactory. To this end, we propose improvements of the HOVz approach, which are based on enhanced variance estimators for the main effect estimate. In order to study the coverage of the new confidence intervals in both fixed- and random-effects meta-analysis models, we perform an extensive simulation study, comparing them to established approaches. Data were generated via a truncated normal and beta distribution model. The results show that our newly proposed confidence intervals based on a Knapp-Hartung-type variance estimator or robust heteroscedasticity consistent sandwich estimators in combination with the integral z-to-r transformation (Hafdahl, 2009, Br. J. Math. Stat. Psychol., 62, 233) provide more accurate coverage than existing approaches in most scenarios, especially in the more appropriate beta distribution simulation model.},
  langid = {english},
  keywords = {confidence intervals,correlations,Fisher's z transformation,meta-analysis,Monte-Carlo-simulation}
}

@article{10.1111/iops.12184,
  title = {Corrections for {{Criterion Reliability}} in {{Validity Generalization}}: {{A False Prophet}} in a {{Land}} of {{Suspended Judgment}}},
  shorttitle = {Corrections for {{Criterion Reliability}} in {{Validity Generalization}}},
  author = {LeBreton, James M. and Scherer, Kelly T. and James, Lawrence R.},
  year = {2014},
  journal = {Industrial and Organizational Psychology},
  volume = {7},
  number = {4},
  pages = {478--500},
  issn = {1754-9434},
  doi = {10.1111/iops.12184},
  urldate = {2023-05-08},
  abstract = {The results of meta-analytic (MA) and validity generalization (VG) studies continue to be impressive. In contrast to earlier findings that capped the variance accounted for in job performance at roughly 16\%, many recent studies suggest that a single predictor variable can account for between 16 and 36\% of the variance in some aspect of job performance. This article argues that this ``enhancement'' in variance accounted for is often attributable not to improvements in science but to a dumbing down of the standards for the values of statistics used in correction equations. With rare exceptions, applied researchers have suspended judgment about what is and is not an acceptable threshold for criterion reliability in their quest for higher validities. We demonstrate a statistical dysfunction that is a direct result of using low criterion reliabilities in corrections for attenuation. Corrections typically applied to a single predictor in a VG study are instead applied to multiple predictors. A multiple correlation analysis is then conducted on corrected validity coefficients. It is shown that the corrections often used in single predictor studies yield a squared multiple correlation that appears suspect. Basically, the multiple predictor study exposes the tenuous statistical foundation of using abjectly low criterion reliabilities in single predictor VG studies. Recommendations for restoring scientific integrity to the meta-analyses that permeate industrial\textendash organizational (I\textendash O) psychology are offered.},
  langid = {english},
  file = {C\:\\Users\\u237972\\Zotero\\storage\\QDNFVDBT\\LeBreton et al_2014_Corrections for Criterion Reliability in Validity Generalization.pdf;C\:\\Users\\u237972\\Zotero\\storage\\ZKWCDDR2\\iops.html}
}

@article{10.1111/j.1745-3992.2007.00099.x,
  title = {Introduction to {{Structural Equation Modeling}}: {{Issues}} and {{Practical Considerations}}},
  shorttitle = {Introduction to {{Structural Equation Modeling}}},
  author = {Lei, Pui-Wa and Wu, Qiong},
  year = {2007},
  journal = {Educational Measurement: Issues and Practice},
  volume = {26},
  number = {3},
  pages = {33--43},
  issn = {1745-3992},
  doi = {10.1111/j.1745-3992.2007.00099.x},
  urldate = {2023-06-07},
  abstract = {Structural equation modeling (SEM) is a versatile statistical modeling tool. Its estimation techniques, modeling capacities, and breadth of applications are expanding rapidly. This module introduces some common terminologies. General steps of SEM are discussed along with important considerations in each step. Simple examples are provided to illustrate some of the ideas for beginners. In addition, several popular specialized SEM software programs are briefly discussed with regard to their features and availability. The intent of this module is to focus on foundational issues to inform readers of the potentials as well as the limitations of SEM. Interested readers are encouraged to consult additional references for advanced model types and more application examples.},
  langid = {english},
  keywords = {measurement model,path model,structural equation modeling},
  file = {C\:\\Users\\u237972\\Zotero\\storage\\MHJZ97NC\\j.1745-3992.2007.00099.html}
}

@article{10.1111/j.2044-8317.2012.02057.xa,
  title = {Some Recommended Statistical Analytic Practices When Reliability Generalization Studies Are Conducted},
  author = {{S{\'a}nchez-Meca}, Julio and {L{\'o}pez-L{\'o}pez}, Jos{\'e} Antonio and {L{\'o}pez-Pina}, Jos{\'e} Antonio},
  year = {2012},
  month = sep,
  journal = {British Journal of Mathematical and Statistical Psychology},
  pages = {n/a-n/a},
  issn = {00071102},
  doi = {10.1111/j.2044-8317.2012.02057.x},
  urldate = {2023-06-17},
  langid = {english}
}

@article{10.1136/bmj.327.7414.557,
  title = {Measuring Inconsistency in Meta-Analyses},
  author = {Higgins, J. P T},
  year = {2003},
  month = sep,
  journal = {BMJ},
  volume = {327},
  number = {7414},
  pages = {557--560},
  issn = {0959-8138, 1468-5833},
  doi = {10.1136/bmj.327.7414.557},
  urldate = {2018-06-28},
  langid = {english},
  file = {C\:\\Users\\u237972\\Zotero\\storage\\2X4K23B8\\search.crossref.org.html}
}

@article{10.1136/bmj.39343.408449.80,
  title = {Uncertainty in Heterogeneity Estimates in Meta-Analyses},
  author = {Ioannidis, John P A and Patsopoulos, Nikolaos A and Evangelou, Evangelos},
  year = {2007},
  month = nov,
  journal = {BMJ},
  volume = {335},
  number = {7626},
  pages = {914--916},
  issn = {0959-8138, 1468-5833},
  doi = {10.1136/bmj.39343.408449.80},
  urldate = {2018-06-28},
  langid = {english}
}

@article{10.1177/00131640021970691,
  title = {Reliability {{Methods}}: {{A Note}} on the {{Frequency}} of {{Use}} of {{Various Types}}},
  shorttitle = {Reliability {{Methods}}},
  author = {Hogan, Thomas P. and Benjamin, Amy and Brezinski, Kristen L.},
  year = {2000},
  month = aug,
  journal = {Educational and Psychological Measurement},
  volume = {60},
  number = {4},
  pages = {523--531},
  publisher = {{SAGE Publications Inc}},
  issn = {0013-1644},
  doi = {10.1177/00131640021970691},
  urldate = {2023-05-31},
  abstract = {This study examined the frequency of use of various types of reliability coefficients for a systematically drawn sample of 696 tests appearing in the APA-published Directory of Unpublished Experimental Mental Measures. Almost all articles included some type of reliability report for at least one test administration. Coefficient alpha was the over-whelming favorite among types of coefficients. Several measures treated almost universally in psychological-testing textbooks were rarely or never used. Problems encountered in the study included ambiguous designations of types of coefficients, reporting reliability based on a study other than the one cited, inadequate information about subscales, and simply incorrect recording of the information given in an original source.},
  langid = {english},
  file = {C\:\\Users\\u237972\\Zotero\\storage\\B9B9UD4J\\Hogan et al_2000_Reliability Methods.pdf}
}

@article{10.1177/00131640121971365,
  title = {Confidence {{Intervals About Score Reliability Coefficients}}, {{Please}}: {{An EPM Guidelines Editorial}}},
  shorttitle = {Confidence {{Intervals About Score Reliability Coefficients}}, {{Please}}},
  author = {Fan, X. and Thompson, B.},
  year = {2001},
  month = aug,
  journal = {Educational and Psychological Measurement},
  volume = {61},
  number = {4},
  pages = {517--531},
  issn = {00000000, 00131644},
  doi = {10.1177/00131640121971365},
  urldate = {2023-06-17},
  file = {C\:\\Users\\u237972\\Zotero\\storage\\5J3HSQ9K\\Fan_Thompson_2001_Confidence intervals for effect sizes.pdf}
}

@article{10.1177/001316402236878,
  title = {Reliability: {{Arguments}} for {{Multiple Perspectives}} and {{Potential Problems}} with {{Generalization}} across {{Studies}}},
  shorttitle = {Reliability},
  author = {Dimitrov, Dimiter M.},
  year = {2002},
  month = oct,
  journal = {Educational and Psychological Measurement},
  volume = {62},
  number = {5},
  pages = {783--801},
  publisher = {{SAGE Publications Inc}},
  issn = {0013-1644},
  doi = {10.1177/001316402236878},
  urldate = {2023-05-31},
  abstract = {The present article addresses reliability issues in light of recent studies and debates focused on psychometrics versus datametrics terminology and reliability generalization (RG) introduced by Vacha-Haase. The purpose here was not to moderate arguments presented in these debates but to discuss multiple perspectives on score reliability and how they may affect research practice, editorial policies, and RG across studies. Issues of classical error variance and reliability are discussed across models of classical test theory, generalizability theory, and item response theory. Potential problems with RG across studies are discussed in relation to different types of reliability, different test forms, different number of items, misspecifications, and confounding independent variables in a single RG analysis.},
  langid = {english},
  file = {C\:\\Users\\u237972\\Zotero\\storage\\VZ6EGQZ3\\Dimitrov_2002_Reliability.pdf}
}

@article{10.1177/0146621602239476,
  title = {Determining the {{Significance}} of {{Correlations Corrected}} for {{Unreliability}} and {{Range Restriction}}},
  author = {Raju, Nambury S. and Brand, Paul A.},
  year = {2003},
  month = jan,
  journal = {Applied Psychological Measurement},
  volume = {27},
  number = {1},
  pages = {52--71},
  publisher = {{SAGE Publications Inc}},
  issn = {0146-6216},
  doi = {10.1177/0146621602239476},
  urldate = {2023-06-07},
  abstract = {A new asymptotic formula for estimating the sampling variance of a correlation coefficient corrected for unreliability and range restriction was proposed. A Monte Carlo assessment of the new sampling variance formula has resulted in the following conclusions. First, the formula-based (analytical) sampling variances were very close to the empirically derived sampling variances based on 5,000 replications. Second, the sampling variance formula was quite robust against committing Type I errors. Third, the statistical power was low to moderate in distinguishing between two unattenuated and unrestricted population correlations. Fourth, the new formula produced smaller sampling variances; was closer to nominal alpha levels; and was more powerful when sample size increased, when the population correlation coefficient increased, when range restriction was less severe, and when both the criterion and predictor reliabilities increased.},
  langid = {english}
}

@article{10.1177/0149206310377113,
  title = {Meta-{{Analytic Choices}} and {{Judgment Calls}}: {{Implications}} for {{Theory Building}} and {{Testing}}, {{Obtained Effect Sizes}}, and {{Scholarly Impact}}},
  shorttitle = {Meta-{{Analytic Choices}} and {{Judgment Calls}}},
  author = {Aguinis, Herman and Dalton, Dan R. and Bosco, Frank A. and Pierce, Charles A. and Dalton, Catherine M.},
  year = {2011},
  month = jan,
  journal = {Journal of Management},
  volume = {37},
  number = {1},
  pages = {5--38},
  issn = {0149-2063, 1557-1211},
  doi = {10.1177/0149206310377113},
  urldate = {2023-06-17},
  abstract = {The authors content analyzed 196 meta-analyses including 5,581 effect-size estimates published in Academy of Management Journal, Journal of Applied Psychology, Journal of Management, Personnel Psychology, and Strategic Management Journal from January 1982 through August 2009 to assess the presumed effects of each of 21 methodological choices and judgment calls on substantive conclusions. Results indicate that, overall, the various meta-analytic methodological choices available and judgment calls involved in the conduct of a meta-analysis have little impact on the resulting magnitude of the meta-analytically derived effect sizes. Thus, the present study, based on actual meta-analyses, casts doubt on previous warnings, primarily based on selective case studies, that judgment calls have an important impact on substantive conclusions. The authors also tested the fit of a multivariate model that includes relationships among theory-building and theory-testing goals, obtained effect sizes, year of publication of the meta-analysis, and scholarly impact (i.e., citations per year). Results indicate that the more a meta-analysis attempts to test an existing theory, the larger the number of citations, whereas the more a meta-analysis attempts to build new theory, the lower the number of citations. Also, in support of scientific particularism, as opposed to scientific universalism, the magnitude of the derived effects is not related to the extent to which a meta-analysis is cited. Taken together, the results provide a comprehensive data-based understanding of how meta-analytic reviews are conducted and the implications of these practices for theory building and testing, obtained effect sizes, and scholarly impact.},
  langid = {english}
}

@article{10.1177/0956797611417632,
  title = {False-{{Positive Psychology}}: {{Undisclosed Flexibility}} in {{Data Collection}} and {{Analysis Allows Presenting Anything}} as {{Significant}}},
  shorttitle = {False-{{Positive Psychology}}},
  author = {Simmons, Joseph P. and Nelson, Leif D. and Simonsohn, Uri},
  year = {2011},
  month = nov,
  journal = {Psychological Science},
  volume = {22},
  number = {11},
  pages = {1359--1366},
  issn = {0956-7976, 1467-9280},
  doi = {10.1177/0956797611417632},
  urldate = {2018-06-28},
  langid = {english},
  file = {C\:\\Users\\u237972\\Zotero\\storage\\TFKK7VBQ\\search.crossref.org.html}
}

@article{10.1177/1094428103257358,
  title = {Apples and {{Oranges}} (and {{Pears}}, {{Oh My}}!): {{The Search}} for {{Moderators}} in {{Meta-Analysis}}},
  shorttitle = {Apples and {{Oranges}} (and {{Pears}}, {{Oh My}}!)},
  author = {Cortina, Jose M.},
  year = {2003},
  month = oct,
  journal = {Organizational Research Methods},
  volume = {6},
  number = {4},
  pages = {415--439},
  issn = {1094-4281, 1552-7425},
  doi = {10.1177/1094428103257358},
  urldate = {2023-06-17},
  abstract = {The purpose of this article is to review current practices with respect to detection and estimation of moderators in meta-analysis and to develop recommendations that are driven by the results of this review and previous research. The first purpose was accomplished through a review of the meta-analyses published in Journal of Applied Psychology from 1978 to 1997. Results show, first, that practices with respect to both the execution of and the reporting of results from searches for moderators are highly variable and, second, that findings relevant for detection of moderators (e.g., percentage variance attributable to artifacts, SD{$\rho$}, etc.) are often highly inconsistent with what has been suggested in the past. These practices held regardless of time of publication, specificity of the question addressed in the paper, and content area. Detailed suggestions for modifications of current practices are offered.},
  langid = {english}
}

@article{10.1177/1094428114563617,
  title = {Are We Correcting Correctly?: {{Interdependence}} of Reliabilities in Meta-Analysis},
  shorttitle = {Are We Correcting Correctly?},
  author = {K{\"o}hler, Tine and Cortina, Jose M. and Kurtessis, James N. and G{\"o}lz, Markus},
  year = {2015},
  journal = {Organizational Research Methods},
  volume = {18},
  pages = {355--428},
  publisher = {{Sage Publications}},
  address = {{US}},
  issn = {1552-7425},
  doi = {10.1177/1094428114563617},
  abstract = {The formulae for attenuation correction in meta-analysis treat reliabilities as if they were independent of each other. The current study puts this assumption of independence to the test by empirically examining the correlation among predictor and criterion reliability estimates across studies. Interdependence of reliabilities would result in either overestimation or underestimation of population correlations depending on the direction of the relationship between the reliabilities. We conducted two studies to examine the extent to which predictor and criterion reliabilities correlate across studies. Study 1 is based on 628 pairs of reliability estimates from 518 studies published in the Academy of Management Journal and the Journal of Applied Psychology between 2004 and 2011, while Study 2 is based on 564 pairs of reliability estimates from 347 studies included in a meta-analysis on perceived organizational support (POS) and some of its antecedents and outcomes. The findings in both studies show substantial correlations between predictor and criterion reliability coefficients across studies. Our article discusses important implications from these findings for future research and for the future conduct of meta-analyses. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  keywords = {Experimentation,Interdependence,Mathematics,Meta Analysis,Statistical Regression,Statistical Reliability,Test Reliability},
  file = {C\:\\Users\\u237972\\Zotero\\storage\\DRQZN695\\Köhler et al_2015_Are we correcting correctly.pdf;C\:\\Users\\u237972\\Zotero\\storage\\MGG9XUBJ\\2015-26955-001.html}
}

@article{10.1177/1094428117741966,
  title = {Bias and {{Precision}} of {{Alternate Estimators}} in {{Meta-Analysis}}: {{Benefits}} of {{Blending Schmidt-Hunter}} and {{Hedges Approaches}}},
  shorttitle = {Bias and {{Precision}} of {{Alternate Estimators}} in {{Meta-Analysis}}},
  author = {Brannick, Michael T. and Potter, Sean M. and Benitez, Bryan and Morris, Scott B.},
  year = {2019},
  month = apr,
  journal = {Organizational Research Methods},
  volume = {22},
  number = {2},
  pages = {490--514},
  publisher = {{SAGE Publications Inc}},
  issn = {1094-4281},
  doi = {10.1177/1094428117741966},
  urldate = {2023-04-24},
  abstract = {We describe a new estimator (labeled Morris) for meta-analysis. The Morris estimator combines elements of both the Schmidt-Hunter and Hedges estimators. The new estimator is compared to (a) the Schmidt-Hunter estimator, (b) the Schmidt-Hunter estimator with variance correction for the number of studies (?k correction?), (c) the Hedges random-effects estimator, and (d) the Bonett unit weights estimator in a Monte Carlo simulation. The simulation was designed to represent realistic conditions faced by researchers, including population random-effects distributions, numbers of studies, and skewed sample size distributions. The simulation was used to evaluate the estimators with respect to bias, coverage of the 95\% confidence interval of the mean, and root mean square error of estimates of the population mean. We also evaluated the quality of credibility intervals. Overall, the new estimator provides better coverage and slightly better credibility values than other commonly used methods. Thus it has advantages of both commonly used approaches without the apparent disadvantages. The new estimator can be implemented easily with existing software; software used in the study is available online, and an example is included in the appendix in the Supplemental Material available online.},
  langid = {english},
  file = {C\:\\Users\\u237972\\Zotero\\storage\\367MKD3B\\Brannick et al_2019_Bias and Precision of Alternate Estimators in Meta-Analysis.pdf}
}

@article{10.1177/1745691610369339,
  title = {Detecting and {{Correcting}} the {{Lies That Data Tell}}},
  author = {Schmidt, Frank},
  year = {2010},
  month = may,
  journal = {Perspectives on Psychological Science},
  volume = {5},
  number = {3},
  pages = {233--242},
  publisher = {{SAGE Publications Inc}},
  issn = {1745-6916},
  doi = {10.1177/1745691610369339},
  urldate = {2023-05-10},
  abstract = {Because of the way in which data are typically analyzed and interpreted, they frequently lie to researchers, leading to conclusions that are not only false but more complex than the underlying reality. The several examples of this presented in this article illustrate the possibility that although data may appear to indicate complex phenomena at the surface structure level, the phenomena may be quite simple at the deep structure level, suggesting the possibility of applying Occam's razor to achieve the scientific goal of parsimony. The approaches to data analysis described in this article may also lead to a solution to the serious problem of construct proliferation in psychology by demonstrating that many constructs are redundant with other existing constructs. The major obstacles to these outcomes are researchers' continued reliance on the use of statistical significance testing in data analysis and interpretation and the failure to correct for the distorting effects of sampling error, measurement error, and other artifacts. Some of these problems have been addressed by the now widespread use of meta-analysis, but examination of the meta-analyses appearing in Psychological Bulletin from 1978 to 2006 shows that most employ a statistically inappropriate model for meta-analysis (the fixed effects model) and that 90\% do not correct for the biasing effects of measurement error. Hence, there is still a long way to go in the improvement of data analysis and interpretation methods.},
  langid = {english}
}

@article{10.1177/1745691617708630,
  title = {Constraints on {{Generality}} ({{COG}}): {{A Proposed Addition}} to {{All Empirical Papers}}},
  shorttitle = {Constraints on {{Generality}} ({{COG}})},
  author = {Simons, Daniel J. and Shoda, Yuichi and Lindsay, D. Stephen},
  year = {2017},
  month = nov,
  journal = {Perspectives on Psychological Science},
  volume = {12},
  number = {6},
  pages = {1123--1128},
  issn = {1745-6916, 1745-6924},
  doi = {10.1177/1745691617708630},
  urldate = {2018-06-28},
  langid = {english},
  file = {C\:\\Users\\u237972\\Zotero\\storage\\8LXAEM9W\\search.crossref.org.html}
}

@article{10.1177/1948550617693063,
  title = {Construct {{Validation}} in {{Social}} and {{Personality Research}}: {{Current Practice}} and {{Recommendations}}},
  shorttitle = {Construct {{Validation}} in {{Social}} and {{Personality Research}}},
  author = {Flake, Jessica K. and Pek, Jolynn and Hehman, Eric},
  year = {2017},
  month = may,
  journal = {Social Psychological and Personality Science},
  volume = {8},
  number = {4},
  pages = {370--378},
  issn = {1948-5506},
  doi = {10.1177/1948550617693063},
  urldate = {2018-11-09},
  abstract = {The verity of results about a psychological construct hinges on the validity of its measurement, making construct validation a fundamental methodology to the scientific process. We reviewed a representative sample of articles published in the Journal of Personality and Social Psychology for construct validity evidence. We report that latent variable measurement, in which responses to items are used to represent a construct, is pervasive in social and personality research. However, the field does not appear to be engaged in best practices for ongoing construct validation. We found that validity evidence of existing and author-developed scales was lacking, with coefficient {$\alpha$} often being the only psychometric evidence reported. We provide a discussion of why the construct validation framework is important for social and personality researchers and recommendations for improving practice.},
  langid = {english},
  file = {C\:\\Users\\u237972\\Zotero\\storage\\H3NUPLQF\\Flake et al_2017_Construct Validation in Social and Personality Research.pdf}
}

@article{10.1177/2515245919838781a,
  title = {Research in {{Social Psychology Changed Between}} 2011 and 2016: {{Larger Sample Sizes}}, {{More Self-Report Measures}}, and {{More Online Studies}}},
  shorttitle = {Research in {{Social Psychology Changed Between}} 2011 and 2016},
  author = {Sassenberg, Kai and Ditrich, Lara},
  year = {2019},
  month = jun,
  journal = {Advances in Methods and Practices in Psychological Science},
  volume = {2},
  number = {2},
  pages = {107--114},
  publisher = {{SAGE Publications Inc}},
  issn = {2515-2459},
  doi = {10.1177/2515245919838781},
  urldate = {2023-05-21},
  abstract = {The debate about false positives in psychological research has led to a demand for higher statistical power. To meet this demand, researchers need to collect data from larger samples\textemdash which is important to increase replicability, but can be costly in both time and money (i.e., remuneration of participants). Given that researchers might need to compensate for these higher costs, we hypothesized that larger sample sizes might have been accompanied by more frequent use of less costly research methods (i.e., online data collection and self-report measures). To test this idea, we analyzed social psychology studies published in 2009, 2011, 2016, and 2018. Indeed, research reported in 2016 and 2018 (vs. 2009 and 2011) had larger sample sizes and relied more on online data collection and self-report measures. Thus, over these years, research improved in its statistical power, but also changed with regard to the methods applied. Implications for social psychology as a discipline are discussed.},
  langid = {english}
}

@article{10.1177/2515245919847196a,
  title = {Correcting for {{Bias}} in {{Psychology}}: {{A Comparison}} of {{Meta-Analytic Methods}}},
  shorttitle = {Correcting for {{Bias}} in {{Psychology}}},
  author = {Carter, Evan C. and Sch{\"o}nbrodt, Felix D. and Gervais, Will M. and Hilgard, Joseph},
  year = {2019},
  month = jun,
  journal = {Advances in Methods and Practices in Psychological Science},
  volume = {2},
  number = {2},
  pages = {115--144},
  publisher = {{SAGE Publications Inc}},
  issn = {2515-2459},
  doi = {10.1177/2515245919847196},
  urldate = {2023-04-24},
  abstract = {Publication bias and questionable research practices in primary research can lead to badly overestimated effects in meta-analysis. Methodologists have proposed a variety of statistical approaches to correct for such overestimation. However, it is not clear which methods work best for data typically seen in psychology. Here, we present a comprehensive simulation study in which we examined how some of the most promising meta-analytic methods perform on data that might realistically be produced by research in psychology. We simulated several levels of questionable research practices, publication bias, and heterogeneity, and used study sample sizes empirically derived from the literature. Our results clearly indicated that no single meta-analytic method consistently outperformed all the others. Therefore, we recommend that meta-analysts in psychology focus on sensitivity analyses?that is, report on a variety of methods, consider the conditions under which these methods fail (as indicated by simulation studies such as ours), and then report how conclusions might change depending on which conditions are most plausible. Moreover, given the dependence of meta-analytic methods on untestable assumptions, we strongly recommend that researchers in psychology continue their efforts to improve the primary literature and conduct large-scale, preregistered replications. We provide detailed results and simulation code at https://osf.io/rf3ys and interactive figures at http://www.shinyapps.org/apps/metaExplorer/.},
  langid = {english}
}

@article{10.1177/2515245919879695,
  title = {Psychological {{Science Needs}} a {{Standard Practice}} of {{Reporting}} the {{Reliability}} of {{Cognitive-Behavioral Measurements}}},
  author = {Parsons, Sam and Kruijt, Anne-Wil and Fox, Elaine},
  year = {2019},
  month = dec,
  journal = {Advances in Methods and Practices in Psychological Science},
  volume = {2},
  number = {4},
  pages = {378--395},
  publisher = {{SAGE Publications Inc}},
  issn = {2515-2459},
  doi = {10.1177/2515245919879695},
  urldate = {2023-02-10},
  abstract = {Psychological science relies on behavioral measures to assess cognitive processing; however, the field has not yet developed a tradition of routinely examining the reliability of these behavioral measures. Reliable measures are essential to draw robust inferences from statistical analyses, and subpar reliability has severe implications for measures? validity and interpretation. Without examining and reporting the reliability of measurements used in an analysis, it is nearly impossible to ascertain whether results are robust or have arisen largely from measurement error. In this article, we propose that researchers adopt a standard practice of estimating and reporting the reliability of behavioral assessments of cognitive processing. We illustrate the need for this practice using an example from experimental psychopathology, the dot-probe task, although we argue that reporting reliability is relevant across fields (e.g., social cognition and cognitive psychology). We explore several implications of low measurement reliability and the detrimental impact that failure to assess measurement reliability has on interpretability and comparison of results and therefore research quality. We argue that researchers in the field of cognition need to report measurement reliability as routine practice so that more reliable assessment tools can be developed. To provide some guidance on estimating and reporting reliability, we describe the use of bootstrapped split-half estimation and intraclass correlation coefficients to estimate internal consistency and test-retest reliability, respectively. For future researchers to build upon current results, it is imperative that all researchers provide psychometric information sufficient for estimating the accuracy of inferences and informing further development of cognitive-behavioral assessments.},
  langid = {english}
}

@article{10.1177/2515245919885611,
  title = {Obtaining {{Unbiased Results}} in {{Meta-Analysis}}: {{The Importance}} of {{Correcting}} for {{Statistical Artifacts}}},
  shorttitle = {Obtaining {{Unbiased Results}} in {{Meta-Analysis}}},
  author = {Wiernik, Brenton M. and Dahlke, Jeffrey A.},
  year = {2020},
  month = mar,
  journal = {Advances in Methods and Practices in Psychological Science},
  volume = {3},
  number = {1},
  pages = {94--123},
  publisher = {{SAGE Publications Inc}},
  issn = {2515-2459},
  doi = {10.1177/2515245919885611},
  urldate = {2023-02-10},
  abstract = {Most published meta-analyses address only artifactual variance due to sampling error and ignore the role of other statistical and psychometric artifacts, such as measurement error variance (due to factors including unreliability of measurements, group misclassification, and variable treatment strength) and selection effects (including range restriction or enhancement and collider biases). These artifacts can have severe biasing effects on the results of individual studies and meta-analyses. Failing to account for these artifacts can lead to inaccurate conclusions about the mean effect size and between-studies effect-size heterogeneity, and can influence the results of meta-regression, publication-bias, and sensitivity analyses. In this article, we provide a brief introduction to the biasing effects of measurement error variance and selection effects and their relevance to a variety of research designs. We describe how to estimate the effects of these artifacts in different research designs and correct for their impacts in primary studies and meta-analyses. We consider meta-analyses of correlations, observational group differences, and experimental effects. We provide R code to implement the corrections described.},
  langid = {english},
  file = {C\:\\Users\\u237972\\Zotero\\storage\\CEDVCRRS\\Wiernik_Dahlke_2020_Obtaining Unbiased Results in Meta-Analysis.pdf}
}

@article{10.1177/2515245920951747,
  title = {Your {{Coefficient Alpha Is Probably Wrong}}, but {{Which Coefficient Omega Is Right}}? {{A Tutorial}} on {{Using R}} to {{Obtain Better Reliability Estimates}}},
  shorttitle = {Your {{Coefficient Alpha Is Probably Wrong}}, but {{Which Coefficient Omega Is Right}}?},
  author = {Flora, David B.},
  year = {2020},
  month = dec,
  journal = {Advances in Methods and Practices in Psychological Science},
  volume = {3},
  number = {4},
  pages = {484--501},
  publisher = {{SAGE Publications Inc}},
  issn = {2515-2459},
  doi = {10.1177/2515245920951747},
  urldate = {2023-05-31},
  abstract = {Measurement quality has recently been highlighted as an important concern for advancing a cumulative psychological science. An implication is that researchers should move beyond mechanistically reporting coefficient alpha toward more carefully assessing the internal structure and reliability of multi-item scales. Yet a researcher may be discouraged upon discovering that a prominent alternative to alpha, namely, coefficient omega, can be calculated in a variety of ways. In this Tutorial, I alleviate this potential confusion by describing alternative forms of omega and providing guidelines for choosing an appropriate omega estimate pertaining to the measurement of a target construct represented with a confirmatory factor analysis model. Several applied examples demonstrate how to compute different forms of omega in R.},
  langid = {english},
  file = {C\:\\Users\\u237972\\Zotero\\storage\\BWNN755T\\Flora_2020_Your Coefficient Alpha Is Probably Wrong, but Which Coefficient Omega Is Right.pdf}
}

@article{10.1177/2515245920952393,
  title = {Measurement {{Schmeasurement}}: {{Questionable Measurement Practices}} and {{How}} to {{Avoid Them}}},
  shorttitle = {Measurement {{Schmeasurement}}},
  author = {Flake, Jessica Kay and Fried, Eiko I.},
  year = {2020},
  month = dec,
  journal = {Advances in Methods and Practices in Psychological Science},
  volume = {3},
  number = {4},
  pages = {456--465},
  publisher = {{SAGE Publications Inc}},
  issn = {2515-2459},
  doi = {10.1177/2515245920952393},
  urldate = {2023-05-20},
  abstract = {In this article, we define questionable measurement practices (QMPs) as decisions researchers make that raise doubts about the validity of the measures, and ultimately the validity of study conclusions. Doubts arise for a host of reasons, including a lack of transparency, ignorance, negligence, or misrepresentation of the evidence. We describe the scope of the problem and focus on how transparency is a part of the solution. A lack of measurement transparency makes it impossible to evaluate potential threats to internal, external, statistical-conclusion, and construct validity. We demonstrate that psychology is plagued by a measurement schmeasurement attitude: QMPs are common, hide a stunning source of researcher degrees of freedom, and pose a serious threat to cumulative psychological science, but are largely ignored. We address these challenges by providing a set of questions that researchers and consumers of scientific research can consider to identify and avoid QMPs. Transparent answers to these measurement questions promote rigorous research, allow for thorough evaluations of a study's inferences, and are necessary for meaningful replication studies.},
  langid = {english}
}

@article{10.1177/25152459211007467,
  title = {An {{Excess}} of {{Positive Results}}: {{Comparing}} the {{Standard Psychology Literature With Registered Reports}}},
  shorttitle = {An {{Excess}} of {{Positive Results}}},
  author = {Scheel, Anne M. and Schijen, Mitchell R. M. J. and Lakens, Dani{\"e}l},
  year = {2021},
  month = apr,
  journal = {Advances in Methods and Practices in Psychological Science},
  volume = {4},
  number = {2},
  pages = {251524592110074},
  issn = {2515-2459, 2515-2467},
  doi = {10.1177/25152459211007467},
  urldate = {2022-07-29},
  abstract = {Selectively publishing results that support the tested hypotheses (``positive'' results) distorts the available evidence for scientific claims. For the past decade, psychological scientists have been increasingly concerned about the degree of such distortion in their literature. A new publication format has been developed to prevent selective reporting: In Registered Reports (RRs), peer review and the decision to publish take place before results are known. We compared the results in published RRs ( N = 71 as of November 2018) with a random sample of hypothesis-testing studies from the standard literature ( N = 152) in psychology. Analyzing the first hypothesis of each article, we found 96\% positive results in standard reports but only 44\% positive results in RRs. We discuss possible explanations for this large difference and suggest that a plausible factor is the reduction of publication bias and/or Type I error inflation in the RR literature.},
  langid = {english}
}

@article{10.1177/25152459221120217,
  title = {Journal {{N-Pact Factors From}} 2011 to 2019: {{Evaluating}} the {{Quality}} of {{Social}}/{{Personality Journals With Respect}} to {{Sample Size}} and {{Statistical Power}}},
  shorttitle = {Journal {{N-Pact Factors From}} 2011 to 2019},
  author = {Fraley, R. Chris and Chong, Jia Y. and Baacke, Kyle A. and Greco, Anthony J. and Guan, Hanxiong and Vazire, Simine},
  year = {2022},
  month = oct,
  journal = {Advances in Methods and Practices in Psychological Science},
  volume = {5},
  number = {4},
  pages = {25152459221120217},
  publisher = {{SAGE Publications Inc}},
  issn = {2515-2459},
  doi = {10.1177/25152459221120217},
  urldate = {2023-05-21},
  abstract = {Scholars and institutions commonly use impact factors to evaluate the quality of empirical research. However, a number of findings published in journals with high impact factors have failed to replicate, suggesting that impact alone may not be an accurate indicator of quality. Fraley and Vazire proposed an alternative index, the N-pact factor, which indexes the median sample size of published studies, providing a narrow but relevant indicator of research quality. In the present research, we expand on the original report by examining the N-pact factor of social/personality-psychology journals between 2011 and 2019, incorporating additional journals and accounting for study design (i.e., between persons, repeated measures, and mixed). There was substantial variation in the sample sizes used in studies published in different journals. Journals that emphasized personality processes and individual differences had larger N-pact factors than journals that emphasized social-psychological processes. Moreover, N-pact factors were largely independent of traditional markers of impact. Although the majority of journals in 2011 published studies that were not well powered to detect an effect of {$\rho$} = .20, this situation had improved considerably by 2019. In 2019, eight of the nine journals we sampled published studies that were, on average, powered at 80\% or higher to detect such an effect. After decades of unheeded warnings from methodologists about the dangers of small-sample designs, the field of social/personality psychology has begun to use larger samples. We hope the N-pact factor will be supplemented by other indices that can be used as alternatives to improve further the evaluation of research.},
  langid = {english}
}

@article{10.1186/1471-2288-11-160,
  title = {Characteristics of Meta-Analyses and Their Component Studies in the {{Cochrane Database}} of {{Systematic Reviews}}: A Cross-Sectional, Descriptive Analysis},
  author = {Davey, Jonathan and Turner, Rebecca M. and Clarke, Mike J. and Higgins, Julian PT},
  year = {2011},
  month = nov,
  journal = {BMC Medical Research Methodology},
  volume = {11},
  number = {1},
  pages = {160},
  issn = {1471-2288},
  doi = {10.1186/1471-2288-11-160},
  abstract = {Cochrane systematic reviews collate and summarise studies of the effects of healthcare interventions. The characteristics of these reviews and the meta-analyses and individual studies they contain provide insights into the nature of healthcare research and important context for the development of relevant statistical and other methods.}
}

@article{10.1186/s12874-015-0024-z,
  title = {The Heterogeneity Statistic {{I2}} Can Be Biased in Small Meta-Analyses},
  author = {{von Hippel}, Paul T},
  year = {2015},
  month = dec,
  journal = {BMC Medical Research Methodology},
  volume = {15},
  number = {1},
  issn = {1471-2288},
  doi = {10.1186/s12874-015-0024-z},
  urldate = {2018-06-28},
  langid = {english},
  file = {C\:\\Users\\u237972\\Zotero\\storage\\RUPAJKVK\\search.crossref.org.html}
}

@book{10.1201/9780429492259,
  title = {Flexible {{Imputation}} of {{Missing Data}}, {{Second Edition}}},
  author = {Van Buuren, Stef},
  year = {2018},
  month = jul,
  edition = {Second},
  publisher = {{Chapman and Hall/CRC}},
  address = {{Second edition. | Boca Raton, Florida : CRC Press, [2019] |}},
  doi = {10.1201/9780429492259},
  urldate = {2023-06-17},
  isbn = {978-0-429-49225-9},
  langid = {english}
}

@article{10.1525/collabra.18738,
  title = {A {{Multi-Site Collaborative Study}} of the {{Hostile Priming Effect}}},
  author = {McCarthy, Randy and Gervais, Will and Aczel, Balazs and {Al-Kire}, Rosemary L. and Aveyard, Mark and Marcella Baraldo, Silvia and Baruh, Lemi and Basch, Charlotte and Baumert, Anna and Behler, Anna and Bettencourt, Ann and Bitar, Adam and Bouxom, Hugo and Buck, Ashley and Cemalcilar, Zeynep and Chekroun, Peggy and Chen, Jacqueline M. and {del Fresno- D{\'i}az}, {\'A}ngel and Ducham, Alec and Edlund, John E. and ElBassiouny, Amanda and Evans, Thomas Rhys and Ewell, Patrick J. and Forscher, Patrick S. and Fuglestad, Paul T. and Hauck, Lauren and Hawk, Christopher E. and Hermann, Anthony D. and Hines, Bryon and Irumva, Mukunzi and Jordan, Lauren N. and {Joy-Gaba}, Jennifer A. and Haley, Catherine and Ka{\v c}m{\'a}r, Pavol and Kezer, Murat and K{\"o}rner, Robert and Kosaka, Muriel and Kovacs, Marton and Lair, Elicia C. and L{\'e}gal, Jean-Baptiste and Leighton, Dana C. and Magee, Michael W. and Markman, Keith and Marton{\v c}ik, Marcel and M{\"u}ller, Martin and Norman, Jasmine B. and Olsen, Jerome and Oyler, Danielle and Phills, Curtis E. and Ribeiro, Gianni and Rohain, Alia and Sakaluk, John and Sch{\"u}tz, Astrid and {Toribio-Fl{\'o}rez}, Daniel and Tsang, Jo-Ann and Vezzoli, Michela and Williams, Caitlin and Willis, Guillermo B. and Young, Jason and Zogmaister, Cristina},
  year = {2021},
  month = feb,
  journal = {Collabra: Psychology},
  volume = {7},
  number = {1},
  pages = {18738},
  issn = {2474-7394},
  doi = {10.1525/collabra.18738},
  urldate = {2023-02-09},
  abstract = {In a now-classic study by Srull and Wyer (1979), people who were exposed to phrases with hostile content subsequently judged a man as being more hostile. And this ``hostile priming effect'' has had a significant influence on the field of social cognition over the subsequent decades. However, a recent multi-lab collaborative study (McCarthy et al., 2018) that closely followed the methods described by Srull and Wyer (1979) found a hostile priming effect that was nearly zero, which casts doubt on whether these methods reliably produce an effect. To address some limitations with McCarthy et al.~(2018), the current multi-site collaborative study included data collected from 29 labs. Each lab conducted a close replication (total N = 2,123) and a conceptual replication (total N = 2,579) of Srull and Wyer's methods. The hostile priming effect for both the close replication (d = 0.09, 95\% CI [-0.04, 0.22], z = 1.34, p = .16) and the conceptual replication (d = 0.05, 95\% CI [-0.04, 0.15], z = 1.15, p = .58) were not significantly different from zero and, if the true effects are non-zero, were smaller than what most labs could feasibly and routinely detect. Despite our best efforts to produce favorable conditions for the effect to emerge, we did not detect a hostile priming effect. We suggest that researchers should not invest more resources into trying to detect a hostile priming effect using methods like those described in Srull and Wyer (1979).},
  file = {C\:\\Users\\u237972\\Zotero\\storage\\3LAV9MMM\\A-Multi-Site-Collaborative-Study-of-the-Hostile.html}
}

@article{10.1525/collabra.247,
  title = {Don't {{Forget}} the {{Model}} in {{Your Model-based Reliability Coefficients}}: {{A Reply}} to {{McNeish}} (2018)},
  shorttitle = {Don't {{Forget}} the {{Model}} in {{Your Model-based Reliability Coefficients}}},
  author = {Savalei, Victoria and Reise, Steven P.},
  editor = {Vazire, Simine and Fried, Eiko},
  year = {2019},
  month = aug,
  journal = {Collabra: Psychology},
  volume = {5},
  number = {1},
  pages = {36},
  issn = {2474-7394},
  doi = {10.1525/collabra.247},
  urldate = {2023-05-31},
  abstract = {McNeish (2018) advocates that researchers abandon coefficient alpha in favor of alternative reliability measures, such as the 1-factor reliability (coefficient omega), a total reliability coefficient based on an exploratory bifactor solution (``Revelle's omega total''), and the glb (``greatest lower bound''). McNeish supports this argument by demonstrating that these coefficients produce higher sample values in several examples. We express three main disagreements with this article. First, we show that McNeish exaggerates the extent to which alpha is different from omega when unidimensionality holds. Second, we argue that, when unidimensionality is violated, most alternative reliability coefficients are model-based, and it is critical to carefully select the underlying latent variable model rather than relying on software defaults. Third, we point out that higher sample reliability values do not necessarily capture population reliability better: many alternative reliability coefficients are upwardly biased except in very large samples. We conclude with a set of alternative recommendations for researchers.},
  file = {C\:\\Users\\u237972\\Zotero\\storage\\JA39W8YD\\Savalei_Reise_2019_Don’t Forget the Model in Your Model-based Reliability Coefficients.pdf;C\:\\Users\\u237972\\Zotero\\storage\\N3FNTRXR\\Don-t-Forget-the-Model-in-Your-Model-based.html}
}

@article{10.18637/jss.v036.i03,
  title = {Conducting {{Meta-Analyses}} in {{{\emph{R}}}} with the {\textbf{Metafor}} {{Package}}},
  author = {Viechtbauer, Wolfgang},
  year = {2010},
  journal = {Journal of Statistical Software},
  volume = {36},
  number = {3},
  issn = {1548-7660},
  doi = {10.18637/jss.v036.i03},
  urldate = {2018-06-28},
  langid = {english}
}

@article{10.2307/1412159,
  title = {The {{Proof}} and {{Measurement}} of {{Association}} between {{Two Things}}},
  author = {Spearman, C.},
  year = {1904},
  month = jan,
  journal = {The American Journal of Psychology},
  volume = {15},
  number = {1},
  eprint = {1412159},
  eprinttype = {jstor},
  pages = {72},
  issn = {00029556},
  doi = {10.2307/1412159},
  urldate = {2023-06-01}
}

@article{10.3102/10769986030003261,
  title = {Bias and {{Efficiency}} of {{Meta-Analytic Variance Estimators}} in the {{Random-Effects Model}}},
  author = {Viechtbauer, Wolfgang},
  year = {2005},
  month = sep,
  journal = {Journal of Educational and Behavioral Statistics},
  volume = {30},
  number = {3},
  pages = {261--293},
  issn = {1076-9986, 1935-1054},
  doi = {10.3102/10769986030003261},
  urldate = {2023-04-24},
  abstract = {The meta-analytic random effects model assumes that the variability in effect size estimates drawn from a set of studies can be decomposed into two parts: heterogeneity due to random population effects and sampling variance. In this context, the usual goal is to estimate the central tendency and the amount of heterogeneity in the population effect sizes. The amount of heterogeneity in a set of effect sizes has implications regarding the interpretation of the meta-analytic findings and often serves as an indicator for the presence of potential moderator variables. Five population heterogeneity estimators were compared in this article analytically and via Monte Carlo simulations with respect to their bias and efficiency.},
  langid = {english},
  file = {C\:\\Users\\u237972\\Zotero\\storage\\KLJRTK3G\\Viechtbauer - 2005 - Bias and Efficiency of Meta-Analytic Variance Esti.pdf}
}

@misc{10.31234/osf.io/dm8xn,
  title = {An Aberrant Abundance of {{Cronbach}}'s Alpha Values at .70},
  author = {Hussey, Ian and Alsalti, Taym and Bosco, Frank and Elson, Malte and Arslan, Ruben C.},
  year = {2023},
  month = feb,
  publisher = {{PsyArXiv}},
  doi = {10.31234/osf.io/dm8xn},
  urldate = {2023-05-21},
  abstract = {Cronbach's alpha ({$\alpha$}) is the most widely reported metric of the reliability of psychological measures. Decisions about an observed {$\alpha$}'s adequacy are often made using rule-of-thumb thresholds, such as {$\alpha$} of at least .70. Such thresholds can put pressure on researchers to make their measures meet these criteria, similar to the pressure to meet the significance threshold with p values. We examined whether {$\alpha$} values reported in the literature are inflated at the rule-of-thumb thresholds ({$\alpha$} = .70, .80, .90), due to, for example, overfitting to in-sample data ({$\alpha$}-hacking) or publication bias. We extracted reported {$\alpha$} values from two very large literatures covering general psychology ({$>$}30,000 {$\alpha$} values taken from {$>$}74,000 published articles in APA journals) and Industrial and Organizational psychology ({$>$}89,000 {$\alpha$} values taken from {$>$}14,000 published articles in I/O journals). The distributions of these values show inflation at the rule-of-thumb thresholds. We discuss the scope, causes and consequences of {$\alpha$}-hacking and how increased transparency, preregistration of measurement strategy, and standardized protocols could mitigate this problem. Code and data available at osf.io/pe3t7. Supplementary materials at osf.io/5xzy4.},
  langid = {american},
  keywords = {alpha-hacking,cronbach's alpha,Industrial and Organizational Psychology,Meta-science,p-hacking,reliability,Social and Behavioral Sciences,Social and Personality Psychology}
}

@article{10.3389/fpsyg.2019.00813,
  title = {The {{Meaningfulness}} of {{Effect Sizes}} in {{Psychological Research}}: {{Differences Between Sub-Disciplines}} and the {{Impact}} of {{Potential Biases}}},
  shorttitle = {The {{Meaningfulness}} of {{Effect Sizes}} in {{Psychological Research}}},
  author = {Sch{\"a}fer, Thomas and Schwarz, Marcus A.},
  year = {2019},
  month = apr,
  journal = {Frontiers in Psychology},
  volume = {10},
  issn = {1664-1078},
  doi = {10.3389/fpsyg.2019.00813},
  urldate = {2020-05-11},
  abstract = {Effect sizes are the currency of psychological research. They quantify the results of a study to answer the research question and are used to calculate statistical power. The interpretation of effect sizes\textemdash when is an effect small, medium, or large?\textemdash has been guided by the recommendations Jacob Cohen gave in his pioneering writings starting in 1962: Either compare an effect with the effects found in past research or use certain conventional benchmarks. The present analysis shows that neither of these recommendations is currently applicable. From past publications without pre-registration, 900 effects were randomly drawn and compared with 93 effects from publications with pre-registration, revealing a large difference: Effects from the former (median r = 0.36) were much larger than effects from the latter (median r = 0.16). That is, certain biases, such as publication bias or questionable research practices, have caused a dramatic inflation in published effects, making it difficult to compare an actual effect with the real population effects (as these are unknown). In addition, there were very large differences in the mean effects between psychological sub-disciplines and between different study designs, making it impossible to apply any global benchmarks. Many more pre-registered studies are needed in the future to derive a reliable picture of real population effects.},
  pmcid = {PMC6470248},
  pmid = {31031679}
}

@article{10.3390/jintelligence8040036,
  title = {Effect {{Sizes}}, {{Power}}, and {{Biases}} in {{Intelligence Research}}: {{A Meta-Meta-Analysis}}},
  shorttitle = {Effect {{Sizes}}, {{Power}}, and {{Biases}} in {{Intelligence Research}}},
  author = {Nuijten, Mich{\`e}le B. and {van Assen}, Marcel A. L. M. and Augusteijn, Hilde E. M. and Crompvoets, Elise A. V. and Wicherts, Jelte M.},
  year = {2020},
  month = oct,
  journal = {Journal of Intelligence},
  volume = {8},
  number = {4},
  pages = {36},
  issn = {2079-3200},
  doi = {10.3390/jintelligence8040036},
  abstract = {In this meta-study, we analyzed 2442 effect sizes from 131 meta-analyses in intelligence research, published from 1984 to 2014, to estimate the average effect size, median power, and evidence for bias. We found that the average effect size in intelligence research was a Pearson's correlation of 0.26, and the median sample size was 60. Furthermore, across primary studies, we found a median power of 11.9\% to detect a small effect, 54.5\% to detect a medium effect, and 93.9\% to detect a large effect. We documented differences in average effect size and median estimated power between different types of intelligence studies (correlational studies, studies of group differences, experiments, toxicology, and behavior genetics). On average, across all meta-analyses (but not in every meta-analysis), we found evidence for small-study effects, potentially indicating publication bias and overestimated effects. We found no differences in small-study effects between different study types. We also found no convincing evidence for the decline effect, US effect, or citation bias across meta-analyses. We concluded that intelligence research does show signs of low power and publication bias, but that these problems seem less severe than in many other scientific fields.},
  langid = {english},
  pmcid = {PMC7720125},
  pmid = {33023250},
  keywords = {bias,effect size,intelligence,meta-meta-analysis,meta-science,power}
}

@article{10.3758/s13428-021-01557-x,
  title = {Testing the Construct Validity of Competing Measurement Approaches to Probed Mind-Wandering Reports},
  author = {Kane, Michael J. and Smeekens, Bridget A. and Meier, Matt E. and Welhaf, Matthew S. and Phillips, Natalie E.},
  year = {2021},
  month = apr,
  journal = {Behavior Research Methods},
  issn = {1554-3528},
  doi = {10.3758/s13428-021-01557-x},
  urldate = {2021-10-12},
  abstract = {Psychology faces a measurement crisis, and mind-wandering research is not immune. The present study explored the construct validity of probed mind-wandering reports (i.e., reports of task-unrelated thought [TUT]) with a combined experimental and individual-differences approach. We examined laboratory data from over 1000 undergraduates at two U.S. institutions, who responded to one of four different thought-probe types across two cognitive tasks. We asked a fundamental measurement question: Do different probe types yield different results, either in terms of average reports (average TUT rates, TUT-report confidence ratings), or in terms of TUT-report associations, such as TUT rate or confidence stability across tasks, or between TUT reports and other consciousness-related constructs (retrospective mind-wandering ratings, executive-control performance, and broad questionnaire trait assessments of distractibility\textendash restlessness and positive-constructive daydreaming)? Our primary analyses compared probes that asked subjects to report on different dimensions of experience: TUT-content probes asked about what they'd been mind-wandering about, TUT-intentionality probes asked about why they were mind-wandering, and TUT-depth probes asked about the extent (on a rating scale) of their mind-wandering. Our secondary analyses compared thought-content probes that did versus didn't offer an option to report performance-evaluative thoughts. Our findings provide some ``good news''\textemdash that some mind-wandering findings are robust across probing methods\textemdash and some ``bad news''\textemdash that some findings are not robust across methods and that some commonly used probing methods may not tell us what we think they do. Our results lead us to provisionally recommend content-report probes rather than intentionality- or depth-report probes for most mind-wandering research.},
  langid = {english}
}

@article{10.5334/jopd.33,
  title = {Estimates of {{Between-Study Heterogeneity}} for 705 {{Meta-Analyses Reported}} in {{{\emph{Psychological Bulletin}}}} {{From}} 1990\textendash 2013},
  author = {Van Erp, Sara and Verhagen, Josine and Grasman, Raoul P. P. P. and Wagenmakers, Eric-Jan},
  year = {2017},
  month = aug,
  journal = {Journal of Open Psychology Data},
  volume = {5},
  number = {1},
  pages = {4},
  issn = {2050-9863},
  doi = {10.5334/jopd.33},
  urldate = {2018-06-28},
  keywords = {Bayesian inference,heterogeneity,informed prior distribution,meta-analysis},
  file = {C\:\\Users\\u237972\\Zotero\\storage\\7TLI4YGW\\search.crossref.org.html;C\:\\Users\\u237972\\Zotero\\storage\\AS2B95IM\\jopd.html}
}

@book{borenstein2009a,
  title = {Introduction to Meta-Analysis},
  author = {Borenstein, Michael},
  year = {2009},
  publisher = {{Wiley-Blackwell}},
  address = {{Oxford}},
  isbn = {978-0-470-05724-7 0-470-05724-6},
  langid = {english}
}

@article{campbell2021,
  title = {Measurement Error in Meta-Analysis ({{MEMA}})\textemdash{{A Bayesian}} Framework for Continuous Outcome Data Subject to Non-Differential Measurement Error},
  author = {Campbell, Harlan and {de Jong}, Valentijn MT and Maxwell, Lauren and Jaenisch, Thomas and Debray, Thomas PA and Gustafson, Paul},
  year = {2021},
  journal = {Research Synthesis Methods},
  volume = {12},
  number = {6},
  pages = {796--815},
  file = {C\:\\Users\\u237972\\Zotero\\storage\\W444ABZU\\jrsm.html}
}

@book{card2015,
  title = {Applied Meta-Analysis for Social Science Research},
  author = {Card, Noel A.},
  year = {2015},
  publisher = {{Guilford Publications}},
  file = {C\:\\Users\\u237972\\Zotero\\storage\\UKYKRDDX\\Card_2015_Applied meta-analysis for social science research.pdf;C\:\\Users\\u237972\\Zotero\\storage\\2XKY5HF4\\books.html}
}

@book{cheung2015,
  title = {Meta-Analysis : A Structural Equation Modeling Approach},
  author = {Cheung, Mike W. L.},
  year = {2015},
  publisher = {{John Wiley \& Sons, Inc.}},
  address = {{Chichester, West Sussex}},
  isbn = {978-1-119-99343-8 1-119-99343-1},
  langid = {english}
}

@manual{constantin2023,
  type = {Manual},
  title = {Parabar: {{Progress}} Bar for Parallel Tasks},
  author = {Constantin, Mihai},
  year = {2023}
}

@book{cook2002,
  title = {Experimental and Quasi-Experimental Designs for Generalized Causal Inference},
  author = {Cook, Thomas D. and Campbell, Donald Thomas and Shadish, William},
  year = {2002},
  publisher = {{Houghton Mifflin Boston, MA}}
}

@manual{dowle2021,
  type = {Manual},
  title = {Data.Table: {{Extension}} of `data.Frame`},
  author = {Dowle, Matt and Srinivasan, Arun},
  year = {2021}
}

@book{enders2010,
  title = {Applied Missing Data Analysis},
  author = {Enders, Craig K.},
  year = {2010},
  series = {Methodology in the Social Sciences},
  publisher = {{Guilford Press}},
  address = {{New York}},
  isbn = {978-1-60623-639-0 1-60623-639-3},
  langid = {english}
}

@article{geyskens2009,
  title = {A Review and Evaluation of Meta-Analysis Practices in Management Research},
  author = {Geyskens, Inge and Krishnan, Rekha and Steenkamp, Jan-Benedict EM and Cunha, Paulo V.},
  year = {2009},
  journal = {Journal of Management},
  volume = {35},
  number = {2},
  pages = {393--419},
  file = {C\:\\Users\\u237972\\Zotero\\storage\\7E9HEDUN\\Geyskens et al_2009_A review and evaluation of meta-analysis practices in management research.pdf}
}

@book{hedges1985,
  title = {Statistical Methods for Meta-Analysis},
  author = {Hedges, Larry V. and Olkin, Ingram},
  year = {1985},
  publisher = {{Academic Press}},
  address = {{Orlando}},
  isbn = {0-12-336380-2 978-0-12-336380-0 0-12-336381-0 978-0-12-336381-7},
  langid = {english}
}

@book{lord1968,
  title = {Statistical Theories of Mental Test Scores / Frederic {{M}}. {{Lord}} and Melvin {{R}}. {{Novick}} ; with Contributions by Allan Birnbaum},
  author = {{Lord} and M., Frederic and Novick, Melvin R.},
  year = {1968},
  publisher = {{Addison-Wesley Pub. Co Reading, Mass}},
  langid = {english}
}

@article{Publisher:AmericanPsychologicalAssociation,
  title = {Validity Generalization Results for Computer Programmers.},
  author = {Schmidt, Frank L. and {Gast-Rosenberg}, Ilene and Hunter, John E.},
  year = {1980},
  journal = {Journal of Applied Psychology},
  volume = {65},
  number = {6},
  pages = {643},
  publisher = {{American Psychological Association}},
  file = {C\:\\Users\\u237972\\Zotero\\storage\\2CZS8MYT\\Schmidt et al_1980_Validity generalization results for computer programmers.pdf;C\:\\Users\\u237972\\Zotero\\storage\\BF3Q6WKY\\1981-04250-001.html}
}

@manual{rcoreteam2020,
  type = {Manual},
  title = {R: {{A}} Language and Environment for Statistical Computing},
  author = {{R Core Team}},
  year = {2020},
  address = {{Vienna, Austria}},
  institution = {{R Foundation for Statistical Computing}}
}

@book{schmidt2015,
  title = {Methods of {{Meta-Analysis}} : {{Correcting Error}} and {{Bias}} in {{Research Findings}}},
  author = {Schmidt, Frank and Hunter, John E.},
  year = {2015},
  edition = {Third edition},
  publisher = {{SAGE}},
  address = {{Los Angeles}},
  isbn = {978-1-4833-1308-5 1-4833-1308-5 978-1-4833-9810-5 1-4833-9810-2},
  langid = {english}
}

@manual{tex.organization:RFoundationforStatisticalComputing,
  type = {Manual},
  title = {R: {{A}} Language and Environment for Statistical Computing},
  author = {{R Core Team}},
  year = {2020},
  address = {{Vienna, Austria}},
  organization = {{R Foundation for Statistical Computing}}
}

@book{wickham2016,
  title = {Ggplot2: {{Elegant}} Graphics for Data Analysis},
  author = {Wickham, Hadley},
  year = {2016},
  publisher = {{Springer-Verlag New York}},
  isbn = {978-3-319-24277-4}
}

@article{zhao2017,
  title = {Comparisons of {{Artifact Correction Procedures}} for {{Meta-Analysis}}: {{An Empirical Examination}} on {{Correcting Reliabilities}}},
  shorttitle = {Comparisons of {{Artifact Correction Procedures}} for {{Meta-Analysis}}},
  author = {Zhao, Lei},
  year = {2017},
  month = jan,
  journal = {Dissertations},
  file = {C\:\\Users\\u237972\\Zotero\\storage\\Y2FJQVJN\\Zhao_2017_Comparisons of Artifact Correction Procedures for Meta-Analysis.pdf;C\:\\Users\\u237972\\Zotero\\storage\\SEWNKI7L\\2605.html}
}
