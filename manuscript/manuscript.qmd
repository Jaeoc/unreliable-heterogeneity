---
format: docx
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

```{r load data and prep, include = FALSE}
doc_type <- knitr::opts_knit$get('rmarkdown.pandoc.to') #to define the type of document, then format tables using an ifelse statement, see also: https://stackoverflow.com/questions/58999720/scientific-journal-table-of-variable-mean-per-year-with-s-d


in_text <- readRDS("in-text-values.RDS")
tables <- readRDS("tables.RDS")
library(kableExtra) #must be loaded to print saved kableextra tables

```

Estimating heterogeneity of effect sizes is usually considered the main purpose of meta-analysis in addition to estimating an average effect size. Heterogeneity of effect sizes (henceforth referred to as heterogeneity) refers to an effect size's sensitivity to variability in study design, or more specifically to sensitivity to differences in four study factors: 1) sample population, 2) settings, 3) treatment variables and 4) measurement variables (e.g., Campbell & Stanley, 2015). Heterogeneity is a primary outcome in meta-analysis because of several reasons. First, its presence signals the existence of moderators and so can be seen as an opportunity for theoretical development (Simons et al., 2017). Second, heterogeneity affects the implementation of research by indicating that an intervention may not be equally effective under all conditions or for all people. Third, the presence of heterogeneity changes the interpretation of the average effect sized derived in meta-analysis from the true population effect size (under homogeneity) to the average of true subpopulation effect sizes (under heterogeneity). As such, estimating heterogeneity with minimal bias is of central importance to meta-analysis (for a more extensive discussion on the importance of heterogeneity, see Olsson Collentine et al., 2020; Simons, 2017).

Meta-analytic estimates are highly dependent on the quality of data in primary studies. However, there is concern that researchers in psychology tend to neglect proper measurement (Flake & Fried, 2020) to the extent that psychology can be said to be in a measurement crisis (Kane et al., 2021). For example, Flake et al. (2017) report that although latent constructs are widely examined in personality and social psychology, researchers barely report any of the validity evidence required to ascertain the extent to which implemented scales measure the constructs of interest. Such inattention to measurement can also bias heterogeneity estimates. We will focus here on reliability of measurement (defined in classical test theory as the ratio of true score variance to measurement error variance; e.g., Lord & Novick, 1968), which is the only measurement validity evidence researchers report consistently (Flake et al., 2017), and which can be corrected for in meta-analysis (Hunter & Schmidt; 2015).

Measurement error (imperfect reliability) is present in any empirical effect size estimate and systematically attenuates observed effect sizes compared to true underlying effect sizes. This affects heterogeneity estimates, which from a statistical perspective estimate the variance between true effect sizes. However, reliability is not necessarily accounted for in heterogeneity estimates, because ‘true’ underlying effect sizes can refer to two different entities. Either effect sizes free from sampling error, or effect sizes free from both sampling error and measurement artifacts (such as imperfect reliability, restriction of range, and dichotomization; Hunter & Schmidt, 2015). Whereas all meta-analytic models attempt to correct for sampling error in their estimates, correcting for measurement artifacts is implicit only in the models of Hunter & Schmidt (2015). This has consequences for heterogeneity estimates reported in many areas of psychology. Inattention to measurement reliability and how it affects heterogeneity estimates can lead researchers to misinterpret their meta-analytic average effect size estimate, ignore the presence of moderators or search for (and discover) non-existent moderators, and to implement research interventions inappropriately.

Meta-analyses in psychology rarely correct for unreliability in primary study measurements. Wiernick and Dahlke (2020) report that among the 71 meta-analytic studies published in the journal Psychological Bulletin between 2016 and 2018, only 6/71 (8%) corrected for unreliability. Similarly, Schmidt (2010) report that only 19/199 (10%) of the meta-analytic studies published in the same journal between 1978 - 2006 corrected for any measurement artifacts. The exception is the subfield of Industrial-Organizational studies where corrections tend to be more common (e.g., Cortina, 2003; Anguinis, 2011).

One reason that few meta-analysts correct for unreliability could be that correcting for unreliability is generally controversial and has been so since it was proposed (for arguments spanning the last century, see Table 2 of LeBreton et al., 2017). On the one hand, correcting for unreliability is seen by some as conceptually problematic because it inflates effect size estimates to match a hypothetical and unachievable scenario of perfect measurement (e.g., Seymour, 1988; LeBreton et al., 2017). Systematically correcting for unreliability may also be expected to lead to overestimation of effect sizes, because reliability estimates are lower bound estimates (i.e., a reliability estimate of 0.8 implies the reliability is between 0.8 - 1). Even worse, the most commonly reported estimate of reliability in psychology (Chronbach's alpha; Flake et al., 2017) as well as improvements thereof tend to underestimate the lower bound of reliability (Sijtsma, 2008). Moreover, from a pragmatic meta-analytic perspective, effect sizes appear to often be overestimated in psychology (e.g., Kvarven et al., 2019; Schäfer & Scharz, 2019; Scheel et al., 2020) due to selective reporting based on the significance of outcomes (Simmons et al., 2011) and publication bias (REF?), so correcting for unreliability may inflate meta-analytic estimates even further and lead to estimates further from their true values rather than closer. On the other hand, researchers in psychology are typically interested in latent constructs rather than observed measures. As such, neglecting measurement artifacts means that computed estimates do not correspond to the entity of interest. From this perspective, correcting for unreliability is desirable even if doing so is challenging (e.g., Oswald et al., 2015; Hunter & Schmidt, 2015), although  treating disattenuated effect sizes as directly comparable to latent scores requires certain assumptions (Borsboom & Mellenbergh, 2002). In the end, whether correcting for unreliability or not, authors and consumers of meta-analyses need to be aware of how this decision affects heterogeneity estimates.

## Bias in heterogeneity estimates due to unreliability

In classical test theory (CTT), reliability of measurement ($R_{xx}$) is defined as the proportion true score variance ($\sigma^2_T$) to observed score variance ($\sigma^2_X$) and is presumed to be constant for all participants in a sample $R_{xx} = \sigma^2_T / \sigma^2_X$. We use unreliability of measurement to refer to $1 - R_{xx}$. Imperfect reliability in measurements leads to attenuation of observed effect sizes. For product-moment correlations the observed correlation can be computed as $r_{xy} = \rho_{xy} \times \sqrt{R_{xx'}} \times \sqrt{R_{yy'}}$ where $r_{xy}$ is the observed correlation between the variables $X$ and $Y$, $\rho_{xy}$ is the true correlation (with or without sampling error) and $R_{xx'}$ and $R_{yy'}$ are the measurement reliabilities for $X$ and $Y$. As such, correcting for unreliability is straightforward for product-moment correlations, but can also be done for other effect size types (Wiernick and Dahlke, 2020). A corrected correlation should also have its sampling variance corrected. This is inflated by a factor equal to the square of the reliabilities, such that if $a = \sqrt{R_{xx'}} \times \sqrt{R_{yy'}}$ then the corrected sampling variance $V_\rho = V_{r} / a^2$ (e.g., Borenstein, 2009, p. 343). Imperfect reliability in primary studies imbues heterogeneity estimates with both a positive and a negative bias.

A positive bias can be expected in heterogeneity estimates of uncorrected effect sizes due to the variability in reliability between primary studies. This is because the attenuation in effect size will then differ from study to study, which results in differences across studies in observed effect sizes beyond their true variability. This is most easily seen by assuming a common true effect size $\mu$ across studies (i.e., a fixed effect and no true heterogeneity) and infinite sample size within studies. Any observed effect size $\hat{\mu}$ can then be seen as the true effect size $\mu$ multiplied by a study’s reliability $R_i$ (assuming $R_i = R_{xx’} = R_{yy’}$). Hence, the distribution of observed effect sizes is the true effect size $\mu$ multiplied by the distribution of reliabilities across studies. That is, $\hat{mu} \sim \mu \times N(\bar{R}, \sigma_R) = N(\mu \bar{R}, \mu \sigma_R$, where $\bar{R}$ is the average reliability across studies and $\sigma_R$ its standard deviation. Because reliability lies between 0 – 1, under these conditions imperfect reliability implies that the observed average effect size will be less than the true effect size by a factor $\bar{R}$ and estimated heterogeneity will be larger than true heterogeneity (i.e., zero) by the product of $\mu \times \sigma_R$ (if heterogeneity is expressed as standard deviation rather than variance). That is, variability in reliability across studies leads to a positive bias in heterogeneity that depends on the size of the true effect size. Table 1 illustrates this effect for three studies with differing reliability and zero true heterogeneity.

Table 1.

*Variance in reliability leads to positive bias in heterogeneity estimates which increases with average effect size*

```{r positive bias table}

if (doc_type == "docx") {
  print("Table 1 approximately here. Cannot be printed nicely to Word, see tables.pdf")
} else{
  tables$positive_bias_table
}

```

A negative bias can be expected in heterogeneity estimates of uncorrected effect sizes because in absolute terms larger effect sizes are more attenuated by a given reliability. That is, although the attenuation formula $r_{xy} = \rho_{xy} \times \sqrt{R_{xx'}} \times \sqrt{R_{yy'}}$ implies that any effect size $\rho_{xy}$ will be attenuated by the same proportion given the same reliabilities $\sqrt{R_{xx'}} \times \sqrt{R_{yy'}}$, this proportion corresponds to a larger absolute value for larger effect sizes. Consequently, in the presence of heterogeneity attenuation will move larger true effect sizes further towards zero than smaller ones, decreasing heterogeneity in observed effect sizes. This negative bias can be observed directly by assuming an average effect size $\theta$ with some heterogeneity $\tau^2$, infinite sample size within studies (i.e., no within-study sampling variance), and equal reliability across studies ($R = R_i = R_{xx'} = R_{yy'}$). Any observed effect size $\hat{\theta_i}$ can then be seen as a study’s true effect size $\theta_i$ multiplied by the study reliability $R$. Hence, the distribution of observed effect sizes is the distribution of true effect sizes $N(\theta, \tau^2)$ multiplied by the reliability $R$, such that $R \times N(\theta, \tau^2) = N(R \theta, R^2 \tau^2) = N(R \theta, R \tau)$. Because reliability lies between 0 – 1, under these conditions imperfect reliability implies that both the observed average effect size and the estimated heterogeneity will be less than their true value by a factor $\bar{R}$ (if heterogeneity is expressed as standard deviation rather than variance). This depiction is not entirely correct for Pearson's $r$ as it is bounded at {-1, 1}, but holds approximately if the mean effect size $\theta$ and heterogeneity $\tau^2$ are not too large. Hence, imperfect reliability in studies can be expected to create a negative bias in heterogeneity estimates whenever true heterogeneity is present. Consider an average reliability of 0.8 (the median reported reliability in psychology; Flake et al., 2017). Then, $0.8 \times N(\theta, \tau) = N(0.8\theta, 0.8 \tau)$, resulting in 20% less standard deviation among observed effect sizes than in true effect sizes. Table 2 illustrates this relationship for infinite sample size (i.e., no sampling variance) and three levels of reliability across three studies.

Table 2.

*Imperfect reliability leads to negative bias in heterogeneity estimates*

```{r negative bias table}

if (doc_type == "docx") {
  print("Table 2 approximately here. Cannot be printed nicely to Word, see tables.pdf")
} else{
  tables$negative_bias_table
}

```

It is difficult to predict the total effect of the negative and positive bias in heterogeneity due to unreliability, and existing literature tends to focus on the positive bias (e.g., Borenstein, 2009, p. 342; Card, 2015, p. 126; Hunter & Schmidt, 2015; Wiernick & Dahlke, 2020). However, given the value of accurate  heterogeneity estimates for many research outcomes in psychology and that the negative bias could often be 20% or more, we consider it important for meta-analysts to have insight into the total bias in heterogeneity estimates that should be expected due to unreliability. This depends on average effect size, true heterogeneity, mean reliability across studies, the variability in reliabilities, and sampling variance within studies. Hence, we perform a Monte-Carlo simulation study to explore the expected bias in heterogeneity estimates due to unreliability in primary studies.


# Methods

All code and data for this project are available at (OSF LINK). For our simulations and analyses we used R (REF) and we used the R package 'metafor' (REF) to perform meta-analyses and estimate heterogeneity. For parallel computing we took advantage of the R packages 'parallel' (REF) and 'parabar' (REF).

## Choice of effect size type for simulations

We focus on correlations as our effect size type in these simulations because correcting them for unreliability is straightforward and because they are the effect size type most commonly corrected in practice. However, unreliability attenuates all types of effect sizes and all can be corrected, either directly (standardized mean differences; Wiernick & Dahlke, 2020) or by first transforming them into correlations. Although effect sizes can generally be transformed into each other (e.g., Borenstein, 2009, p. 46) this should be done with care as it can sometimes affect conclusions (e.g., we found a violation of monotonicity in heterogeneity estimates when converting effect sizes into correlations in a previous project; Olsson-Collentine, 2020, supplement A).

Perhaps the most common way to meta-analyze correlations is to first transform them into Fisher $z$ correlations (Borenstein et al., 2009, p. 41 - 43). The Fisher $z$ transformation is the inverse hyperbolic tangent of the product moment correlation and can be computed as $z = 0.5 \times ln\left(\frac{1+r}{1-r}\right)$ and its variance approximated as $V_z = 1 / (n -3)$. There is controversy in the use of the Fisher $z$ transformation for meta-analysis (e.g., Hunter & Schmidt, 2015, p. XX, ch. 5 "Use of Fisher’s z in Meta-Analysis of Correlations"), largely related to whether Pearson's $r$ or Fisher's $z$ leads to less bias in average correlations (Schulze, 2007). However, estimating heterogeneity does not seem to have been the focus of most of the literature's discussion of the appropriateness of the Fisher $z$ transformation (Field, 2005; Hafdahl & Williams, 2009, Hunter & Schmidt, 2015). Hall and Brannick (2002) do report heterogeneity estimates but focus on the coverage of prediction intervals ('credibility intervals' in Hunter & Schmidt terminology) which combine the heterogeneity estimate and point estimate, and in Brannick et al., (2019) they examine results only after corrections for attenuation. Given the inconclusive evidence and that the Fisher $z$ transformation is often applied in practice, we use it as one of the effect size types in our simulations.

The other effect size type we use is Pearson's $r$. Pearson's $r$ has the advantage that resulting heterogeneity estimates on this scale are more interpretable than on the Fisher $z$ scale. Although Fisher $z$ correlations can be back-transformed into Pearson's $r$ for interpretation, its heterogeneity cannot. A workaround is to compute a prediction interval and convert this interval onto the product-moment scale (Hedges & Vevea, 1998). The width of the interval can then be used as an estimate of the heterogeneity. We prefer to compute comparable heterogeneity values for Pearson's $r$ and Fisher's $z$ (see the methods section 'Parameter values') and report the bias in heterogeneity estimates, as we believe this is more interpretable for applied meta-analysts.

The disadvantage to using Pearson's $r$ when estimating heterogeneity is that 1) the effect size is bounded to {-1, 1} and 2) the sampling variance $\sigma^2_r = \frac{(1-\rho^2)^2}{n - 1}$ depends on the true effect size $\rho$. The bounded nature of the Pearson correlation means that large levels of heterogeneity will lead to substantial truncation of effect sizes if the average effect size is also large. That the sampling variance covaries with effect size leads to a small positive bias in heterogeneity estimates (supplement XX). This can be solved by replacing $\rho$ with the average correlation across studies $\bar{r}$ (Hunter & Schmidt, 2015), or using the Fisher $z$ transformation. Fisher $z$ correlations have the additional advantage that they are not bounded to {-1, 1}. A disadvantage to using Fisher $z$ correlations, in addition to interpretability of heterogeneity estimates, is that they require backtransforming into product-moment correlations before adding measurement error, and then transforming to Fisher $z$ again for meta-analysis. Especially for large Fisher $z$ values (which can happen if the average effect size and heterogeneity are large) this transformation process may introduce inaccuracies in the estimates. Given the common use of both effect size types and their mix of advantages and disadvantages we report results for both.

## Meta-analytic model

We apply two meta-analytic models to ensure that our results are not model-dependent: the Hedges and Vevea (1998) random-effects  model and the Hunter and Schmidt (2015) 'bare-bones' random-effects model. The Hedges and Vevea (HV) model uses inverse variance weight computed as $w_i = 1 / (V_i + T^2)$ (e.g., Borenstein et al., 2010) where $V_i$ is the sampling variance of study $i$ and $T^2$ is the estimated between-studies variance. The Hunter and Schmidt (HS) model weighs studies by sample size $w_i = N_i$. The differences between these weights are relatively minor (Borenstein et al., 2010; Hunter and Schmidt, 2015, p. XX). Although the HV model is typically applied to Fisher $z$ transformed correlations (e.g., Borenstein et al., p. 41-43), we apply it to both transformed and untransformed correlations to enable a more direct comparison with the HS model and facilitate interpretation. The HS model typically includes correcting for unreliability or other measurement artifacts. However, we implement it without corrections ('bare-bones'; Hunter & Schmidt, 2015) because this allows us to focus on the bias in heterogeneity estimates rather than the performance of corrections.

Estimating heterogeneity in both the HV and bare-bones HS model essentially consists of comparing the observed variance in effect sizes with what would be expected from sampling error alone and then ascribing any excess observed variance to heterogeneity (Borenstein et al., 2010, Box 1; Hunter & Schmidt, 2015, p. XX). However, the process for doing so differs somewhat between methods. We apply the Restricted Maximum Likelihood (REML) estimator of heterogeneity when estimating heterogeneity in the Hedges and Vevea (1998) model, because it is the generally recommended heterogeneity estimator (Veroniki et al., 2016; Viechtbauer, 2005). Describing the REML estimator in detail is beyond the scope of this paper and we refer interested readers to the description in Veroniki et al. (2016). The Hunter and Schmidt (2015) model estimates heterogeneity as $\tau^2 = \sigma^2_r - \sigma^2_e$ where  $\sigma^2_r$ is the observed variance across correlations and $\sigma^2_e$ is the average sampling error variance across studies, computed as $ \frac{\Sigma[N_i \sigma^2_{ei}]}{\Sigma N_i}$ (p. 87, 2nd edition) and $\sigma^2_{ei}$ is the usual sampling variance for the Pearson correlation but with $\rho$ replaced with the sample size weighted average correlation $\bar{r} = \frac{\Sigma n_i r_i}{\Sigma n_i}$ instead of $r_i$ as is common. A generalized version of the HS estimator for effect sizes other than correlations can be found in Viechtbauer (2005; 2015).


## Simulation study design

Figure 1 provides an overview of the design of this simulation study. Our design was broadly the same for product-moment correlations and Fisher's $z$, with some minor differences we highlight in the text below detailing the procedure.

```{r Figure 1, out.width= "100%"}
knitr::include_graphics("../figures/flowchart-simulation-design.png")
```

(FIGURE 1: a flowchart summarizing the simulation design)

When generating data for meta-analysis we proceeded as follows. First,  we sampled $i = 1, ..., k$ true study effect sizes $\theta_i$ from $Ntrunc(\theta, \tau^2)$. That is, a normal distribution with a mean of $\theta$ and a variance of $\tau^2$ that we truncated at $\{-1, 1\}$ to stay within the bounds of the product-moment correlation. We implemented the truncation in R using inverse transform sampling (link: functions.r). For Fisher' $z$ no truncation was applied.

For each of the $k$ sampled true study effect sizes $\theta_i$ we then sampled one observed effect size $\gamma_i$ from a truncated normal distribution $Ntrunc(\theta_i, \sigma_i^2)$, using the same truncation procedure. The sampling variance $\sigma_i^2$ for each true effect size $\theta_i$ we computed using the standard formula for Pearson's $r$ $\sigma_i^2 =  \frac{(1 - \theta_i^2)^2}{N_i - 1}$ where $N_i$ is the total sample size for study $i = 1, ..., k$. For Fisher's $z$ no truncation was applied and we approximated the sampling variance as $\sigma^2_i = \frac{1}{N_i - 3}$. At this point we have $k$ sampled effect sizes $\gamma_i$ in Pearson's $r$ or Fisher's $z$ without measurement error.

To add measurement error we first sampled a reliability $R_i$ for each study from a truncated ($\{0, 1\}$) normal distribution $Ntrunc(\bar{R}, \delta)$, given some average reliability across studies $\bar{R}$ and standard deviation in reliability $\delta$. We assumed that both the dependent and independent variable within a study were measured with the same reliability such that $R_i = R_{iXX'} = R_{iYY'}$. We then computed observed effect sizes $r_i$ for each study given the attenuation formula $r_{i} = \gamma_i \times \sqrt{R_{XX'}} \times \sqrt{R_{YY'}}$. Because we assumed equal reliability in $X$ and $Y$ this simplifies to $r_i = \gamma_i \times R_i$. Fisher's $z$ we transformed to product-moment correlations before adding measurement error and then back-transformed into Fisher's $z$ before the next step. At this point we have $k$ effect sizes $r_i$ in Pearson's $r$ or Fisher's $z$ with sampling error and measurement error. We then applied either the HV meta-analytic model or the HS model, as described in the section 'the meta-analytic model'. The procedure described in the current section was replicated 10,000 times for each combination of parameter values.

## Parameter values

We ran our analyses across a range of within-study sample sizes $N$ and number of studies $K$. Within meta-analyses we used a fixed sample size across all studies such that $N = N_i$. We made this choice rather than treating sample size as a random variable (as done by e.g., Field, 2005; Brannick et al., 2019) to be able to observe the effect of changes in sample size on estimates, and because we are focused on the average heterogeneity estimates rather than its sampling variance. We base our range of sample sizes on empirical estimates of typical sample size in correlational research in psychology. Fraley et al., report median sample sizes for between-person studies in 9 psychology journals between 2011 - 2018 which ranged between 69 - 496 depending on journal and year. Sassenber and Ditrich (2019) find a median sample size of 110 (interquartile range, 71 - 195) across 4 journals and years (2009, 2011, 2016, and 2018) in social psychology, and Bakker et al. (personal communication, 2023) find median sample sizes between 24 - 184 for 6 journals in psychology for the years 1995, 2006, 2019. Given these empirical findings we consider the following sample size values {50, 100, 150, 200}.

When focusing on the bias of heterogeneity estimate, the number of meta-analyzed studies, $K$, would not matter for an unbiased estimator. However, both the heterogeneity estimator we use, restricted maximum likelihood, and most other heterogeneity estimators are truncated at zero (Viechtbauer, 2005). Lower values for $K$ then results in a larger proportion of truncated estimates below zero because of increased sampling variance in $\tau^2$. As such, values of $K$ can affect bias, especially at lower levels of heterogeneity. We consider the following values of $K$: {5, 20, 40, 100}.

We vary the grand mean $\theta$ from 0 to 0.6 in steps of 0.1 to cover all realistic effect sizes and explore the boundaries of the interaction between effect size and reliability variance. For Fisher's $z$ we transformed these value into equivalent effect sizes at the Fisher's $z$ scale. For context to  these values consider the empirical estimates of typical correlational effect sizes provided by Nuijten et al. (2020) and Schäfer & Scharz (2019). Nuijten et al. performed a meta-meta-analysis on the fields of intelligence research and reports a median meta-analytic effect size of 0.24 for predictive validity and correlational studies. This estimate is likely positively biased due to publication bias and selective reporting. Schäfer & Scharz report a median $r$ of 0.16 amongst preregistered research in psychology in general, with an 'upper median' (i.e., the 83.35% quantile) of 0.41. This estimate is likely less affected by positive bias but should not be expected to be unbiased as there appears to be some positive bias also in preregistered research on average (Scheel, 2021).

We defined the between-studies standard deviation ($\tau$) to cover a wide range of variability in true effect sizes and empirical estimates reported in the psychological literature. Van Erp et al. (2017) provide empirical heterogeneity estimates from 747 meta-analyses in 61 article published in the journal Psychological Bulletin between 1990 - 2013. The median (uncorrected)Pearson's $r$ $\tau$ value in these data was 0.17 (interquartile range 0.1 - 0.24). The point of this study is that empirical heterogeneity estimates may not be accurately estimated. Nonetheless, we selected approximately the interquartile range of $\tau$-values reported in van Erp et al. for the product-moment correlation, both to make sure we covered the empirically reported range of values and because we considered them to represent reasonable variability on the product-moment correlation scale. The values we selected were {0, 0.1, 0.15, 0.2}. We selected the largest heterogeneity level to avoid excessive truncation for our maximum average effect size of 0.6: $\theta_i \sim N(\theta = 0.6, \tau = 0.2)$ implies approximately 95% of effect sizes will be below 1. Our values are similar to those used by Brannick et al. (2019) in their simulation study on the performance of different heterogeneity estimators after corrections for unreliability (they used $\tau = $ 0, 0.08, 0.13, 0.2).

Fisher's $z$ is measured at a different scale and it is not possible to directly convert $\tau$-values expressed in Pearson's $r$ into corresponding $\tau$-values on the Fisher's $z$ scale. To compare results between Fisher's $z$ and the product-moment correlation we defined heterogeneity on the $I^2$ scale. The $I^2$ index is a relative measure of heterogeneity describing the percentage of total variance that is due to heterogeneity (Higgins, 2002; 2003), and as such is measured from 0 - 100%. The $I^2$ index can be defined as $I^2 = \frac{\tau^2}{s^2 + \tau^2}$ where $s^2 = \frac{\Sigma w_i(k - 1)}{(\Sigma w_i)^2 - \Sigma w_i^2}$ and $w_i$ is the precision $w_i = 1 / \sigma^2$. This means that the $I^2$ value generally depends on both the number of studies $K$ and the sample size $N_i$ in these studies (Borenstein, 2017). However, because we keep $N_i$ fixed across studies, the value of $I^2$ depends only on $N = N_i$ and the heterogeneity.

We thus computed the $I^2$ index for each sample size condition and $\tau$-values defined in Pearson's $r$, and then computed corresponding $\tau$-values on the Fisher's $z$ scale given these $I^2$-values and sample sizes. Two complications were that the value for $\sigma^2$ varies with effect size for Pearson's $r$ and that effect size varies across studies because of $\tau$. We resolved this by setting $\theta = 0$ and computing the expected $\sigma^2$ given $\tau$ and sample size and then using this expected value to compute $I^2$. We used the 'law of the unconscious statistician' to compute the expected value of $\sigma^2$, which says that the expected value of a function $g(X)$ of a random variable can be expressed in terms of the probability distribution of X: $E[g(X)] = \int_{-\infty}^\infty g(x) f(x) dx$. In our case, $g(x)$ corresponds to the sampling variance of the product-moment correlation, $f(x) \sim N(\theta, \tau^2)$ and because the product-moment correlation is bounded at {-1, 1} we set $\infty$ instead to 0.999. The resulting $\tau$ values for Fisher $z$ were only minimally different from {0, 0.1, 0.15, 0.2}. For example, for $N = 50$ the corresponding Fisher $z$ $\tau$-values were {0, 0.1031263, 0.1566007, 0.2123514} and as $N$ increased heterogeneity values were more similar (see supplement XX).

Finally, we defined average reliability levels and their standard deviation based on empirical estimates from the literature. Flake et al. (2017) report that based on 245 estimates of Cronbach's alpha in psychology the average estimate was 0.79 with a standard deviation of 0.13. The interquartile range was approximately 0.68 - 0.87 for studies wherein the associated scale lacked a reference and 0.79 - 0.88 for scales that did have a reference. Sánchez-Meca et al. (2003) report the reliability estimates based on five 'reliability generalization studies'. These five studies summarized reliability in 25 - 51 primary studies (184 total), and the mean reliability ranged from 0.767 to 0.891 with standard deviations ranging between 0.034 - 0.133. Given some likely positive bias in such empirical values (Hussey et al., 2023), we examined the following mean reliabilities $\{0.6, 0.7, 0.8, 0.9\}$, and standard deviations $\{0, 0.05, 0.1, 0.15\}$.

# Results

The overall bias in heterogeneity estimates due to imperfect reliability in primary studies tended to be negative. We found that the positive bias due to variance in reliability across studies was less than the negative bias due to average unreliability across all levels of heterogeneity in our primary analysis. As such, we report here only the condition with maximum variance in reliability (SD = 0.15). Analyses with lower levels of reliability variance can be found in supplement XX. In addition, although a smaller number of studies in a meta-analysis (k) leads to a larger positive bias in the absence of heterogeneity, as would be expected, there was no practical difference in the bias once heterogeneity was above 0.1. As such, we report only results for k = 20 in the main manuscript. Analyses with other values for k (5, 40, 200) can be found in supplement XX. Finally, the effect on bias in heterogeneity estimates of varying within-study sample size was relatively small, and we similarly report only results for N = 150 in the main manuscript and present results for other sample sizes in supplement XX.

Figure 2 show the absolute and relative bias in heterogeneity estimates for different levels of heterogeneity (measured in $\tau$) and meta-analytic models. Each column of Figure 2 corresponds to one nominal level of heterogeneity ($\tau$ $\epsilon$ {0, 0.1, 0.15, 0.2}) and each row to a different analysis (HV with Fisher $z$, HV with Pearson's $r$, and HS). The black unbroken lines indicate actual heterogeneity, which may differ from nominal heterogeneity because of truncation or in the case of Fisher $z$ because the comparable heterogeneity levels were slightly different (see methods). The different dashed lines in Figure 2 shows the $\hat{\tau}$ estimates for four different levels of average reliability across studies, and the x-axis shows the average (superpopulation) effect size. As can be seen, the bias is marginally worse for Fisher $z$ than the other two methods (although Fisher $z$ is on a different scale) but the results are approximately the same for all three methods. As such, we will focus in the remainder of the results on Pearson's $r$ (row two).

```{r Figure 2, echo=FALSE, out.width = '100%'}
knitr::include_graphics("../figures/z-r-hs-plot.png")
```

_Figure 2._ Unreliability in primary studies leads to a net negative bias when heterogeneity is positive.  The x-axis indicates average effect size and the y-axis estimated heterogeneity in standard deviations. Columns indicate the nominal true heterogeneity standard deviation $\tau$. Due to truncation in Pearson's $r$ or translation to Fisher's $z$ these values may differ from actual true heterogeneity standard deviation (black solid lines). Each dashed line correspond to an average reliability in primary studies. Standard deviation in reliability was 0.15. Code to reproduce figure: LINK.

As expected, for zero heterogeneity (leftmost column Figure 2) we see an overestimate of heterogeneity that increases with effect size. Although Pearson's $r$ has a small positive bias due to the dependance between effect size and sampling variance (supplement XX), more generally there are two sources of positive bias in the absence of heterogeneity. First, and the only one that applies when $\mu = 0$, there is positive bias due to the truncation of negative heterogeneity estimates (Viechtbauer, 2005). Second, there is bias due to the variance in reliability. For example, for $mu = 0.2$ (the closest value to the median correlation reported by Schäfer & Schwarz) and $\bar{R} = 0.8$ the bias for Pearson's $r$ compared to true zero heterogeneity is `r in_text$bias_tau_zero$mu_0.2`. For $\mu = 0.4$ (the upper median reported by Schäfer & Schwarz) and $\bar{R} = 0.8$ the bias is `r in_text$bias_tau_zero$mu_0.4`. For the highest average reliability (0.9) a somewhat less inclined slope can be seen. This is because the standard deviation for this reliability level is severely truncated when the reliability is bounded at 1. The increase in positive bias with effect size can also be seen in the presence of heterogeneity (all other columns). However, it is superseded by the negative bias.

For all heterogeneity levels in Figure 2 above zero we see an overall underestimate of heterogeneity due to unreliability in primary studies. This negative bias can be relatively severe, is worse with lower effect size and lower average reliability, and worsens in an absolute sense but improves in a relative sense as heterogeneity increases. For example, for $\bar{R} = 0.8$, $\mu = 0.2$, $\tau = 0.1$, the bias is `r in_text$bias_increasing_tau$tau_0.1` for Pearson's $r$, for $\bar{R} = 0.8$, $\mu = 0.2$, $\tau = 0.15$ the bias is `r in_text$bias_increasing_tau$tau_0.15`, and for $\bar{R} = 0.8$, $\mu = 0.2$, $\tau = 0.2$ the bias is `r in_text$bias_increasing_tau$tau_0.2`. For an effect size of $\mu = 0.2$ true heterogeneity between 0.1 - 0.2 is underestimated by between `r round(100*in_text$underestimate_mu02$underestimate_min$underestimate, 0)`% ($\tau = 0.2$, $\bar{R} = 0.9$) and `r round(100*in_text$underestimate_mu02$underestimate_max$underestimate, 0)`% ($\tau = 0.1$, $\bar{R} = 0.6$).

To explore further under what conditions the positive and negative bias in heterogeneity estimates would reach an equilibrium, we added an analysis for several conditions with $\tau < 0.1$. We used the following values {0.02, 0.04, 0.06, 0.08}. Figure 3 presents the results of this analysis for Pearson's $r$. Because these lower heterogeneity levels can be expected to be affected by truncation to a larger extent (which results in a positive bias), we present results for four different numbers of studies in the meta-analysis (rows Figure 3, lower $k$ is associated with larger positive bias). The standard deviation in reliabilities is again 0.15. Figure 3 shows that even when only little heterogeneity is present, it will generally be underestimated. If $\mu < 0.3$ then imperfect reliability leads to a negative bias in heterogeneity estimates for all $\tau < 0.06$ in Pearson's $r$. Even for lower levels of heterogeneity, heterogeneity will generally be positively biased only when average effect size is large or the number of meta-analyzed studies is small.

```{r Figure 3, echo=FALSE, out.width = '100%'}
knitr::include_graphics("../figures/r_tau_0.02-0.08.png")
```

_Figure 3._ Unreliability in primary studies typically leads to a net negative bias even for small degrees of heterogeneity. Effect size is Pearson's $r$ and rows indicate the number of studies in the meta-analysis. The x-axis indicates average effect size and the y-axis estimated heterogeneity in standard deviations. Columns indicate the nominal true heterogeneity standard deviation $\tau$. Due to truncation in Pearson's $r$ or translation to Fisher's $z$ these values may differ from actual true heterogeneity standard deviation (black solid lines). Each dashed line correspond to an average reliability in primary studies. Standard deviation in reliability was 0.15. Code to reproduce figure: LINK.

# Discussion

We used Monte-Carlo simulations to study the total bias that should be expected in meta-analytic heterogeneity estimates due to unreliability in primary studies. Our results indicate that uncontrolled unreliability in primary studies should under almost all circumstances be expected to lead to an severe underestimate of heterogeneity in meta-analysis. We observed a positive bias in heterogeneity estimates due to unreliability only under zero heterogeneity, or under exceptional circumstances (i.e., unusual combinations of low heterogeneity, large variance in primary study reliabilities, large effect sizes, and low number of meta-analyzed studies). This generally negative bias was observed under conditions that should have been favorable to a positive bias (i.e., large variance in primary study reliability), indicating that negative bias should probably be expected in all cases except for zero true heterogeneity. Our results were approximately the same whether one used a classic random-effects meta-analytic model (Hedges & Vevea, 1998) or a 'bare-bones' psychometric meta-analytic model (Hunter & Schmidt, 2015), and whether one used Pearson's $r$ as the effect size or Fisher's $z$, although the negative bias was slightly larger with Fisher's $z$. The severity of the underestimation of heterogeneity suggests meta-analyses in psychology often miss moderators and a risk that theoretical development in psychology has been misguided by meta-analyses.

The negative bias in heterogeneity estimates can be extreme if reliability is low. For a typical effect size around $r$ = 0.2, larger heterogeneity ($\tau > 0.1$) can be expected to be underestimated by about 15% - 60% primarily depending on average reliability in primary studies (higher reliability leads to a smaller underestimate). The underestimate is worse with lower average effect size, but although it improves with larger effect sizes it is likely to remain negative, and less variability in primary study reliabilities will lead to larger negative bias. As true heterogeneity increases, the proportional underestimate decreases to some degree, but the underestimate in absolute terms will worsen. Although small degrees of true heterogeneity ($\tau < .08$) can be overestimated under certain conditions, this would typically require a combination of a small number of studies, large variance in study reliabilities, and a large average effect size. The severity and generality of the underestimation means that meta-analysts cannot ignore the effect of reliability on their heterogeneity estimates.

If true heterogeneity is zero, there is a positive bias in observed heterogeneity estimates, but for even small degrees of true heterogeneity there tends to be a negative bias in observed estimates. As such, it is not possible to know whether a small degree of observed heterogeneity is due to overestimated true zero heterogeneity or underestimated small heterogeneity. However, for observed heterogeneity above $\tau > 0.025$ one can usually be confident that the estimate is an underestimate of true heterogeneity rather than an overestimate of zero heterogeneity if the average effect sizes is below approximately $r = 0.2$.

Similarly, if researchers find zero heterogeneity in their observed estimates, true heterogeneity is most likely to be zero or small. Larger true heterogeneity (e.g., $\tau > 0.08$) has a higher expected observed heterogeneity value even when average reliability is as low as 0.6, and as such is expected to result in zero estimates less often than zero or smaller true heterogeneity for fixed conditions. For example, for the conditions reported in the main article (i.e., K = 20, N = 150, $\sigma_R$ = 0.15), an assuming a typical (reported) average reliability of 0.8 and a zero average correlation, then an observed zero heterogeneity estimate is XX times more likely to correspond to true zero heterogeneity than $\tau = 0.1$. That said, an observed zero heterogeneity estimate is still possible to observe under larger true heterogeneity levels as variability in heterogeneity estimates is often large in practice (Ioannidis; ref2). In addition, separating between zero and small heterogeneity is infeasible considering realistic sample sizes in meta-analyses in psychology (Olsson-Collentine et al., 2020). In our view, the distinction between zero and small heterogeneity in meta-analysis is primarily a hypothetical one, as we consider low heterogeneity and meta-analysis of a large number of studies unlikely to cooccur in practice.

We found that using Pearson's $r$ or Fisher's $z$ as our effect size resulted in very similar degrees of bias in heterogeneity estimates due to uncorrected unreliability in primary studies. The underestimate is slightly worse for Fisher's $z$ than for Pearson's $r$. The usage of Pearson's $r$ adds a slight positive bias to heterogeneity estimates (due to its bounded nature and effect size dependent sampling variance) which apparently is beneficial for heterogeneity estimates when not correcting for unreliability. However, the additional negative bias when using Fisher's $z$ is small and we consider the difference negligible in the context of the severe underestimates generated by unreliability.

## Should meta-analysts correct for unreliability in primary studies?

Despite the severity of underestimation due to unreliability, it is neither straightforward to correct for unreliability using the formulas of classical test theory nor self-evident that doing so is the optimal solution. Although our simulation results are not affected by whether researchers are interested in observed measures or latent constructs, it is safe to assume that the majority of researcher in psychology are interested in latent constructs. A central assumption behind applying the classical test theory correction to examine latent constructs is that disattenuated effect sizes (“true scores”) correspond approximately to latent construct scores (Hunter & Schmidt, 2015; Borsboom & Mellenbergh, 2002). Given that these assumptions hold, or one is satisfied with correcting for measurement error regardless of the possibility of a latent construct interpretation, there are several complexities that arise when attempting to correct for unreliability in practice. First, reliability estimates are lower bound estimates, meaning systematically correcting for unreliability can be expected to lead to overestimated effect sizes and heterogeneity. Unfortunately, the most reported reliability estimate (Chronbach’s alpha; Flake et al., 2017) is known to also underestimate the lower bound (Sijtsma, YEAR). Second, estimates of reliability (e.g., those based on a single measurement occasion such as Cronbach’s alpha) do not necessarily account for all types of measurement error (Hunter & Schmidt, 2015, p. XX, section “Using the appropriate reliability coefficient”). If so, they overestimate reliability. Hence, from a meta-analytic perspective, most reliability estimates available for correcting effect sizes are likely to be affected by both a positive and a negative bias compared to the true reliability, but what the total effect is likely to be is uncertain (and hence how accurate a heterogeneity estimate based on these corrections would be). Even so, in view of these two problems one could argue that an imperfect correction of unreliability is likely to be preferable over no correction (e.g., Oswald, YEAR), especially given the severe underestimation of heterogeneity. Further study would be useful to provide evidence to whether this is truly the case.

Another concrete problem is that reliability estimates are sometimes not reported in primary studies. Missing reliability estimates must then be imputed, which is usually done by taking the average of observed reliabilities (e.g., Gnambs & Sengewald. 2023) or imputing some ‘typical’ reliability (Hunter & Schmidt, 2015, p. XX). Both approaches have problems. There can be much debate about what a ‘typical’ reliability is (LeBreton et al., 2014) and selecting different values can have large effect on estimates. Mean imputation is viewed as inappropriate by experts in missing data (e.g., Enders, 2010, p. 42-43; van Buuren, 2018, p. 12; Schafer, 2002, p. 159) partly because the pooled estimate will be biased unless observed values coincide with the population mean, particularly so if missing values are related to the value of the statistic (and it seems highly likely that missingness is more common with low reliability). However, it is not clear that more complex imputation procedures make sense except with substantially higher number of studies in than is common in meta-analyses in psychology.

These are practical concerns with applying reliability corrections, but a more pernicious challenge to treating disattenuated effect sizes as equivalent to construct scores is the problem of validity (Borsboom & Mellenbergh, 2002; Hunter & Schmidt, 2015, p. 41 – 53, 2nd edition). First, doing so requires assuming that the constructs underlying measurements are unidimensional in nature (Borsboom & Mellenbergh, 2002), which in many cases is a strong assumption. Second, to treat disattenuated effect sizes as latent construct scores requires assuming that the applied measurement instruments measure the construct of interest directly rather than through some proxy variable. Although corrections akin to the classic unreliability correction can be applied for simple causal models, these methods quickly become insufficient as the relationship between latent constructs grows in complexity (Hunter & Schmidt, 2015, p. 41 – 53, 2nd edition). As such, some argue that simple corrections for unreliability under the classical test theory framework are untenable and potentially misleading if one’s interest lies in latent constructs (Borsboom & Mellenbergh, 2002). From this perspective, researchers should consider modern psychometric models such as Structural Equation Models (SEM; e.g., REF) which permit delving deeper into measurements and assumptions about latent construct relationships.

Meta-analysis can be conceptualized as a SEM model, which holds promise that the benefits of modern psychometric theory might be applied also when the interest is in summarizing research across multiple studies. In Meta-Analytic Structural Equation Modeling (MASEM; e.g., Cheung & Chan, 2005) unreliability in primary studies can be corrected in three ways (Gnambs & Sengewald, 2023): 1) by estimating a SEM model in each primary study and then pooling estimates (parameter-based MASEM: Cheung, 2015, p. 241), 2) by directly modeling latent variables in the MASEM, or 3) by first correcting correlations before introducing them into the MASEM framework (i.e., fundamentally identical to the classical test theory approach). The first option is limited because it is rarely the case that the same SEM model can be fit across multiple independent datasets, except in highly structure cases (e.g., Brunner et al., 2022). The second option requires item-level statistics rather than summary values, which are rarely available to meta-analysts. The third option, naturally, suffers the same challenges and assumptions as corrections under classical test theory. As such, the potential to use MASEM to take advantages of modern psychometric theory when correcting for unreliability is challenging in practice. That said, multi-lab replication projects, which are increasingly common in psychology (e.g., REF1, REF2), are well-suited to take advantage of MASEM.

In the end, meta-analysis depends on the quality of the primary studies that make up its data. As long as researchers in psychology underappreciate the importance of measurement, meta-analytic estimates will be poor. For meta-analysts that consider acceptable the challenges and assumptions of corrections to unreliability, the many caveats inherent in doing so should be acknowledged and estimates based on both corrected and uncorrected estimates reported. Meta-analysts who consider corrections untenable need to consider that heterogeneity estimates may be severely underestimated. In either case, meta-analysts should get used to extracting reliability information whenever available from primary studies. The good news is that large true heterogeneity ($\tau > 0.1$) is on average distinguishable from zero or small estimates even in observed heterogeneity estimates.

## Limitations

In our simulations we assumed equal reliability for the measures $X$ and $Y$ (i.e., $R_i = R_{xx'} = R_{yy'}$) within a study, which is not realistic. However, as we never use the reliabilities individually (i.e., only expressed as $\sqrt{R_{xx'}} \times \sqrt{R_{yy'}}$) we do not expect major impact on our results. Consider that if $R_i = 0.7$ then $\sqrt{R_{xx'}} \times \sqrt{R_{yy'}} = 0.7$ whereas if if $R_{xx'}$ and $R_{yy'}$ took other values from a distribution centered on 0.7 this value would be lower (e.g., if $R_{xx'} = 0.5$ and $R_{yy'} = 0.9$ then $\sqrt{R_{xx'}} \times \sqrt{R_{yy'}} \approx 0.67$). As we use the results of $\sqrt{R_{xx'}} \times \sqrt{R_{yy'}}$ in our simulations to generating observed effect sizes from true effect sizes (i.e., $r_{xy} = \rho_{xy} \times \sqrt{R_{xx'}} \times \sqrt{R_{yy'}}$, where $\rho_{xy}$ is the true effect size after sampling error), letting the reliability for $X$ and $Y$ vary would lead to slightly larger attenuation of observed effect sizes on average. That is, letting the reliability of $X$ and $Y$ vary within studies would lead to larger negative bias in observed heterogeneity estimates than in our simulations, indicating that the effect might even be somewhat more severe than in our estimates.

We also only considered bias in heterogeneity due to unreliability in primary studies, but there exist other measurement artifacts such as restriction of range, dichotomization and more (Hunter & Schmidt, 2015). Although measurement error is the only measurement artifact that is always present, many of these other measurement artifacts may impact heterogeneity estimates when they are present. Based on Hunter & Schmidt’s arguments that measurement artifacts all tend to attenuate effect sizes in a similar way to unreliability, we expect that their net effect on heterogeneity estimates may similarly be a negative bias. The extent to which this prediction is true (and whether these measurement artifacts can be corrected for in MASEM; Gnambs & Sengewalds, 2023) awaits further research.

We also used a fixed sample size (N) across studies within a meta-analysis in our simulations, which is unrealistic. We showed in Supplement XX that increasing the fixed sample size led to slightly better observed heterogeneity estimates for low reliability. Implementing a distribution of sample sizes from which within meta-analyses sample sizes was drawn (e.g., as done by REF1; ref2) could be one way to increase realism in sample sizes, although to the detriment of interpretability. However, given a sufficiently large number of repetitions in our simulations, this would not change estimates of bias, assuming the same mean sample sizes were used. That said, moving from fixed sample sizes to a distribution of sample sizes would affect the variability of any estimates, but this was not the focus of our study.

We did not examine variance in heterogeneity estimates, or their root mean squared error. Our focus was on to what extent unreliability leads to bias in heterogeneity estimates and should be a concern for substantive researcher attempting to use meta-analysis to learn more about the contextual sensitivity of their research topic. As such, although we apply different meta-analytic models and effect sizes, the purpose was not to compare the efficiency of these different modes of estimation, but to show our results with the dominant meta-analytic approaches for correlations and demonstrate that they are not to any large degree dependent on the meta-analytic approach. As the effect of unreliability in primary studies is simply to change the heterogeneity of effect sizes the results of previous studies on the comparable performance of different heterogeneity estimators applies also here (e.g., REF1, REF2, REF3). That said, examining the variance of heterogeneity estimates could be useful to gain a better idea of how likely a particular observed heterogeneity estimate is to belong to different distributions of true heterogeneity. As this would depend on N, K, mean reliability, variability in reliability, average effect size, observed heterogeneity, and the true heterogeneity levels being compared, implementing such an analysis would probably be best done on a case-by-case basis (e.g., through a webapp such as developed for sensitivity analyses of the effect of publication bias on heterogeneity by Augusteijn et al., YEAR). Given the challenges in correcting for unreliability, such a sensitivity analysis could be a promising tool to reason around the impact of unreliability in applied meta-analysis.

Our simulations assume reliabilities to be known. This is never the case, and any estimate of reliability is at best a lower bound estimate. By assuming reliabilities to be known, we were able to show the bias of observed heterogeneity estimates compared to the ideal case of no measurement error. However, this may mislead readers to believe that a simple correction for unreliability will leave them with an unbiased estimate. As discussed in the section ‘should meta-analysts correct for unreliability?’ correcting for reliability based on estimated reliability is not straightforward and whether the resulting estimate will be unbiased is contingent on many factors and in many cases unresolved. As such, examining the effect of corrections based on estimated reliabilities was beyond the scope of this study.

## Conclusion

Imperfect measurement reliability in primary studies generally leads to a severe underestimate of observed meta-analytic heterogeneity. As few meta-analyses in psychology correct for unreliability in primary studies, heterogeneity is likely underestimated in a large proportion of meta-analyses in psychology. Consequently, variability in effects has been ignored, moderators potentially neglected and theoretical development in psychology been misguided by meta-analyses. Sophisticated methods (MASEM) for correcting measurement unreliability are only applicable in exceptional cases and corrections based on classical test theory come with caveats and assumptions. Accurate estimation of meta-analytic heterogeneity is difficult and will remain so unless measurement concerns (Flake & Fried, 2020; Flake et al., 2017; Kane et al., 2021) are taken seriously in primary research. The good news is that study designs (multi-lab replication studies) which can apply more sophisticated versions of MASEM are becoming increasingly common in psychology, and that large true heterogeneity can on average be distinguished from zero or small heterogeneity even in the absence of corrections to unreliability in primary studies.
