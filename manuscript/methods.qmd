---
format: docx
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```


# Methods

All code and data for this project are available at (OSF LINK). For our simulations and analyses we used R (REF) and we used the R package 'metafor' (REF) to perform meta-analyses and estimate heterogeneity. For parallel computing we took advantage of the R packages 'parallel' (REF) and 'parabar' (REF).

## Choice of effect size type for simulations

We focus on correlations as our effect size type in these simulations because correcting them for unreliability is straightforward and because they are the effect size type most commonly corrected in practice. However, unreliability attenuates all types of effect sizes and all can be corrected, either directly (standardized mean differences; Wiernick & Dahlke, 2020) or by first transforming them into correlations. Although effect sizes can generally be transformed into each other (e.g., Borenstein, 2009, p. 46) this should be done with care as it can sometimes affect conclusions (e.g., we found a violation of monotonicity in heterogeneity estimates when converting effect sizes into correlations in a previous project; Olsson-Collentine, 2020, supplement A).

Perhaps the most common way to meta-analyze correlations is to first transform them into Fisher $z$ correlations (Borenstein et al., 2009, p. 41 - 43). The Fisher $z$ transformation is the inverse hyperbolic tangent of the product moment correlation and can be computed as $z = 0.5 \times ln\left(\frac{1+r}{1-r}\right)$ and its variance approximated as $V_z = 1 / (n -3)$. There is controversy in the use of the Fisher $z$ transformation for meta-analysis (e.g., Hunter & Schmidt, 2015, p. XX, ch. 5 "Use of Fisher’s z in Meta-Analysis of Correlations"), largely related to whether Pearson's $r$ or Fisher's $z$ leads to less bias in average correlations (Schulze, 2007). We observe that estimating heterogeneity does not seem to have been the focus of the literature's discussion of the appropriateness of the Fisher $z$ transformation (Brannick et al., 2019; Field, 2005; Hafdahl & Williams, 2009, Hunter & Schmidt, 2015) and that the Fisher $z$ transformation is often applied in practice.

Using Pearson's $r$ for meta-analysis when interested in heterogeneity has the advantage that resulting estimates on this scale are more interpretable that on the Fisher $z$ scale. Although Fisher $z$ correlations can be back-transformed into Pearson's $r$ for interpretation, its heterogeneity cannot. A workaround is to compute a prediction interval and convert this interval onto the product-moment scale (REF). The width of the interval can then be used as an estimate of the heterogeneity. We do not believe this increases interpretability in our case and so prefer to directly estimate the heterogeneity. However, the disadvantage to using Pearson's $r$ when estimating heterogeneity is that 1) the effect size is bounded to {-1, 1} and 2) the sampling variance $$ depends on the effect size. The bounded nature of the Pearson correlation means that large levels of heterogeneity will lead to substantial truncation of effect sizes if the average effect size is also large. That the sampling variance covaries with effect size leads to a small positive bias in heterogeneity estimates (supplement XX). This can be solved by replacing $\rho$ with the average correlation across studies $\bar{r}$ (Hunter & Schmidt, 2015), and removing this covariance is also the primary intention behind the Fisher $z$ transformation. Fisher $z$ correlations also have the advantage that they are not bounded to {-1, 1}. A disadvantage to using Fisher $z$ correlations, in addition to interpretability of heterogeneity estimates, is that they require first backtransforming into product-moment correlations before adding measurement error, and then transforming to Fisher $z$ again for meta-analysis. Especially for large Fisher $z$ values (which can happen if the average effect size and heterogeneity are large) this transformation process may introduce inaccuracies in the estimates. Given the common use of both effect size types and their mix of advantages and disadvantages we report results for both.

## Meta-analytic model

We apply two meta-analytic models to ensure that our results are not model-dependent: the Hedges and Vevea (1998) random-effects  model and the Hunter and Schmidt (2015) 'bare-bones' random-effects model. The Hedges and Vevea (HV) model uses inverse variance weight computed as $w_i = 1 / (V_i + T^2)$ (e.g., Borenstein et al., 2010) where $V_i$ is the sampling variance of study $i$ and $T^2$ is the estimated between-studies variance. The Hunter and Schmidt (HS) model weighs studies by sample size $w_i = N_i$. The differences between these weights are relatively minor (Borenstein et al., 2010; Hunter and Schmidt, 2015, p. XX). Although the HV model is typically applied to Fisher z transformed correlations (e.g., Borenstein et al., p. 41-43), we apply it to both transformed and untransformed correlations for the reasons described in the previous section. The HS model typically includes correcting for unreliability or other measurement artifacts. However, we implement it without corrections ('bare-bones'; Hunter & Schmidt, 2015) because this allows us to focus on the bias in heterogeneity estimates rather than the performance of corrections.

Estimating heterogeneity in both the HV and bare-bones HS model essentially consists of comparing the observed variance in effect sizes with what would be expected from sampling error alone and then ascribing any excess observed variance to heterogeneity (Borenstein et al., 2010, Box 1; Hunter & Schmidt, 2015, p. XX). However, the process for doing so differs somewhat between methods. We apply the Restricted Maximum Likelihood (REML) estimator of heterogeneity when estimating heterogeneity in the Hedges and Vevea (1998) model, because it is the generally recommended heterogeneity estimator (Veroniki et al., 2016; Viechtbauer, 2005). Describing the REML estimator in detail is beyond the scope of this paper and we refer interested readers to the description in Viechtbauer (2005). The Hunter and Schmidt (2015) model estimates heterogeneity as $\tau^2 = \sigma^2_r - \sigma^2_e$ where  $\sigma^2_r$ is the observed variance across correlations and $\sigma^2_e$ is the average sampling error variance across studies, computed as $ \frac{\Sigma[N_i \sigma^2_{ei}]}{\Sigma N_i}$ (p. 87, 2nd edition) and $\sigma^2_{ei}$ is the usual sampling variance for the Pearson correlation but with $\rho$ replaced with the sample size weighted average correlation $\bar{r} = \frac{\Sigma n_i r_i}{\Sigma n_i}$ instead of $r_i$ as is common. A generalized version of the HS estimator for effect sizes other than correlations can be found in Viechtbauer (2005; 2015) (see ?rma).


## Simulation study design

Figure 1 provides an overview of the design of this simulation study. Our design was broadly the same for product-moment correlations and Fisher's $z$, with some minor differences we highlight in the text below detailing the procedure.

```{r Figure 1, out.width= "100%"}
knitr::include_graphics("../figures/flowchart-simulation-design.png")
```

(FIGURE 1: a flowchart summarizing the simulation design)

When generating data for meta-analysis we proceeded as follows. First,  we sampled $i = 1, ..., k$ true study effect sizes $\theta_i$ from $Ntrunc(\theta, \tau^2)$. That is, a normal distribution with a mean of $\theta$ and a variance of $\tau^2$ that we truncated at $\{-1, 1\}$ to stay within the bounds of the product-moment correlation. We implemented the truncation in R using inverse transform sampling (link: functions.r). For Fisher' $z$ no truncation was applied.

For each of the $k$ sampled true study effect sizes $\theta_i$ we then sampled one observed effect size $\gamma_i$ from a truncated normal distribution $Ntrunc(\theta_i, \sigma_i^2)$, using the same truncation procedure. The sampling variance $\sigma_i^2$ for each true effect size $\theta_i$ we computed using the standard formula for Pearson's $r$ $\sigma_i^2 =  \frac{(1 - \theta_i^2)^2}{N_i - 1}$ where $N_i$ is the total sample size for study $i = 1, ..., k$. For Fisher's $z$ no truncation was applied and we approximated the sampling variance as $\sigma^2_i = \frac{1}{N_i - 3}$. At this point we have $k$ sampled effect sizes $\gamma_i$ in Pearson's $r$ or Fisher's $z$ without measurement error.

To add measurement error we first sampled a reliability $R_i$ for each study from a truncated ($\{0, 1\}$) normal distribution $Ntrunc(\bar{R}, \delta)$, given some average reliability across studies $\bar{R}$ and standard deviation in reliability $\delta$. We assumed that both the dependent and independent variable within a study were measured with the same reliability such that $R_i = R_{iXX'} = R_{iYY'}$. We then computed observed effect sizes $r_i$ for each study given the attenuation formula $r_{i} = \gamma_i \times \sqrt{R_{XX'}} \times \sqrt{R_{YY'}}$. Because we assumed equal reliability in $X$ and $Y$ this simplifies to $r_i = \gamma_i \times R_i$.   Fisher's $z$ we transformed to product-moment correlations before adding measurement error and then back-transformed into Fisher's $z$ before the next step. At this point we have $k$ effect sizes $r_i$ in Pearson's $r$ or Fisher's $z$ with sampling error and measurement error. We then applied either the HV meta-analytic model or the HS model, as described in the section 'the meta-analytic model'. The procedure described in the current section was replicated 10,000 times for each combination of parameter values.

## Parameter values

We ran our analyses across a range of within-study sample sizes $N$ and number of studies $K$. Within meta-analyses we used a fixed sample size across all studies such that $N = N_i$. We made this choice rather than treating sample size as a random variable (as done by e.g., REF1, REF2) to be able to observe the effect of changes in sample size on estimates, and because we are focused on the average heterogeneity estimates rather than its sampling variance. We base our range of sample sizes on empirical estimates of typical sample size in correlational research in psychology by XXX (still needs lookup) and consider the following values {VALUES}.

When focusing on the bias of heterogeneity estimate, the value of $K$ would not matter for an unbiased estimator. However, both the heterogeneity estimator we use, restricted maximum likelihood, and most other heterogeneity estimators are truncated at zero (REF Viechtbauer). Lower values for $K$ then results in a larger proportion of truncated estimates below zero because of increased sampling variance. As such, the values of $K$ can affect bias, especially at lower levels of heterogeneity. In their review of meta-analyses in psychology, van Erp et al. (YEAR) find $K$ values ranging from XX to XX, and in our simulations we consider the following values {VALUES}.

We vary the grand mean $\theta$ from 0 to 0.6 in steps of 0.1 to cover all realistic effect sizes and explore the boundaries of the interaction between effect size and reliability variance. For Fisher's $z$ we transformed these value into equivalent effect sizes at the Fisher's $z$ scale. For context to  these values consider the empirical estimates of typical correlational effect sizes provided by Nuijten et al., (REF) and Schäfer & Scharz (REF). Nuijten et al. performed a meta-meta-analysis on the fields of RESEARCH and reports a median meta-analytic effect size of 0.24 for predictive validity and correlational studies. This estimate is likely positively biased due to publication bias and selective reporting. Schäfer & Scharz report a median $r$ of 0.16 amongst preregistered research in FIELD, with an 'upper median' (i.e., the 83.35% quantile) of 0.41. This estimate is likely less affected by positive bias but should not be expected to be unbiased (REF Scheel).

We defined the between-studies standard deviation ($\tau$) to cover a wide range of variability in true effect sizes and empirical estimates reported in the psychological literature. Van Erp et al., (REF) provide empirical heterogeneity estimates from XX meta-analyses in XX article published in the journal Psychological Bulletine between 1990 - 2013. The median (uncorrected)Pearson's $r$ $\tau$ value in these data was 0.17 (interquartile range 0.1 - 0.24). The point of this study is that empirical heterogeneity estimates may not be accurately estimated. Nonetheless, we selected approximately the interquartile range of $\tau$-values reported in van Erp et al. for the product-moment correlation, both to make sure we covered the empirically reported range of values and because we considered them reasonable variability on the product-moment correlation scale. The values we selected were {0, 0.1, 0.15, 0.2}. We selected the largest heterogeneity level to avoid excissive truncation for our maximum average effect size of 0.6: $\theta_i \sim N(\theta = 0.6, \tau = 0.2)$ implies approximately 95% of effect sizes will be below 1. Our values are similar to those used by Brannick et al. (2019) in their simulation study on the performance of different heterogeneity estimators after corrections for unreliability (they used $\tau = $ 0, 0.08, 0.13, 0.2).

Fisher's $z$ is measured at a different scale and it is not possible to directly convert $\tau$-values expressed in Pearson's $r$ into corresponding $\tau$-values on the Fisher's $z$ scale. To be able to compare results between Fisher's $z$ and the product-moment correlation we defined heterogeneity on the $I^2$ scale. The $I^2$ index is a relative measure of heterogeneity describing the percentage of total variance that is due to heterogeneity (REF Higgins), and as such is measured from 0 - 100%. The $I^2$ index can be defined as $I^2 = \frac{\tau^2}{s^2 + \tau^2}$ where $s^2 = \frac{\Sigma w_i(k - 1)}{(\Sigma w_i)^2 - \Sigma w_i^2}$ and $w_i$ is the precision $w_i = 1 / \sigma^2$. This means that the $I^2$ value generally depends on both the number of studies $K$ and the sample size $N_i$ in these studies (REF: Higgins, I2 is not an absolute measure of heterogeneity). However, because we keep $N_i$ fixed across studies (see below), the value of $I^2$ depends only on $N = N_i$ and the heterogeneity.

We thus computed the $I^2$ index for each sample size condition and $\tau$-values defined in Pearson's $r$, and then computed corresponding $\tau$-values on the Fisher's $z$ scale given these $I^2$-values and sample sizes. Two complications were that the value for $\sigma^2$ varies with effect size for Pearson's $r$ and that effect size varies across studies because of $\tau$. We resolved this by setting $\theta = 0$ and computing the expected $\sigma^2$ given $\tau$ and sample size and then using this expected value to compute $I^2$. We used the 'law of the unconscious statistician' to compute the expected value of $\sigma^2$, which says that the expected value of a function $g(X)$ of a random variable can be expressed in terms of the probability distribution of X: $E[g(X)] = \int_{-\infty}^\infty g(x) f(x) dx$. In our case, $g(x)$ corresponds to the sampling variance of the product-moment correlation, $f(x) \sim N(\theta, \tau^2)$ and because the product-moment correlation is bounded at {-1, 1} we set $\infty$ instead to 0.999. The resulting $\tau$ values for Fisher $z$ are only minimally different from {0, 0.1, 0.15, 0.2}. For example, for $N = 150$ the corresponding Fisher $z$ $\tau$-values were {VALUES} and as $N$ increased heterogeneity values were more similar (see supplement XX).

Finally, we defined average reliability levels and their standard deviation based on empirical estimates from the literature. Flake et al. (YEAR) report that based on 245 estimates of Cronbach's alpha in psychology the average estimate was 0.79 with a standard deviation of 0.13. The interquartile range was approximately 0.68 - 0.87 for studies wherein the associated scale lacked a reference and 0.79 - 0.88 for scales that did have a reference. Sanchez-Meca et al. (REF) report the reliability estimates based on five 'reliability generalization studies'. These five studies summarized reliability in 25 - 51 primary studies (184 total), and the mean reliability ranged from 0.767 to 0.891 with standard deviations ranging between 0.034 - 0.133. Given some likely positive bias in such empirical values, we examined the following mean reliabilities $\{0.6, 0.7, 0.8, 0.9\}$, and standard deviations $\{0, 0.05, 0.1, 0.15\}$.
