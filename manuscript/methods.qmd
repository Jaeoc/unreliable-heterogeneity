---
format: docx
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```


# Methods

All code and data for this project are available at (OSF LINK). For our simulations and analyses we used R (REF) and we used the R package 'metafor' (REF) to perform meta-analyses and estimate heterogeneity, For parallel computing we took advantage of the R packages 'parallel' (REF) and 'parabar' (REF).

## Choice of effect size type for simulations

We use correlations as our effect size type in these simulations because correcting them for unreliability is straightforward and because they are the effect size type most commonly corrected in practice. However, unreliability attenuates all types of effect sizes and all can be corrected, either directly (standardized mean differences; Wiernick & Dahlke, 2020) or by first transforming them into correlations (with associated assumptions).

Perhaps the most common way to meta-analyze correlations is to first transform them into Fisher's z correlations (Borenstein et al., 2009, p. 41 - 43). The Fisher z transformation is the inverse hyperbolic tangent of the product moment correlation and can be computed as $equation$ and its variance approximated as $equation2$. There is some controversy in the use of the Fisher z transformation for meta-analysis (for a good summary see Brannick et al., 2019, p. 492), in particular among proponents of Hunter and Schmidt's (2015) meta-anlytic models.

We perform simulations both on product-moment correlations and Fisher z transformed correlations. For our purpose of estimating heterogeneity, Fisher z correlations have several advantages: 1) their sampling variance is not dependent on the size of the correlation, 2) they are not bounded to {-1, 1}. However, they also have two disadvantages: 1) that the heterogeneity estimate is measure on the Fisher z scale, which makes it hard to interpret. A workaround is to compute a prediction interval and convert this interval onto the product-moment scale (REF) for an indirect estimate of the heterogeneity. Second, adding measurement error to the Fisher z correlations requires first transforming them into product-moment correlations, adding measurement error, and then backtransforming to Fisher z for meta-analysis.

The primary advantage to using product-moment correlations in the meta-analysis when interested in heterogeneity is that values on this scale are more interpretable that on the Fisher z scale. The disadvantage is that the sampling variance is dependent on the effect size since it is computed as $equation$. Given the common use of both effect size types and their mix of advantages and disadvantages we report results for both.

## Simulation study design

Figure 1 provides an overview of the design of this simulation study. Our design was broadly the same for product-moment correlations and Fisher's $z$, with some minor differences we highlight in the text below detailing the procedure.

```{r Figure 1, out.width= "100%"}
knitr::include_graphics("../figures/flowchart-simulation-design.png")
```

(FIGURE 1: a flowchart summarizing the simulation design)

When generating data for meta-analysis we proceeded as follows. First,  we sampled $i = 1, ..., k$ true study effect sizes $\theta_i$ from $Ntrunc(\theta, \tau^2)$. That is, a normal distribution with a mean of $\theta$ and a variance of $\tau^2$ that we truncated at $\{-1, 1\}$ to stay within the bounds of the product-moment correlation. We implemented the truncation in R using inverse transform sampling (link: functions.r). For Fisher' $z$ no truncation was applied.

For each of the $k$ sampled true study effect sizes $\theta_i$ we then sampled one observed effect size $\gamma_i$ from a truncated normal distribution $Ntrunc(\theta_i, \sigma_i^2)$, using the same truncation procedure. The sampling variance $\sigma_i^2$ for each true effect size $\theta_i$ we computed using the standard formula for Pearson's $r$ $\sigma_i^2 =  \frac{(1 - \theta_i^2)^2}{N_i - 1}$ where $N_i$ is the total sample size for study $i = 1, ..., k$. For Fisher's $z$ no truncation was applied and we approximated the sampling variance as $\sigma^2_i = \frac{1}{N_i - 3}$. At this point we have $k$ sampled effect sizes $\gamma_i$ in Pearson's $r$ or Fisher's $z$ without measurement error.

To add measurement error we first sampled a reliability $R_i$ for each study from a truncated ($\{0, 1\}$) normal distribution $Ntrunc(\bar{R}, \delta)$, given some average reliability across studies $\bar{R}$ and standard deviation in reliability $\delta$. We assumed that both the dependent and independent variable within a study were measured with the same reliability such that $R_i = R_{iXX'} = R_{iYY'}$. We then computed observed effect sizes $r_i$ for each study given the attenuation formula $r_{i} = \gamma_i \times \sqrt{R_{XX'}} \times \sqrt{R_{YY'}}$. Because we assumed equal reliability in $X$ and $Y$ this simplifies to $r_i = \gamma_i \times R_i$.   Fisher's $z$ we transformed to product-moment correlations before adding measurement error and then back-transformed into Fisher's $z$ before the next step. At this point we have $k$ effect sizes $r_i$ in Pearson's $r$ or Fisher's $z$ with sampling error and measurement error, ready for meta-analysis.

We meta-analyzed the $k$ effect sizes using a random effects model with inverse variance weights and the Restricted Maximum Likelihood (REML) estimator of heterogeneity. We choose the REML estimator because it is a heterogeneity estimator that performs well and is widely implemented (e.g., REF, REF2), but also show our results using the classic DerSimonian-Laird (DL) estimator in supplement XX. We used inverse variance weights due to their widespread use, but as we used a fixed sample size across studies within a meta-analysis (see next section) the weights are of little importance. As a sensitivity analysis, we also implemented the Hunter and Schmidt (2015) model which uses a different heterogeneity estimator and found qualitatively similar results (supplement XX). The procedure described in this section was replicated 10,000 times for each combination of parameter values.

## Parameter values

We vary the grand mean $\theta$ from 0 to 0.6 in steps of 0.1 to cover all realistic effect sizes and explore the boundaries of the the interaction between effect size and reliability variance. For Fisher's $z$ we transformed these value into equivalent effect sizes at the Fisher's $z$ scale. To put these values into context we turn to the empirical estimates of typical correlational effect sizes provided by Nuijten et al., (REF) and Schäfer & Scharz (REF). Nuijten et al. performed a meta-meta-analysis on the fields of RESEARCH and reports a median meta-analytic effect size of 0.24 for predictive validity and correlational studies. This estimate is likely positively biased due to publication bias and selective reporting. Schäfer & Scharz report a median $r$ of 0.16 amongst preregistered research in FIELD, with an 'upper median' (i.e., the 83.35% quantile) of 0.41. This estimate is likely less affected by positive bias but should not be expected to be unbiased (REF Scheel).

We defined the between-studies variance ($\tau^2$) to cover a wide range of variance in true effect sizes and empirical estimates reported in the psychological literature. Van Erp et al., (REF) provide empirical heterogeneity estimates from a large number of meta-analyses. The median (uncorrected)Pearson's $r$ $\tau$ value in these data was 0.17 (interquartile range 0.1 - 0.24). As the point of this study is that empirical heterogeneity estimates may not be accurately estimated we selected a range of $\tau$ estimates for the product-moment that we considered reasonable and that also covered most of the empirical estimates from van Erp et al. These were {0, 0.1, 0.15, 0.2}. We selected the largest heterogeneity level to avoid excissive truncation for our maximum average effect size of 0.6: $\theta_i \sim N(\theta = 0.6, \tau = 0.2)$ implies approximately 95% of effect sizes will be below 1.

Fisher's z is measured at a different scale and it is not possible to directly convert $\tau$-values expressed in Pearson's $r$ into corresponding $\tau$-values on the Fisher's $z$ scale. To be able to compare results between Fisher's $z$ and the product-moment correlation we defined heterogeneity on the $I^2$ scale. The $I^2$ index is a relative measure of heterogeneity describing the percentage of total variance that is due to heterogeneity (REF Higgins), an as such is measured from 0 - 100%. The $I^2$ index can be defined as $I^2 = \frac{\tau^2}{s^2 + \tau^2}$ where $s^2 = \frac{\Sigma w_i(k - 1)}{(\Sigma w_i)^2 - \Sigma w_i^2}$ and $w_i$ is the precision $w_i = 1 / \sigma^2$. This means that its value generally depends on both the number of studies $K$ and the sample size $N_i$ in these studies (REF: Higgins, I2 is not an absolute measure of heterogeneity). However, because we keep $N_i$ fixed across studies (see below), the value of $I^2$ depends only on $N = N_i$ and the heterogeneity.

We thus computed the $I^2$ index for each sample size condition and $\tau$-values defined in Pearson's $r$, and then computed corresponding $\tau$-values on the Fisher's $z$ scale given these $I^2$-values and sample sizes. Two complications were that the value for $\sigma^2$ varies with effect size for Pearson's $r$ and that effect size varies across studies with $\tau$. We resolved this by setting $\theta = 0$ and computing the expected $\sigma^2$ given $\tau$ and sample size and then using this expected value to compute $I^2$. We used the 'law of the unconscious statistician' to compute the expected value of $\sigma^2$ which says that the expected value of a function $g(X)$ of a random variable can be expressed in terms of the probability distribution of X: $E[g(X)] = \int_{-\infty}^\infty g(x) f(x) dx$. In our case, $g(x)$ corresponds to the sampling variance of the product-moment correlation, $f(x) \sim N(\theta, \tau^2)$ and because the product-moment correlation is bounded at {-1, 1} we set $\infty$ instead to 0.999. The resulting $\tau$ values for fisher z are only minimally different from {0, 0.1, 0.15, 0.2}, but can be found in supplement XX.

We ran our analyses across a range of sample sizes $N$ and number of studies $K$. Within meta-analyses we used a fixed sample size across all studies such that $N = N_i$. We made this choice rather than treating sample size as a random variable (as done by e.g., REF1, REF2) to be able to observe the effect of changes in sample size on estimates, and because we are focused on the average heterogeneity estimates rather than its sampling variance. We base our range of sample sizes on empirical estimates of typical sample size in correlational research in psychology by XXX (still needs lookup) and consider the following values {VALUES}.

When focusing on the bias of heterogeneity estimate, the value of $K$ would not matter for an unbiased estimator. However, both the heterogeneity estimator we use, restricted maximum likelihood, and most other heterogeneity estimators are truncated at zero (REF Viechtbauer). Lower values for $K$ then results in a larger proportion of truncated estimates below zero because of increased sampling variance. As such, the values of $K$ can affect bias, especially at lower levels of heterogeneity. In their review of meta-analyses in psychology, van Erp et al. (YEAR) find $K$ values ranging from XX to XX, and in our simulations we consider the following values {VALUES}.

Finally, we defined average reliability levels and their standard deviation based on empirical estimates from the literature. Flake et al. (YEAR) report that based on 245 estimates of Cronbach's alpha in psychology the average estimate was 0.79 with a standard deviation of 0.13. The interquartile range was approximately 0.68 - 0.87 for studies wherein the associated scale lacked a reference and 0.79 - 0.88 for scales that did have a reference. Sanchez-Meca et al. (REF) report the reliability estimates based on five 'reliability generalization studies'. These five studies summarized reliability in 25 - 51 primary studies (184 total), and the mean reliability ranged from 0.767 to 0.891 with standard deviations ranging between 0.034 - 0.133. Given some likely positive bias in such empirical values, we examined the following mean reliabilities $\{0.6, 0.7, 0.8, 0.9\}$, and standard deviations $\{0, 0.05, 0.1, 0.15\}$.
