---
format: docx
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

```{r load data and prep, include = FALSE}
doc_type <- knitr::opts_knit$get('rmarkdown.pandoc.to') #to define the type of document, then format tables using an ifelse statement, see also: https://stackoverflow.com/questions/58999720/scientific-journal-table-of-variable-mean-per-year-with-s-d

tables <- readRDS("tables.RDS")
library(kableExtra) #must be loaded to print saved kableextra tables

```

Estimating heterogeneity of effect sizes is usually considered the main purpose of meta-analysis in addition to estimating an average effect size. Heterogeneity of effect sizes (henceforth referred to as heterogeneity) refers to an effect size's sensitivity to variability in study design, or more specifically to sensitivity to differences in four study factors: 1) sample population, 2) settings, 3) treatment variables and 4) measurement variables (e.g., Campbell & Stanley, 2015). Heterogeneity is a primary outcome in meta-analysis because its presence signals the existence of moderators and so can be seen as an opportunity for theoretical development (Simons et al., 2017), because it affects the implementation of research by indicating that an intervention may not be equally effective under all conditions or for all people, and because its presence changes the interpretation of the average effect sized derived in meta-analysis from the true population effect size (under homogeneity) to the average of true subpopulation effect sizes (under heterogeneity). As such, estimating heterogeneity without bias is of central importance to meta-analysis (for a more extensive discussion on the importance of heterogeneity, see Olsson Collentine et al., 2020; Simons, 2017).

Meta-analytic estimates are highly dependent on the quality of the data in the primary studies that the method summarizes. However, there is concern that researchers in psychology tend to neglect proper measurement (ref measurement-schmesurement) to the extent that psychology can be said to be in a measurement crisis (Kane et al., 2021). For example, Flake et al. (2017) report that although latent constructs are widely examined in personality and social psychology, researchers barely report any of the validity evidence required to ascertain the extent to which implemented scales measure the constructs of interest. Such inattention to measurement can also bias heterogeneity estimates. We will focus here on reliability of measurement (defined in classical test theory as the ratio of true score variance to measurement error variance; e.g., REF), which is the only measurement validity evidence researchers report consistently (Flake et al., 2017), and which can be corrected for in meta-analysis (Hunter & Schmidt; 2015).

Measurement error (imperfect reliability) is present in any empirical effect size estimate and systematically attenuates observed effect sizes compared to true underlying effect sizes. This affects heterogeneity estimates, which from a statistical perspective estimate the variance between true effect sizes. However, reliability is not necessarily accounted for in heterogeneity estimates, because ‘true’ underlying effect sizes can refer to two different entities. Either effect sizes free from sampling error, or effect sizes free from both sampling error and measurement artifacts (such as imperfect reliability, restriction of range, and dichotomization; Hunter & Schmidt, 2015). Whereas all meta-analytic models attempt to correct for sampling error in their estimates, correcting for measurement artifacts is implicit only in the models of Hunter & Schmidt (2015). This has consequences for heterogeneity estimates reported in many areas of psychology. Inattention to measurement reliability and how it affects heterogeneity estimates can lead researchers to misinterpret their meta-analytic average effect size estimate, ignore the presence of moderators or search for (and discover) non-existent moderators, and to implement research interventions inappropriately.

Meta-analyses in psychology rarely correct for unreliability in primary study measurements. Wiernick and Dahlke (2020) report that among the 71 meta-analytic studies published in the journal Psychological Bulletin between 2016 and 2018, only 6/71 (8%) corrected for unreliability. Similarly, Schmidt (2010) report that only 19/199 (10%) of the meta-analytic studies published in the same journal between 1978 - 2006 corrected for any measurement artifacts. The exception is the subfield of Industrial-Organizational studies where corrections tend to be more common (e.g., Cortina, 2003; Anguinis, 2011).

One reason that few meta-analysts correct for unreliability could be that correcting for unreliability is generally controversial and has been so since it was proposed (for arguments spanning the last century, see Table 2 of LeBreton et al., 2017). On the one hand, correcting for unreliability is seen by some as conceptually problematic because it inflates effect size estimates to match a hypothetical and unachievable scenario of perfect measurement (e.g., Seymour, 1988; LeBreton et al., 2017). Systematically correcting for unreliability may also be expected to lead to overestimation of effect sizes, because reliability estimates are lower bound estimates (i.e., a reliability estimate of 0.8 implies the reliability is between 0.8 - 1). Even worse, the most commonly reported estimate of reliability in psychology (Chronbach's alpha; Flake et al., 2017) as well as improvements thereof tend to underestimate the lower bound of reliability (REF Klaas). Moreover, from a pragmatic meta-analytic perspective, effect sizes appear to often be overestimated (e.g., REF1, REF2) due to factors such as publication bias (REF1) and _p_-hacking (REF2), so correcting for unreliability may inflate meta-analytic estimates even further and lead to estimates further from their true values rather than closer. On the other hand, researchers in psychology are typically interested in latent constructs rather than observed measures. As such, neglecting measurement artifacts means that computed estimates do not correspond to the entity of interest. From this perspective, correcting for unreliability is desirable even if doing so is challenging (e.g., Oswald et al., 2015; Hunter & Schmidt, 2015). In the end, whether correcting for unreliability or not, authors and consumers of meta-analyses need to be aware of how this decision affects heterogeneity estimates.

## Bias in heterogeneity estimates due to unreliability

In classical test theory (CTT), reliability of measurement ($R_{xx}$) is defined as the proportion true score variance ($\sigma^2_T$) to observed score variance ($\sigma^2_X$) and is presumed to be constant for all participants in a sample $R_{xx} = \sigma^2_T / \sigma^2_X$. We use unreliability of measurement to refer to $1 - R_{xx}$. Imperfect reliability in measurements leads to attenuation of observed effect sizes. For product-moment correlations the observed correlation can be computed as $r_{xy} = \rho_{xy} \times \sqrt{R_{xx'}} \times \sqrt{R_{yy'}}$ where $r_{xy}$ is the observed correlation between the variables $X$ and $Y$, $\rho_{xy}$ is the true correlation (with or without sampling error) and $R_{xx'}$ and $R_{yy'}$ are the measurement reliabilities for $X$ and $Y$. As such, correcting for unreliability is straightforward for product-moment correlations, but can also be done for other effect size types (Wiernick and Dahlke, 2020). A corrected correlation should also have its sampling variance corrected. This is inflated by a factor equal to the square of the reliabilities, such that if $a = \sqrt{R_{xx'}} \times \sqrt{R_{yy'}}$ then the corrected sampling variance $V_\rho = V_{r} / a^2$ (e.g., Borenstein, 2009, p. 343). Imperfect reliability in primary studies imbues heterogeneity estimates with both a positive and a negative bias.

A positive bias can be expected in heterogeneity estimates of uncorrected effect sizes due to the variance in reliability between primary studies. This is because the attenuation in effect size will then differ from study to study, which results in differences across studies in observed effect sizes beyond their true variability. This is most easily seen by assuming infinite sample size (i.e., no within-study variance) and in the context of a fixed effect model (i.e., with a common underlying mean across studies and no true heterogeneity). Table 1 illustrates how under these conditions three studies with differing reliability will observe variability in effect sizes beyond their true variability (i.e., zero). Table 1 also shows how this effect increases as the true effect size increases.

Table 1.

*Variance in reliability leads to positive bias in heterogeneity estimates which increases with average effect size*

```{r positive bias table}

if (doc_type == "docx") {
  print("Table 1 approximately here. Cannot be printed nicely to Word, see tables.pdf")
} else{
  tables$positive_bias_table
}

```

A negative bias can be expected in heterogeneity estimates of uncorrected effect sizes because given the same unreliability, the attenuation in absolute value increases with effect size. That is, although the attenuation formula $r_{xy} = \rho_{xy} \times \sqrt{R_{xx'}} \times \sqrt{R_{yy'}}$ implies that any effect size $\rho_{xy}$ will be attenuated by the same proportion given the same reliabilities $\sqrt{R_{xx'}} \times \sqrt{R_{yy'}}$, this proportion corresponds to a larger absolute value for larger effect sizes. As a consequence, in the presence of heterogeneity larger true effect sizes will move further towards zero than smaller ones, decreasing heterogeneity in observed effect sizes. Table 2 illustrates this relationship for infinite sample size (i.e., no sampling variance) and three levels of reliability across three studies.

Table 2.

*Imperfect reliability leads to negative bias in heterogeneity estimates*

```{r negative bias table}

if (doc_type == "docx") {
  print("Table 2 approximately here. Cannot be printed nicely to Word, see tables.pdf")
} else{
  tables$negative_bias_table
}

```

Attenuation of effect size can also be understood as a downwards shift of the distribution of true effect sizes. To illustrate, assume an infinite sample size (i.e., no within-study sampling variance), some heterogeneity $\tau^2$, equal reliability in outcome and measurement such that $R_{xx'} = R_{yy'} = R$, and that all studies within a population of studies have the same reliability. Observed effect sizes given measurement error then corresponds to multiplying all true effect sizes $\theta_i$ by a factor equal to the reliability $R$ (i.e., multiplying the distribution of effect sizes by a constant $R$). If the distribution of effect sizes is normally distributed $\theta_i \sim N(\theta, \tau^2)$ with a mean $\theta$ (i.e., the assumption of the random effects model), then $R \times N(\theta, \tau) = N(R\theta, R^2 \tau^2) = N(R\theta, R\tau)$. Because reliability is always a value \< 1, it can be seen that the square root of the observed heterogeneity for uncorrected effect sizes is less than its true value by a factor $R$. This depiction is not entirely correct for Pearson's $r$ as it is bounded at {-1, 1}, but holds approximately as long as the mean effect size $\theta$ and heterogeneity $\tau^2$ are not too large.

To make this concrete, consider an average reliability of 0.8 (the median reported reliability in psychology; Flake et al., 2017). Then, $0.8 \times N(\theta, \tau) = N(0.8\theta, 0.8 \tau)$, resulting in 20% less standard deviation among observed effect sizes than in true effect sizes. We can consider the positive bias in the same vein. Assuming a common true effect size $\mu$ across studies and infinite sample size within studies we can view $\mu$ as a constant. In this case, attenuation consists of a distribution of reliabilities $R \sim N(\bar{R}, \sigma_R)$ with some mean $\bar{R}$ and standard deviation $\sigma_R$ across studies, that is multiplied by the constant $\mu$ to yield a distribution of observed effect sizes $r \sim N(\mu \bar{R}, \mu \sigma_R)$. To make this concrete, consider some values $\mu = 0.2$, $\bar{R} = 0.8$, $\sigma_R = 0.1$ which then yield $r \sim N(0.16, 0.01)$. Again, this depiction is not entirely correct as reliability is bounded at {0, 1}, but holds approximately as long as the mean reliability $\bar{R}$ and its standard deviation $\sigma_R$ are not too large.

It is difficult to predict the total effect of the negative and positive bias in heterogeneity due to unreliability. This depends on average effect size, the amount of heterogeneity, mean reliability across studies, the variability in reliabilities, and sampling variance within studies. Hence, we perform a Monte-Carlo simulation study to explore the expected bias in heterogeneity estimates when meta-analyzing primary studies with imperfect measurement reliabilities.
