---
format: docx
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

```{r load data and prep, include = FALSE}
doc_type <- knitr::opts_knit$get('rmarkdown.pandoc.to') #to define the type of document, then format tables using an ifelse statement, see also: https://stackoverflow.com/questions/58999720/scientific-journal-table-of-variable-mean-per-year-with-s-d

tables <- readRDS("tables.RDS")
library(kableExtra) #must be loaded to print saved kableextra tables

```

Estimating heterogeneity of effect sizes is usually considered the main purpose of meta-analysis in addition to estimating an average effect size. Heterogeneity of effect sizes (henceforth referred to as heterogeneity) refers to an effect size's sensitivity to variability in study design, or more specifically to sensitivity to differences in four study factors: 1) sample population, 2) settings, 3) treatment variables and 4) measurement variables (e.g., Campbell & Stanley, 2015). Heterogeneity is a primary outcome in meta-analysis because its presence signals the existence of moderators and so can be seen as an opportunity for theoretical development (Simons et al., 2017), because it affects the implementation of research by indicating that an intervention may not be equally effective under all conditions or for all people, and because its presence changes the interpretation of the average effect sized derived in meta-analysis from the true population effect size (under homogeneity) to the average of true subpopulation effect sizes (under heterogeneity). As such, estimating heterogeneity without bias is of central importance to meta-analysis (for a more extensive discussion on the importance of heterogeneity, see Olsson Collentine et al., 2020; Simons, 2017).

Meta-analytic estimates are highly dependent on the quality of data in primary studies. However, there is concern that researchers in psychology tend to neglect proper measurement (Flake & Fried, 2020) to the extent that psychology can be said to be in a measurement crisis (Kane et al., 2021). For example, Flake et al. (2017) report that although latent constructs are widely examined in personality and social psychology, researchers barely report any of the validity evidence required to ascertain the extent to which implemented scales measure the constructs of interest. Such inattention to measurement can also bias heterogeneity estimates. We will focus here on reliability of measurement (defined in classical test theory as the ratio of true score variance to measurement error variance; e.g., REF), which is the only measurement validity evidence researchers report consistently (Flake et al., 2017), and which can be corrected for in meta-analysis (Hunter & Schmidt; 2015).

Measurement error (imperfect reliability) is present in any empirical effect size estimate and systematically attenuates observed effect sizes compared to true underlying effect sizes. This affects heterogeneity estimates, which from a statistical perspective estimate the variance between true effect sizes. However, reliability is not necessarily accounted for in heterogeneity estimates, because ‘true’ underlying effect sizes can refer to two different entities. Either effect sizes free from sampling error, or effect sizes free from both sampling error and measurement artifacts (such as imperfect reliability, restriction of range, and dichotomization; Hunter & Schmidt, 2015). Whereas all meta-analytic models attempt to correct for sampling error in their estimates, correcting for measurement artifacts is implicit only in the models of Hunter & Schmidt (2015). This has consequences for heterogeneity estimates reported in many areas of psychology. Inattention to measurement reliability and how it affects heterogeneity estimates can lead researchers to misinterpret their meta-analytic average effect size estimate, ignore the presence of moderators or search for (and discover) non-existent moderators, and to implement research interventions inappropriately.

Meta-analyses in psychology rarely correct for unreliability in primary study measurements. Wiernick and Dahlke (2020) report that among the 71 meta-analytic studies published in the journal Psychological Bulletin between 2016 and 2018, only 6/71 (8%) corrected for unreliability. Similarly, Schmidt (2010) report that only 19/199 (10%) of the meta-analytic studies published in the same journal between 1978 - 2006 corrected for any measurement artifacts. The exception is the subfield of Industrial-Organizational studies where corrections tend to be more common (e.g., Cortina, 2003; Anguinis, 2011).

One reason that few meta-analysts correct for unreliability could be that correcting for unreliability is generally controversial and has been so since it was proposed (for arguments spanning the last century, see Table 2 of LeBreton et al., 2017). On the one hand, correcting for unreliability is seen by some as conceptually problematic because it inflates effect size estimates to match a hypothetical and unachievable scenario of perfect measurement (e.g., Seymour, 1988; LeBreton et al., 2017). Systematically correcting for unreliability may also be expected to lead to overestimation of effect sizes, because reliability estimates are lower bound estimates (i.e., a reliability estimate of 0.8 implies the reliability is between 0.8 - 1). Even worse, the most commonly reported estimate of reliability in psychology (Chronbach's alpha; Flake et al., 2017) as well as improvements thereof tend to underestimate the lower bound of reliability (REF Klaas). Moreover, from a pragmatic meta-analytic perspective, effect sizes appear to often be overestimated in psychology (e.g., Kvarven et al., 2019; Schäfer & Scharz, 2019; Scheel et al., 2020) due to selective reporting based on the significance of outcomes (Simmons et al., 2011) and publication bias (REF?), so correcting for unreliability may inflate meta-analytic estimates even further and lead to estimates further from their true values rather than closer. On the other hand, researchers in psychology are typically interested in latent constructs rather than observed measures. As such, neglecting measurement artifacts means that computed estimates do not correspond to the entity of interest. From this perspective, correcting for unreliability is desirable even if doing so is challenging (e.g., Oswald et al., 2015; Hunter & Schmidt, 2015). In the end, whether correcting for unreliability or not, authors and consumers of meta-analyses need to be aware of how this decision affects heterogeneity estimates.

## Bias in heterogeneity estimates due to unreliability

In classical test theory (CTT), reliability of measurement ($R_{xx}$) is defined as the proportion true score variance ($\sigma^2_T$) to observed score variance ($\sigma^2_X$) and is presumed to be constant for all participants in a sample $R_{xx} = \sigma^2_T / \sigma^2_X$. We use unreliability of measurement to refer to $1 - R_{xx}$. Imperfect reliability in measurements leads to attenuation of observed effect sizes. For product-moment correlations the observed correlation can be computed as $r_{xy} = \rho_{xy} \times \sqrt{R_{xx'}} \times \sqrt{R_{yy'}}$ where $r_{xy}$ is the observed correlation between the variables $X$ and $Y$, $\rho_{xy}$ is the true correlation (with or without sampling error) and $R_{xx'}$ and $R_{yy'}$ are the measurement reliabilities for $X$ and $Y$. As such, correcting for unreliability is straightforward for product-moment correlations, but can also be done for other effect size types (Wiernick and Dahlke, 2020). A corrected correlation should also have its sampling variance corrected. This is inflated by a factor equal to the square of the reliabilities, such that if $a = \sqrt{R_{xx'}} \times \sqrt{R_{yy'}}$ then the corrected sampling variance $V_\rho = V_{r} / a^2$ (e.g., Borenstein, 2009, p. 343). Imperfect reliability in primary studies imbues heterogeneity estimates with both a positive and a negative bias.

A positive bias can be expected in heterogeneity estimates of uncorrected effect sizes due to the variability in reliability between primary studies. This is because the attenuation in effect size will then differ from study to study, which results in differences across studies in observed effect sizes beyond their true variability. This is most easily seen by assuming a common true effect size $\mu$ across studies (i.e., a fixed effect and no true heterogeneity) and infinite sample size within studies. Any observed effect size $\hat{\mu}$ can then be seen as the true effect size $\mu$ multiplied by a study’s reliability $R_i$ (assuming $R_i = R_{xx’} = R_{yy’}$). Hence, the distribution of observed effect sizes is the true effect size $\mu$ multiplied by the distribution of reliabilities across studies. That is, $\hat{mu} \sim \mu \times N(\bar{R}, \sigma_R) = N(\mu \bar{R}, \mu \sigma_R$, where $\bar{R}$ is the average reliability across studies and $\sigma_R$ its standard deviation. Because reliability lies between 0 – 1, under these conditions imperfect reliability implies that the observed average effect size will be less than the true effect size by a factor $\bar{R}$ and estimated heterogeneity will be larger than true heterogeneity (i.e., zero) by the product of $\mu \times \sigma_R$ (if heterogeneity is expressed as standard deviation rather than variance). That is, variability in reliability across studies leads to a positive bias in heterogeneity that depends on the size of the true effect size. Table 1 illustrates this effect for three studies with differing reliability and zero true heterogeneity.

Table 1.

*Variance in reliability leads to positive bias in heterogeneity estimates which increases with average effect size*

```{r positive bias table}

if (doc_type == "docx") {
  print("Table 1 approximately here. Cannot be printed nicely to Word, see tables.pdf")
} else{
  tables$positive_bias_table
}

```

A negative bias can be expected in heterogeneity estimates of uncorrected effect sizes because in absolute terms larger effect sizes are more attenuated by a given reliability. That is, although the attenuation formula $r_{xy} = \rho_{xy} \times \sqrt{R_{xx'}} \times \sqrt{R_{yy'}}$ implies that any effect size $\rho_{xy}$ will be attenuated by the same proportion given the same reliabilities $\sqrt{R_{xx'}} \times \sqrt{R_{yy'}}$, this proportion corresponds to a larger absolute value for larger effect sizes. Consequently, in the presence of heterogeneity attenuation will move larger true effect sizes further towards zero than smaller ones, decreasing heterogeneity in observed effect sizes. This negative bias can be observed directly by assuming an average effect size $\theta$ with some heterogeneity $\tau^2$, infinite sample size within studies (i.e., no within-study sampling variance), and equal reliability across studies ($R = R_i = R_{xx'} = R_{yy'}$). Any observed effect size $\hat{\theta_i}$ can then be seen as a study’s true effect size $\theta_i$ multiplied by the study reliability $R$. Hence, the distribution of observed effect sizes is the distribution of true effect sizes $N(\theta, \tau^2)$ multiplied by the reliability $R$, such that $R \times N(\theta, \tau^2) = N(R \theta, R^2 \tau^2) = N(R \theta, R \tau)$. Because reliability lies between 0 – 1, under these conditions imperfect reliability implies that both the observed average effect size and the estimated heterogeneity will be less than their true value by a factor $\bar{R}$ (if heterogeneity is expressed as standard deviation rather than variance). This depiction is not entirely correct for Pearson's $r$ as it is bounded at {-1, 1}, but holds approximately if the mean effect size $\theta$ and heterogeneity $\tau^2$ are not too large. Hence, imperfect reliability in studies can be expected to create a negative bias in heterogeneity estimates whenever true heterogeneity is present. Consider an average reliability of 0.8 (the median reported reliability in psychology; Flake et al., 2017). Then, $0.8 \times N(\theta, \tau) = N(0.8\theta, 0.8 \tau)$, resulting in 20% less standard deviation among observed effect sizes than in true effect sizes. Table 2 illustrates this relationship for infinite sample size (i.e., no sampling variance) and three levels of reliability across three studies.

Table 2.

*Imperfect reliability leads to negative bias in heterogeneity estimates*

```{r negative bias table}

if (doc_type == "docx") {
  print("Table 2 approximately here. Cannot be printed nicely to Word, see tables.pdf")
} else{
  tables$negative_bias_table
}

```

It is difficult to predict the total effect of the negative and positive bias in heterogeneity due to unreliability, and existing literature tends to focus on the positive bias (e.g., Borenstein, 2009, p. 342; Card, 2015, p. 126; Hunter & Schmidt, 2015; Wiernick & Dahlke, 2020). However, given the value of accurate  heterogeneity estimates for many research outcomes in psychology and that the negative bias could often be 20% or more, we consider it important for meta-analysts to have insight into the total bias in heterogeneity estimates that should be expected due to unreliability. This depends on average effect size, true heterogeneity, mean reliability across studies, the variability in reliabilities, and sampling variance within studies. Hence, we perform a Monte-Carlo simulation study to explore the expected bias in heterogeneity estimates due to unreliability in primary studies.
