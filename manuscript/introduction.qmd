---
format: docx
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```



Estimating heterogeneity of effect sizes is usually considered the second main purpose of meta-analysis, after estimating an average effect size. Heterogeneity of effect sizes (henceforth referred to as heterogeneity) refers to an effect size's sensitivity to variability in study design, or more specifically to sensitivity to differences in four study factors: 1) sample population, 2) settings, 3) treatment variables and 4) measurement variables (e.g., Campbell & Stanley, 2015). From a more statistical perspective, heterogeneity is defined as the variance between 'true' subpopulation effect sizes. However, true subpopulation effect sizes can refer to two different entities. Either effect sizes free from sampling error, or effect sizes free from both sampling error and measurement artifacts (such as measurement error, restriction of range, and dichotomization; Hunter & Schmidt, YEAR). Of these measurement artifacts, measurement error is present in any empirical effect size estimate and systematically attenuates effect sizes. It is usually measured as reliability (defined in classical test theory as the ratio of true score variance to measurement error variance; REF) and is relatively common to report in primary studies in psychology (REF: Flake). However, whereas all meta-analytic models attempt to correct for sampling error in their estimates, correcting for imperfect reliability is implicit only in the models of Hunter & Schmidt (YEAR).

One reason could be that correcting for unreliability is controversial and has been so since it was proposed (for some arguments spanning the last century, see Table 2 of LeBreton et al, YEAR). On the one hand, correcting for unreliability is seen by some as conceptually problematic because it inflates effect size estimates to match a hypothetical and unachievable scenario of perfect measurement (e.g., REF1, REF2). Systematically correcting for unreliability may also be expected to lead to overestimation of effect sizes, because reliability estimates are lower bound estimates (i.e., a reliability estimate of 0.8 implies the reliability is between 0.8 - 1). Even worse, the most commonly reported estimate of reliability in psychology (Chronbach's alpha; REF Flake) as well as improvements thereof tend to underestimate the lower bound of reliability (REF Klaas). Moreover, from a pragmatic meta-analytic perspective, effect sizes appear to often be overestimated (e.g., REF1, REF2, REF3), so correcting for unreliability may inflate meta-analytic estimates even further and lead to estimates further from their true values rather than closer. On the other hand, researchers in psychology are typically interested in latent constructs rather than observed measures. As such, neglecting measurement artifacts means that computed estimates do not correspond to the entity of interest. From this perspective, correcting for unreliability is desirable even if doing so is challenging (e.g., Oswald, Hunter & Schmidt).

Meta-analyses in psychology rarely correct for unreliability in primary study measurements. Wiernick and Dahlke (REF) report that among the 71 meta-analytic studies published in the journal Psychological Bulletin between 2016 and 2018, only 6/71 (8%) corrected for unreliability. Similarly, Schmidt, Oh and Hayes (REF) report that only 19/199 (10%) of the meta-analytic studies published in the same journal between 1978 - 2006 corrected for any measurement artifacts. The exception is the subfield of Industrial-Organizational studies. Although at the meta-analysis level rather than study level, Cortina (REF) showed that about 80% of the 1,294 meta-analyses published across 59 meta-analytic studies in the Journal of Applied Psychology between 1978 - 1997 corrected for measurement artifacts, and Anguinis (REF) that around 50% of meta-analyses identified in studies published across several management journal between 1989 - 2009 corrected for unreliability of measurement. In the end, whether correcting for unreliability or not, authors and consumers of meta-analyses should be aware of how this decision affects heterogeneity estimates.

Imperfect reliability in primary studies imbues heterogeneity estimates with both a positive and a negative bias. The positive bias is discussed extensively by Hunter and Schmidt (REF) who introduced the concept of psychometric meta-analysis (i.e., meta-analysis corrected for measurement artifacts in primary studies, such as unreliability), as well as discussed in other highly cited meta-analysis handbooks (Borenstein, 2009, p.342; Card, 2012, p. 126) and recent literature (Wiernick and Dahlke, 2020). However, there appears to be a lack of acknowledgement that imperfect reliability can also lead to a negative bias in heterogeneity estimates. The only mention we found thereof was in the 2nd edition of _Methods of Meta-analysis_ (Hunter & Schmidt, p. 142) in a paragraph later absent from the 3rd edition (REF). Below we detail the argument for both a positive and negative bias in heterogeneity estimates.

## Bias in heterogeneity estimates due to unreliability

In classical test theory (CTT), reliability of measurement ($R_{xx}$) is defined as the proportion true score variance ($\sigma^2_T$) to observed score variance ($\sigma^2_X$) and is presumed to be constant for all participants in a sample $R_{xx} = \sigma^2_T / \sigma^2_X$. We use unreliability of measurement to refer to $1 - R_{xx}$. Imperfect reliability in measurements leads to attenuation of observed effect sizes. For product-moment correlations the attenuation is simply computed as $r_{xy} = \rho_{xy} \times \sqrt{R_{xx'}} \times \sqrt{R_{yy'}}$ where $r_{xy}$ is the observed correlation between the variables $X$ and $Y$, $\rho_{xy}$ is the true correlation (with or without sampling error) and $R_{xx'}$ and $R_{yy'}$ are the measurement reliabilities for $X$ and $Y$. As such, correcting for unreliability is straightforward for product-moment correlations (although the sampling variance must then also be corrected: REF), but can also be done for other effect size types (REF: Wiernick).

A positive bias can be expected in heterogeneity estimates of uncorrected effect sizes due to the variance in reliability between primary studies. This is because the attenuation in effect size will then differ from study to study, which results in differences across studies in observed effect sizes beyond their true variability. This is most easily seen by assuming infinite sample size (i.e., no within-study variance) and in the context of a fixed effect model (i.e., with a common underlying mean across studies and no true heterogeneity). Table 1 illustrates how under these conditions three studies with differing reliability will observe variability in effect sizes beyond their true variability (i.e., zero). Table 1 also shows how this effect increases as the true effect size increases.

(TABLE 1; reproduced with permission from Olsson-Collentine, 2020)

A negative bias can be expected in heterogeneity estimates of uncorrected effect sizes due to larger attenuation of larger effect sizes for the same level of unreliability. That is, although the attenuation formula $r_{xy} = \rho_{xy} \times \sqrt{R_{xx'}} \times \sqrt{R_{yy'}}$ implies that any effect size $\rho_{xy}$ will be attenuated by the same proportion for fixed reliabilities $\sqrt{R_{xx'}} \times \sqrt{R_{yy'}}$, this proportion corresponds to a larger absolute value for larger effect sizes. As a consequence, in the presence of heterogeneity larger true effect sizes will move further towards zero than smaller ones, decreasing heterogeneity in observed effect sizes.

Table 2 illustrates this relationship for infinite sample size (i.e., no sampling variance) and several levels of average reliability across three studies.

(Table 2)

Attenuation of effect size can also be understood as a downwards shift of the distribution of true effect sizes. To illustrate, assume an infinite sample size (i.e., no within-study sampling variance), some heterogeneity $\tau^2$, equal reliability in outcome and measurement such that $R_{xx'} = R_{yy'} = R$, and that all studies within a population of studies have the same reliability. Observed effect sizes given measurement error then corresponds to multiplying all true effect sizes $\theta_i$ by a factor equal to the reliability $R$. This is equal to multiplying the distribution of effect sizes by a constant $R$. If the distribution of effect sizes is normally distributed $\theta_i \sim N(\theta, \tau^2)$ with a mean $\theta$ (i.e., the assumption of the random effects model), then $R \times N(\theta, \tau) = N(R\theta, R^2  \tau^2) = N(R\theta, R\tau)$. Because reliability is always a value < 1, it can be seen that the square root of the observed heterogeneity for uncorrected effect sizes is less than its true value by a factor $R$. This depiction is not entirely correct for Pearson's $r$ as it is bounded at {-1, 1}, but holds approximately as long as the mean effect size $\theta$ and heterogeneity $\tau^2$ are not too large.

To make this concrete, consider an average reliability of 0.8 (the median reported reliability in psychology; REF). Then, $0.8 \times N(\theta, \tau) = N(0.8\theta, 0.8 \tau)$, resulting in 20% less heterogeneity among observed effect sizes than in true effect sizes. We can consider the positive bias in the same vein. Assuming a common true effect size $\mu$ across studies and infinite sample size within studies we can view $\mu$ as a constant. In this case, attenuation consists of a distribution of reliabilities $R \sim N(\bar{R}, \sigma_R)$ with some mean $\bar{R}$ and standard deviation $\sigma_R$ across studies, that is multiplied by the constant $\mu$ to yield a distribution of observed effect sizes $r \sim N(\mu \bar{R}, \mu \sigma_R)$. To make this concrete, consider some values $\mu = 0.2$, $\bar{R} = 0.8$, $\sigma_R = 0.1$ which then yield $r \sim N(0.16, 0.01)$. Again, also this depiction is not entirely correct as reliability is bounded at {0, 1}, but holds approximately as long as the mean reliability $\bar{R}$ and its standard deviation $\sigma_R$ are not too large.

It is difficult to predict the total effect of the negative and positive bias in heterogeneity due to unreliability. This depends on average effect size, the amount of heterogeneity, mean reliability across studies, the variability in reliabilities, and as the sampling variance for Pearson's $r$ is affected by its effect size, sample size within studies may also matter. We thus perform a Monte-Carlo simulation study to examine the expected bias in heterogeneity estimates when meta-analyzing primary studies with imperfect measurement reliabilities.
